{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "460c7477",
   "metadata": {},
   "source": [
    "## Creating episodes metadata (formerly numbers, names, urls; now raw title, slug, guest name, url)\n",
    "\n",
    "### Original section:\n",
    "\n",
    "print(\n",
    "            \"\\n--- STAGE 2: Processing Episode Metadata & Generating Transcript URLs ---\"\n",
    "        )\n",
    "        processed_metadata_filepath_for_saving = os.path.join(\n",
    "            PROCESSED_DATA_DIR, \"num_name_url_df.parquet\"\n",
    "        )\n",
    "\n",
    "        # Step 2.1 Create numbers and names dictionary from html\n",
    "\n",
    "        numbers_names_dict = create_numbers_names_dict_from_html(episodes_html_filepath)\n",
    "\n",
    "        # Step 2.2 Create numbers and names dataframe from numbers and names dictionary\n",
    "\n",
    "        numbers_names_df = create_numbers_names_df_from_dict(numbers_names_dict)\n",
    "\n",
    "        # Step 2.3 Create URL's and add to the datframe, save dataframe\n",
    "\n",
    "        num_name_url_df = create_urls_and_save_to_numbers_names_df(\n",
    "            numbers_names_df, processed_metadata_filepath_for_saving\n",
    "        )\n",
    "\n",
    "        print(\"STAGE 2 Complete: numbers, names, urls dataframe saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bfd3f0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root Set to: c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\n",
      "V2 Test Directory Set to: c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Project Root and Imports ---\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import os.path \n",
    "from pathlib import Path\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz, process # <--- THE CRITICAL FIX\n",
    "\n",
    "# Get the path of the directory containing this notebook (e.g., /project/notebooks)\n",
    "# os.getcwd() typically works well in notebooks for this purpose.\n",
    "notebook_dir = os.getcwd() \n",
    "\n",
    "# Go UP one directory level to find the Project Root (e.g., /project)\n",
    "# NOTE: If your notebook is deeper, you might need another '../'\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "\n",
    "# Add the Project Root to Python's search path (sys.path)\n",
    "# This allows Python to find and import modules like 'utils' and 'off_menu'\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Now, imports should work\n",
    "from off_menu.utils import try_read_html_string_from_filepath, try_read_parquet, extract_html, save_text_to_file\n",
    "from off_menu.config import episodes_list_url, transcript_base_url, restaurants_url\n",
    "from off_menu.data_extraction import extract_and_save_html\n",
    "from off_menu.data_processing import create_mentions_by_res_name_dict, create_return_exploded_res_mentions_df, _clean_transcript_str_from_html\n",
    "\n",
    "# --- 2. Define Data Paths ---\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, \"raw\")\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, \"processed\")\n",
    "ANALYTICS_DATA_DIR = os.path.join(DATA_DIR, \"analytics\")\n",
    "\n",
    "# --- 3. Define and Create Test Temp Directory (V2_tests) ---\n",
    "Test_data_dir = os.path.join(DATA_DIR, \"test_temp\")\n",
    "new_test_folder = \"V2_tests\"\n",
    "V2_tests_dir = os.path.join(Test_data_dir, new_test_folder)\n",
    "\n",
    "# Create the directory structure, avoiding errors if it already exists\n",
    "os.makedirs(V2_tests_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Project Root Set to: {PROJECT_ROOT}\")\n",
    "print(f\"V2 Test Directory Set to: {V2_tests_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea43b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Access and save test data ---\n",
    "    \n",
    "test_episodes_html_filepath = os.path.join(V2_tests_dir, \"episodes.html\")\n",
    "test_restaurants_html_filepath = os.path.join(V2_tests_dir, \"restaurants.html\")\n",
    "\n",
    "extract_and_save_html(episodes_list_url, test_episodes_html_filepath)\n",
    "extract_and_save_html(restaurants_url, test_restaurants_html_filepath)\n",
    "\n",
    "print(\"Test HTML data downloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d809b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exceptions list\n",
      "['Best of 2024: Live', 'Best of 2024: Part 2', 'Best of 2024: Part 1', 'Best of 2023: Part 2', 'Best of 2023: Part 1', 'Best of 2022: Part 2', 'Best of 2022: Part 1', 'Best of 2021: Part 2', 'Best of 2021: Part 1', 'Best of 2020', 'Best of 2019']\n",
      "Names List\n",
      "[{'raw_title': 'Kunal Nayyar', 'slug': 'kunal-nayyar', 'guest_name': 'Kunal Nayyar'}, {'raw_title': 'Joy Crookes', 'slug': 'joy-crookes', 'guest_name': 'Joy Crookes'}, {'raw_title': 'Elle Fanning', 'slug': 'elle-fanning', 'guest_name': 'Elle Fanning'}, {'raw_title': 'Lucia Keskin', 'slug': 'lucia-keskin', 'guest_name': 'Lucia Keskin'}, {'raw_title': 'Ian Smith', 'slug': 'ian-smith', 'guest_name': 'Ian Smith'}, {'raw_title': 'Jen Brister (Tasting Menu)', 'slug': 'jen-brister-tasting-menu', 'guest_name': 'Jen Brister'}, {'raw_title': 'Gillian Anderson', 'slug': 'gillian-anderson', 'guest_name': 'Gillian Anderson'}, {'raw_title': 'Greg James', 'slug': 'greg-james', 'guest_name': 'Greg James'}, {'raw_title': 'Rhys James', 'slug': 'rhys-james', 'guest_name': 'Rhys James'}, {'raw_title': 'Nina Conti', 'slug': 'nina-conti', 'guest_name': 'Nina Conti'}]\n",
      "[{'raw_title': 'Ep 217: Ross Noble (Christmas Special)', 'slug': 'ep-217-ross-noble-christmas-special', 'guest_name': 'Ross Noble'}, {'raw_title': 'Ep 216: Dawn French (Christmas Special)', 'slug': 'ep-216-dawn-french-christmas-special', 'guest_name': 'Dawn French'}, {'raw_title': 'Ep 215: Paul Rudd', 'slug': 'ep-215-paul-rudd', 'guest_name': 'Paul Rudd'}, {'raw_title': 'Ep 214: Steve-O', 'slug': 'ep-214-steve-o', 'guest_name': 'Steve-O'}, {'raw_title': 'Ep 213: Harriet Kemsley', 'slug': 'ep-213-harriet-kemsley', 'guest_name': 'Harriet Kemsley'}, {'raw_title': 'Ep 212: Garth Marenghi', 'slug': 'ep-212-garth-marenghi', 'guest_name': 'Garth Marenghi'}, {'raw_title': 'Ep 211: Steve Coogan', 'slug': 'ep-211-steve-coogan', 'guest_name': 'Steve Coogan'}, {'raw_title': 'Ep 210: Paapa Essiedu', 'slug': 'ep-210-paapa-essiedu', 'guest_name': 'Paapa Essiedu'}, {'raw_title': 'Ep 209: Dr Maggie Aderin-Pocock', 'slug': 'ep-209-dr-maggie-aderin-pocock', 'guest_name': 'Dr Maggie Aderin-Pocock'}, {'raw_title': 'Ep 208: Izuka Hoyle', 'slug': 'ep-208-izuka-hoyle', 'guest_name': 'Izuka Hoyle'}]\n"
     ]
    }
   ],
   "source": [
    "# Turning \"create_numbers_names_dict_from_html\" into => create_ep_names_slugs_list_from_html\n",
    "# The function calls _create_epnumber_epname_dict, which will need editing\n",
    "\n",
    "# -------------------------\n",
    "# 1 slugify helper\n",
    "# -------------------------\n",
    "def slugify(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert text to a simple dash-separated, lowercase slug.\n",
    "    Example: \"Richard Herring (Bonus Episode)\" -> \"richard-herring-bonus-episode\"\n",
    "    \"\"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", text or \"\")\n",
    "    # remove parentheses but keep their content separated by space\n",
    "    s = s.replace(\"(\", \" \").replace(\")\", \" \")\n",
    "    # remove all characters except word chars, whitespace and hyphen\n",
    "    s = re.sub(r\"[^\\w\\s-]\", \"\", s)\n",
    "    # collapse whitespace to single dash and strip leading/trailing dashes\n",
    "    s = re.sub(r\"\\s+\", \"-\", s).strip(\"-\")\n",
    "    return s.lower()\n",
    "\n",
    "# -------------------------\n",
    "# 2 extract guest name\n",
    "# -------------------------\n",
    "def extract_guest_name(raw_title: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract guest name using the simple rule:\n",
    "      - split on first colon ':'\n",
    "      - take the right hand side if a separator exists\n",
    "      - remove any trailing parenthetical content e.g. ' (Bonus Episode)'\n",
    "      - strip whitespace\n",
    "    \"\"\"\n",
    "    if not raw_title:\n",
    "        return \"\"\n",
    "\n",
    "    s = raw_title.strip()\n",
    "\n",
    "    # Split on the first recognized separator in the remaining string.\n",
    "    # We prefer colon first as your original method did; then hyphens or em-dash.\n",
    "    if \":\" in s:\n",
    "        parts = s.split(\":\", 1)\n",
    "        candidate = parts[1].strip()\n",
    "    else:\n",
    "        # no separator found: either the whole string *is* the guest (as for new episodes)\n",
    "        candidate = s\n",
    "\n",
    "    # remove any parenthetical content at end or inside e.g \"Name (Live) extra\"\n",
    "    candidate = re.sub(r\"\\(.*?\\)\", \"\", candidate).strip()\n",
    "\n",
    "    # final clean: collapse multiple spaces\n",
    "    candidate = re.sub(r\"\\s+\", \" \", candidate).strip()\n",
    "\n",
    "    return candidate\n",
    "\n",
    "\n",
    "def create_tuple_inc_ep_slugs_and_guests_list_from_html(html_string: str) -> Tuple[List[Dict[str, Any]], List[str]]:\n",
    "    \"\"\"\n",
    "    Parse episodes HTML and return a tuple:\n",
    "      (\n",
    "        [list of valid episode records],\n",
    "        [list of raw_titles for excluded 'Best of' episodes]\n",
    "      )\n",
    "    \"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html_string, \"html.parser\")\n",
    "    episode_divs = soup.find_all(\"div\", class_=\"image-slide-title\")\n",
    "\n",
    "    # 1. Initialize two separate lists\n",
    "    records: List[Dict[str, Any]] = []\n",
    "    exceptions: List[str] = [] \n",
    "\n",
    "    for div in episode_divs:\n",
    "        raw_title = div.get_text(separator=\" \", strip=True)\n",
    "        \n",
    "        # 2. Check the condition using the string method\n",
    "        if raw_title.startswith(\"Best of\"):\n",
    "            # 3. If it is a \"Best of\" episode, append the title to the exceptions list\n",
    "            exceptions.append(raw_title)\n",
    "            # Skip the rest of the loop for this title and move to the next 'div'\n",
    "            continue\n",
    "        # menus to be buried with exception?\n",
    "        # christmas dinner party exception?\n",
    "            \n",
    "        # If the 'if' condition was false (i.e., it's a regular episode), the code continues here:\n",
    "        \n",
    "        guest_name = extract_guest_name(raw_title)\n",
    "        slug_full = slugify(raw_title)\n",
    "\n",
    "        records.append({\n",
    "            \"raw_title\": raw_title,\n",
    "            \"slug\": slug_full,\n",
    "            \"guest_name\": guest_name\n",
    "        })\n",
    "\n",
    "    # 4. Return both lists as a tuple\n",
    "    return records, exceptions\n",
    "\n",
    "# --- Running test\n",
    "\n",
    "episodes_html_str = try_read_html_string_from_filepath(test_episodes_html_filepath)\n",
    "\n",
    "test_episodes_list = create_tuple_inc_ep_slugs_and_guests_list_from_html(episodes_html_str)\n",
    "\n",
    "print(\"Exceptions list\")\n",
    "print(test_episodes_list[1])\n",
    "\n",
    "print(\"Names List\")\n",
    "print(test_episodes_list[0][:10])\n",
    "print(test_episodes_list[0][100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73392a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_title</th>\n",
       "      <th>slug</th>\n",
       "      <th>guest_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kunal Nayyar</td>\n",
       "      <td>kunal-nayyar</td>\n",
       "      <td>Kunal Nayyar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joy Crookes</td>\n",
       "      <td>joy-crookes</td>\n",
       "      <td>Joy Crookes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elle Fanning</td>\n",
       "      <td>elle-fanning</td>\n",
       "      <td>Elle Fanning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lucia Keskin</td>\n",
       "      <td>lucia-keskin</td>\n",
       "      <td>Lucia Keskin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ian Smith</td>\n",
       "      <td>ian-smith</td>\n",
       "      <td>Ian Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>Ep 5: Aisling Bea</td>\n",
       "      <td>ep-5-aisling-bea</td>\n",
       "      <td>Aisling Bea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Ep 4: Nish Kumar</td>\n",
       "      <td>ep-4-nish-kumar</td>\n",
       "      <td>Nish Kumar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Ep 3: Richard Osman</td>\n",
       "      <td>ep-3-richard-osman</td>\n",
       "      <td>Richard Osman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>Ep 2: Grace Dent</td>\n",
       "      <td>ep-2-grace-dent</td>\n",
       "      <td>Grace Dent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Ep 1: Scroobius Pip</td>\n",
       "      <td>ep-1-scroobius-pip</td>\n",
       "      <td>Scroobius Pip</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>321 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               raw_title                slug     guest_name\n",
       "0           Kunal Nayyar        kunal-nayyar   Kunal Nayyar\n",
       "1            Joy Crookes         joy-crookes    Joy Crookes\n",
       "2           Elle Fanning        elle-fanning   Elle Fanning\n",
       "3           Lucia Keskin        lucia-keskin   Lucia Keskin\n",
       "4              Ian Smith           ian-smith      Ian Smith\n",
       "..                   ...                 ...            ...\n",
       "316    Ep 5: Aisling Bea    ep-5-aisling-bea    Aisling Bea\n",
       "317     Ep 4: Nish Kumar     ep-4-nish-kumar     Nish Kumar\n",
       "318  Ep 3: Richard Osman  ep-3-richard-osman  Richard Osman\n",
       "319     Ep 2: Grace Dent     ep-2-grace-dent     Grace Dent\n",
       "320  Ep 1: Scroobius Pip  ep-1-scroobius-pip  Scroobius Pip\n",
       "\n",
       "[321 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_slugs_guests_df_from_list_of_dict(titles_list: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes the list of dicts of raw titles, slugs and guest names and returns a dataframe\n",
    "    \"\"\"\n",
    "    df_episodes_metadata = pd.DataFrame(titles_list)\n",
    "    return df_episodes_metadata\n",
    "\n",
    "test_eps_metadata_df = create_slugs_guests_df_from_dict(test_episodes_list[0])\n",
    "test_eps_metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195fbb7d",
   "metadata": {},
   "source": [
    "### Checking for duplicate names (e.g. Ed and James have multiple eps; 100, 200, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "156a1c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             raw_title  ...                                                url\n",
      "18   Ep 300: Ed Gamble and James Acaster (with spec...  ...  https://podscripts.co/podcasts/off-menu-with-e...\n",
      "219  Ep 100: Ed Gamble and James Acaster (with Gues...  ...  https://podscripts.co/podcasts/off-menu-with-e...\n",
      "\n",
      "[2 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "filter_condition = test_eps_metadata_df['guest_name'] == \"Ed Gamble and James Acaster\"\n",
    "\n",
    "# 2. Apply the Filter to the DataFrame\n",
    "# When you pass the filter_condition to the DataFrame, \n",
    "# Pandas only returns the rows where the condition is True.\n",
    "specific_guest_rows = test_eps_metadata_df[filter_condition]\n",
    "\n",
    "# 3. View the results\n",
    "print(specific_guest_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fab82ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_url_from_row(row: pd.Series) -> str:\n",
    "    \"\"\"Creates a podscripts transcript URL from an episode's metadata.\"\"\"\n",
    "    slug = row[\"slug\"]\n",
    "    url = f\"{transcript_base_url}{slug}\"\n",
    "    return url\n",
    "\n",
    "def create_urls_and_save_to_slugs_guests_df(\n",
    "    input_dataframe: pd.DataFrame, output_filepath: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates transcript URLs for a DataFrame of episode metadata and saves it.\n",
    "\n",
    "    This function adds a new column 'url' to the input DataFrame by applying\n",
    "    a helper function to each row. The modified DataFrame is then saved as a\n",
    "    Parquet file to the specified path.\n",
    "\n",
    "    Args:\n",
    "        input_dataframe (pd.DataFrame): The DataFrame containing episode metadata\n",
    "                                        with 'episode_number' and 'guest_name' columns.\n",
    "        output_filepath (str): The full file path where the resulting DataFrame\n",
    "                               will be saved in Parquet format.\n",
    "\n",
    "    Returns:\n",
    "        None: The function modifies the input DataFrame and saves a file to disk,\n",
    "              but does not return a value.\n",
    "    \"\"\"\n",
    "    df = input_dataframe\n",
    "    df[\"url\"] = df.apply(_create_url_from_row, axis=1)\n",
    "    df.to_parquet(output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50c8284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processed_metadata_filepath_for_saving = os.path.join(\n",
    "            V2_tests_dir, \"test_metadata.parquet\")\n",
    "\n",
    "create_urls_and_save_to_slugs_guests_df(test_eps_metadata_df, test_processed_metadata_filepath_for_saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51d88e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_title</th>\n",
       "      <th>slug</th>\n",
       "      <th>guest_name</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kunal Nayyar</td>\n",
       "      <td>kunal-nayyar</td>\n",
       "      <td>Kunal Nayyar</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joy Crookes</td>\n",
       "      <td>joy-crookes</td>\n",
       "      <td>Joy Crookes</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elle Fanning</td>\n",
       "      <td>elle-fanning</td>\n",
       "      <td>Elle Fanning</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lucia Keskin</td>\n",
       "      <td>lucia-keskin</td>\n",
       "      <td>Lucia Keskin</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ian Smith</td>\n",
       "      <td>ian-smith</td>\n",
       "      <td>Ian Smith</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>Ep 5: Aisling Bea</td>\n",
       "      <td>ep-5-aisling-bea</td>\n",
       "      <td>Aisling Bea</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Ep 4: Nish Kumar</td>\n",
       "      <td>ep-4-nish-kumar</td>\n",
       "      <td>Nish Kumar</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Ep 3: Richard Osman</td>\n",
       "      <td>ep-3-richard-osman</td>\n",
       "      <td>Richard Osman</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>Ep 2: Grace Dent</td>\n",
       "      <td>ep-2-grace-dent</td>\n",
       "      <td>Grace Dent</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Ep 1: Scroobius Pip</td>\n",
       "      <td>ep-1-scroobius-pip</td>\n",
       "      <td>Scroobius Pip</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>321 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               raw_title  ...                                                url\n",
       "0           Kunal Nayyar  ...  https://podscripts.co/podcasts/off-menu-with-e...\n",
       "1            Joy Crookes  ...  https://podscripts.co/podcasts/off-menu-with-e...\n",
       "2           Elle Fanning  ...  https://podscripts.co/podcasts/off-menu-with-e...\n",
       "3           Lucia Keskin  ...  https://podscripts.co/podcasts/off-menu-with-e...\n",
       "4              Ian Smith  ...  https://podscripts.co/podcasts/off-menu-with-e...\n",
       "..                   ...  ...                                                ...\n",
       "316    Ep 5: Aisling Bea  ...  https://podscripts.co/podcasts/off-menu-with-e...\n",
       "317     Ep 4: Nish Kumar  ...  https://podscripts.co/podcasts/off-menu-with-e...\n",
       "318  Ep 3: Richard Osman  ...  https://podscripts.co/podcasts/off-menu-with-e...\n",
       "319     Ep 2: Grace Dent  ...  https://podscripts.co/podcasts/off-menu-with-e...\n",
       "320  Ep 1: Scroobius Pip  ...  https://podscripts.co/podcasts/off-menu-with-e...\n",
       "\n",
       "[321 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_eps_metadata_urls = try_read_parquet(test_processed_metadata_filepath_for_saving)\n",
    "test_eps_metadata_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be749730",
   "metadata": {},
   "source": [
    "## Merging restaurant mentions with episodes metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39c122a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurant_name</th>\n",
       "      <th>guest_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Red Chilli</td>\n",
       "      <td>Sophie Duker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Orana</td>\n",
       "      <td>Ian Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barbacoa El Primo</td>\n",
       "      <td>Finn Wolfhard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>La Taberna Del Gourmet</td>\n",
       "      <td>Rhod Gilbert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Gastrobar</td>\n",
       "      <td>James Acaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>Estelle Manor</td>\n",
       "      <td>AJ Odudu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>Partisan</td>\n",
       "      <td>CMAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>The Black Swan</td>\n",
       "      <td>Maisie Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>The Black Swan</td>\n",
       "      <td>Ed Gamble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>Bettys Cafe Tea Rooms</td>\n",
       "      <td>Elis James</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>839 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            restaurant_name     guest_name\n",
       "0                Red Chilli   Sophie Duker\n",
       "1                     Orana      Ian Smith\n",
       "2         Barbacoa El Primo  Finn Wolfhard\n",
       "3    La Taberna Del Gourmet   Rhod Gilbert\n",
       "4             Ron Gastrobar  James Acaster\n",
       "..                      ...            ...\n",
       "738           Estelle Manor       AJ Odudu\n",
       "739                Partisan           CMAT\n",
       "740          The Black Swan    Maisie Adam\n",
       "740          The Black Swan      Ed Gamble\n",
       "741   Bettys Cafe Tea Rooms     Elis James\n",
       "\n",
       "[839 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate res mentions dataframe ready for new merging function\n",
    "\n",
    "# Step 4.1 Create dict of mentions with res name as keys and list of guests who mention as values\n",
    "guests_who_mention_res_by_res_name_dict = create_mentions_by_res_name_dict(\n",
    "            test_restaurants_html_filepath\n",
    ")\n",
    "# Step 4.2 Convert into exploded dataframe (one line per guest who mentions)\n",
    "exploded_res_mentions_df = create_return_exploded_res_mentions_df(\n",
    "    guests_who_mention_res_by_res_name_dict\n",
    ")\n",
    "\n",
    "exploded_res_mentions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d0cb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guest_name</th>\n",
       "      <th>url</th>\n",
       "      <th>slug</th>\n",
       "      <th>restaurants_mentioned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kunal Nayyar</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>kunal-nayyar</td>\n",
       "      <td>[Moti Mahal, The Tamil Prince, The Dover, Kutir]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joy Crookes</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>joy-crookes</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elle Fanning</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>elle-fanning</td>\n",
       "      <td>[Lady M, Red Lobster, Popeyes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lucia Keskin</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>lucia-keskin</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ian Smith</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ian-smith</td>\n",
       "      <td>[Orana, Skál, Mudbrick Vineyard]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>Aisling Bea</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-5-aisling-bea</td>\n",
       "      <td>[Cafe Gratitude, Burger and Lobster]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Nish Kumar</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-4-nish-kumar</td>\n",
       "      <td>[Bademiya, The Owl and The Pussycat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Richard Osman</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-3-richard-osman</td>\n",
       "      <td>[Five Guys, Cora Pearl, Berners Tavern]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>Grace Dent</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-2-grace-dent</td>\n",
       "      <td>[Little Owl, Trullo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Scroobius Pip</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-1-scroobius-pip</td>\n",
       "      <td>[Oli Babas Kerb Camden]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>321 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        guest_name  ...                             restaurants_mentioned\n",
       "0     Kunal Nayyar  ...  [Moti Mahal, The Tamil Prince, The Dover, Kutir]\n",
       "1      Joy Crookes  ...                                                []\n",
       "2     Elle Fanning  ...                    [Lady M, Red Lobster, Popeyes]\n",
       "3     Lucia Keskin  ...                                                []\n",
       "4        Ian Smith  ...                  [Orana, Skál, Mudbrick Vineyard]\n",
       "..             ...  ...                                               ...\n",
       "316    Aisling Bea  ...              [Cafe Gratitude, Burger and Lobster]\n",
       "317     Nish Kumar  ...              [Bademiya, The Owl and The Pussycat]\n",
       "318  Richard Osman  ...           [Five Guys, Cora Pearl, Berners Tavern]\n",
       "319     Grace Dent  ...                              [Little Owl, Trullo]\n",
       "320  Scroobius Pip  ...                           [Oli Babas Kerb Camden]\n",
       "\n",
       "[321 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New merging function\n",
    "\n",
    "def combine_save_mentions_and_ep_metadata_dfs(\n",
    "    exploded_restaurants_guest_df: pd.DataFrame,\n",
    "    ep_metadata_filepath: str,\n",
    "    output_df_filepath: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Takes in exploded (one line per guest/mention) mentions/guest df, and ep metadata (numbers, names, url) dataframe\n",
    "    filepath, and output filepath, and combines the dataframes. The combined dataframe is then saved as a\n",
    "    Parquet file to the specified path.\n",
    "\n",
    "    Args:\n",
    "        exploded_restaurants_guest_df (pd.DataFrame): A dataframe with 1 row for each mention of a restaurant (exploded)\n",
    "        ep_metadata_filepath (str): String filepath for the episode metadata dataframe\n",
    "        output_df_filepath (str): String filepath for where to save the combined dataframe\n",
    "\n",
    "    Returns:\n",
    "        None: The function combines the dataframes, and saves to a parquet.\n",
    "    \"\"\"\n",
    "    # Fetch metadata filepath\n",
    "    df_episodes_metadata = try_read_parquet(ep_metadata_filepath)\n",
    "    # Left merge on guest, with numbers, names, url (df_episodes_metadata)\n",
    "    merged_df = pd.merge(\n",
    "        df_episodes_metadata, exploded_restaurants_guest_df, on=\"guest_name\", how=\"left\"\n",
    "    )\n",
    "    # Aggregating rows so we have one row per episode, with a list of restaurant mentions\n",
    "    # Note groupby creates groups based on the args (three identical in this case). as_index False means also have an index col (don't use first col as index)\n",
    "    # Note .agg aggregates the data, it creates a new col called restaurants mentioned, from the col 'restaurant_name', applying the method 'dropna' to each group (restuarants that were in the restaurant_name cell), dropna gets rid of the NaN's\n",
    "    # Note NaN's are placeholders for missing data (means ilterally not a number, which is confusing as it could be text...)\n",
    "    ep_meta_and_mentions_df = (\n",
    "        merged_df.groupby([\"guest_name\", \"url\", \"slug\"], as_index=False, sort=False)\n",
    "        .agg(restaurants_mentioned=(\"restaurant_name\", lambda x: list(x.dropna())))\n",
    "        .rename(columns={\"restaurant_name\": \"restaurants_mentioned\"})\n",
    "    )\n",
    "    # Save the dataframe\n",
    "    ep_meta_and_mentions_df.to_parquet(output_df_filepath, index=False)\n",
    "\n",
    "\n",
    "# Calling the function\n",
    "\n",
    "test_full_episodes_metadata_path = os.path.join(V2_tests_dir, \"test_episodes_metadata_full.parquet\")\n",
    "\n",
    "combine_save_mentions_and_ep_metadata_dfs(\n",
    "        exploded_res_mentions_df,\n",
    "        test_processed_metadata_filepath_for_saving,\n",
    "        test_full_episodes_metadata_path,\n",
    "    )\n",
    "\n",
    "full_episodes_metadata_test_df = try_read_parquet(test_full_episodes_metadata_path)\n",
    "full_episodes_metadata_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec127737",
   "metadata": {},
   "source": [
    "## New web scraper\n",
    "### Needed to generate test data from the above html + to improve functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c9c29e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced DataFrame created with 11 rows and saved to: c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\ten_test_episodes_full_metadata.parquet\n",
      "Sliced DataFrame created with 11 rows and saved to: c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\second_ten_test_episodes_full_metadata.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guest_name</th>\n",
       "      <th>url</th>\n",
       "      <th>slug</th>\n",
       "      <th>restaurants_mentioned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Ross Noble</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-217-ross-noble-christmas-special</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Nick Frost</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-207-nick-frost</td>\n",
       "      <td>[Geranium, The Red House, The Yellow House]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Jenny Eclair</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-197-jenny-eclair</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Lily Allen</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-187-lily-allen</td>\n",
       "      <td>[Dorian, Afghan Kitchen]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Fern Brady</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-178-fern-brady</td>\n",
       "      <td>[Lune, Chinese Tapas House, Bubala, Fortitude ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Ania Magliano</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-169-ania-magliano</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Felicity Ward</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-159-felicity-ward</td>\n",
       "      <td>[The European, Supper Club, Meatball, Harrys S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Adam Buxton</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-149-adam-buxton</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Nadiya Hussain</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-139-nadiya-hussain</td>\n",
       "      <td>[YO Sushi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Jason Reitman</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-129-jason-reitman</td>\n",
       "      <td>[Shokunin, White Bear, Mikawa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>Jamie Oliver</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-119-jamie-oliver</td>\n",
       "      <td>[Rasa, Craftworks]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         guest_name  ...                              restaurants_mentioned\n",
       "100      Ross Noble  ...                                                 []\n",
       "110      Nick Frost  ...        [Geranium, The Red House, The Yellow House]\n",
       "120    Jenny Eclair  ...                                                 []\n",
       "130      Lily Allen  ...                           [Dorian, Afghan Kitchen]\n",
       "140      Fern Brady  ...  [Lune, Chinese Tapas House, Bubala, Fortitude ...\n",
       "150   Ania Magliano  ...                                                 []\n",
       "160   Felicity Ward  ...  [The European, Supper Club, Meatball, Harrys S...\n",
       "170     Adam Buxton  ...                                                 []\n",
       "180  Nadiya Hussain  ...                                         [YO Sushi]\n",
       "190   Jason Reitman  ...                     [Shokunin, White Bear, Mikawa]\n",
       "200    Jamie Oliver  ...                                 [Rasa, Craftworks]\n",
       "\n",
       "[11 rows x 4 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate test data (full episodes metadata rows 0, 10, 20, 30, 40, 50...100)\n",
    "\n",
    "ten_test_episodes_metadata_output_path = os.path.join(V2_tests_dir, \"ten_test_episodes_full_metadata.parquet\")\n",
    "\n",
    "indices_to_slice = range(0, 101, 10)\n",
    "\n",
    "# 2. Slice the DataFrame by position using .iloc\n",
    "# .iloc stands for 'integer location' and is used for positional indexing.\n",
    "sliced_df = full_episodes_metadata_test_df.iloc[indices_to_slice]\n",
    "\n",
    "# 3. Save the sliced DataFrame to a Parquet file\n",
    "# index=False ensures the default Pandas index (0, 1, 2, ...) is not saved as a column\n",
    "sliced_df.to_parquet(ten_test_episodes_metadata_output_path, index=False)\n",
    "\n",
    "print(f\"Sliced DataFrame created with {len(sliced_df)} rows and saved to: {ten_test_episodes_metadata_output_path}\")\n",
    "sliced_df\n",
    "\n",
    "# Second batch of test data \n",
    "\n",
    "second_ten_test_episodes_metadata_output_path = os.path.join(V2_tests_dir, \"second_ten_test_episodes_full_metadata.parquet\")\n",
    "\n",
    "indices_to_slice_2 = range(100, 201, 10)\n",
    "\n",
    "second_ten_test_data_df = full_episodes_metadata_test_df.iloc[indices_to_slice_2]\n",
    "\n",
    "second_ten_test_data_df.to_parquet(second_ten_test_episodes_metadata_output_path, index=False)\n",
    "\n",
    "print(f\"Sliced DataFrame created with {len(second_ten_test_data_df)} rows and saved to: {second_ten_test_episodes_metadata_output_path}\")\n",
    "\n",
    "second_ten_test_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd5940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old with some edits (unsuccessful)\n",
    "\n",
    "def _save_transcripts_html(eps_dataframe, directory):\n",
    "    \"\"\"\n",
    "    Iterates through a DataFrame of episodes, downloads the HTML content from\n",
    "    the episode URL, and saves it to a specified directory.\n",
    "\n",
    "    Skips files that already exist and includes a random delay to be\n",
    "    polite to the server.\n",
    "\n",
    "    Args:\n",
    "        eps_dataframe (pd.DataFrame): DataFrame containing episode metadata\n",
    "                                      (including 'episode_number' and 'url').\n",
    "        directory (str): The directory to save the HTML files to.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    for index, row in eps_dataframe.iterrows():\n",
    "        guest_name = row[\"guest_name\"]\n",
    "        episode_url = row[\"url\"]\n",
    "        filename = f\"{guest_name}.html\"\n",
    "        filepath = os.path.join(directory, filename)\n",
    "\n",
    "        # Skip episodes that already exist\n",
    "        if os.path.exists(filepath):\n",
    "            print(\n",
    "                f\"  Skipping Episode {guest_name}, at index{index}: File already exists at {filepath}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Delay to be polite to the server and avoid 429 errors\n",
    "        sleep_time = random.uniform(1, 3)  # Sleep for 1 to 3 seconds\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "        html_content_str = extract_html(episode_url)\n",
    "\n",
    "        # Check for None before attempting to save\n",
    "        # The extract_html function returns None on failure (like a 429 error)\n",
    "        if html_content_str:\n",
    "            save_text_to_file(html_content_str, filename, directory)\n",
    "        else:\n",
    "            print(\n",
    "                f\"  Skipping save for Episode {episode_num} due to failed extraction.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa82cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT scraper annotated\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ---- Simple logger ----\n",
    "# logger = logging.getLogger(\"scraper\")\n",
    "if not logger.handlers:\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s %(message)s\"))\n",
    "    logger.addHandler(ch)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# ---- Helper: random-ish UA list (small) ----\n",
    "_SIMPLE_USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "]\n",
    "\n",
    "\n",
    "def _choose_headers():\n",
    "    return {\"User-Agent\": random.choice(_SIMPLE_USER_AGENTS)}\n",
    "\n",
    "\n",
    "# ---- Downloader with retries, backoff, persistence ----\n",
    "def download_transcripts(\n",
    "    url_map: Dict[str, str], # links the url to the guest name via a dict, because it uses name as filename (note needs to use slug as some names repeate.g. ed & james)\n",
    "    out_dir: str, # Directory to save html to\n",
    "    status_path: str, # Path to status JSON file\n",
    "    max_attempts_per_url: int = 5,\n",
    "    backoff_base: float = 1.0,\n",
    "    max_workers: int = 3, # Number of \"workers\" (threads , things that try to run concurrently in a single overarching process)\n",
    "    session: Optional[requests.Session] = None, # The session if we have one open for some reason (single session = more effieicnt)\n",
    "    timeout: float = 12.0,\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Download a set of URLs and save the HTML files locally.\n",
    "\n",
    "    Args:\n",
    "        url_map: mapping slug_or_filename -> url. Eg {\"paul-rudd\": \"https://.../ep-215-paul-rudd\"}\n",
    "                 Or you can map guest_name -> url.\n",
    "        out_dir: directory to save files (will be created).\n",
    "        status_path: path to JSON status file to persist attempts and outcomes.\n",
    "        max_attempts_per_url: maximum attempts per url before giving up.\n",
    "        backoff_base: base seconds for exponential backoff (1.0 is a reasonable default).\n",
    "        max_workers: number of concurrent download workers (1..6 recommended).\n",
    "        session: optional requests.Session() - if None a new one is created.\n",
    "        timeout: request timeout in seconds.\n",
    "\n",
    "    Returns:\n",
    "        status dict mapping key -> { \"url\", \"attempts\", \"status\", \"saved_path\", \"last_error\" }\n",
    "    \"\"\"\n",
    "\n",
    "    out_dir = Path(out_dir) \n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    status_path = Path(status_path) # Turns strings into paths for use saving/reading\n",
    "\n",
    "    # Load existing status if present (allows resume)\n",
    "    if status_path.exists():\n",
    "        try:\n",
    "            with open(status_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                status = json.load(f)\n",
    "        except Exception:\n",
    "            status = {}\n",
    "    else:\n",
    "        status = {}\n",
    "\n",
    "    # Initialize status entries for any missing keys\n",
    "    # Makes statuses for all \"keys\" (guest names/episodes)\n",
    "    for key, url in url_map.items():\n",
    "        if key not in status:\n",
    "            status[key] = {\n",
    "                \"url\": url,\n",
    "                \"attempts\": 0,\n",
    "                \"status\": \"pending\",  # pending | success | failed\n",
    "                \"saved_path\": None,\n",
    "                \"last_error\": None,\n",
    "            }\n",
    "\n",
    "    # Use a single session for pooling - more efficient that starting multiple sessions apparently\n",
    "    session = session or requests.Session()\n",
    "\n",
    "    def _attempt_download(key: str, meta: Dict) -> Dict:\n",
    "        url = meta[\"url\"]\n",
    "        attempts = meta[\"attempts\"]\n",
    "        result = dict(meta)\n",
    "        # If already succeeded, skip\n",
    "        if meta.get(\"status\") == \"success\":\n",
    "            return result\n",
    "\n",
    "        # If we've already reached max attempts, mark failed and skip\n",
    "        if attempts >= max_attempts_per_url:\n",
    "            result[\"status\"] = \"failed\"\n",
    "            result[\"last_error\"] = \"max_attempts_reached\"\n",
    "            return result\n",
    "\n",
    "        try:\n",
    "            # Build headers and request - headers are in the request and say what browser I'm using, we're faking three diff ones to rotate between\n",
    "            # To look less bot like\n",
    "            # Resp is a response object which is what comes back from the request, and contains the html text among other things\n",
    "            headers = _choose_headers()\n",
    "            resp = session.get(url, headers=headers, timeout=timeout)\n",
    "            # If success\n",
    "            if resp.status_code == 200:\n",
    "                # Save file (deterministic name using key)\n",
    "                # note needs changing to slug\n",
    "                filename = f\"{key}.html\"\n",
    "                saved_path = str(out_dir / filename)\n",
    "                with open(saved_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "                    fh.write(resp.text)\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"success\",\n",
    "                    \"saved_path\": saved_path,\n",
    "                    \"last_error\": None,\n",
    "                })\n",
    "                logger.info(\"Saved %s -> %s\", url, saved_path) # The logger lets us know whats going on, better than prints as level of detail can be\n",
    "                # changed dynamically\n",
    "                return result\n",
    "\n",
    "            # Retryable status codes (429 Too Many Requests, 5xx)\n",
    "            if resp.status_code in (429, 500, 502, 503, 504):\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"pending\",\n",
    "                    \"last_error\": f\"status_{resp.status_code}\"\n",
    "                })\n",
    "                logger.warning(\"Retryable HTTP %s for %s (attempt %s)\", resp.status_code, url, attempts + 1)\n",
    "                return result\n",
    "\n",
    "            # Non-retryable: mark failed with info\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"failed\",\n",
    "                \"last_error\": f\"status_{resp.status_code}\"\n",
    "            })\n",
    "            logger.error(\"Non-retryable HTTP %s for %s\", resp.status_code, url)\n",
    "            return result\n",
    "\n",
    "        except requests.RequestException as e: # network level errors, considered retryable\n",
    "            # Network error: retryable\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"pending\",\n",
    "                \"last_error\": repr(e)\n",
    "            })\n",
    "            logger.warning(\"RequestException for %s (attempt %s): %s\", url, attempts + 1, e)\n",
    "            return result\n",
    "\n",
    "    # Worker function wraps attempts + backoff\n",
    "    # Note meta is the status for this key, and it may be changed throuhg attemotin downloads to new meta\n",
    "    def _worker_task(key):\n",
    "        meta = status[key]\n",
    "        # If already success or permanently failed, return\n",
    "        if meta.get(\"status\") == \"success\" or meta.get(\"attempts\", 0) >= max_attempts_per_url:\n",
    "            return key, meta\n",
    "\n",
    "        # attempt download\n",
    "        new_meta = _attempt_download(key, meta)\n",
    "\n",
    "        # If still pending (retry-worthy), sleep exponential backoff before returning\n",
    "        if new_meta[\"status\"] == \"pending\":\n",
    "            # compute sleep: base * 2^(attempts-1) + jitter\n",
    "            sleep = backoff_base * (2 ** (new_meta[\"attempts\"] - 1))\n",
    "            jitter = random.uniform(0, 1.0)\n",
    "            sleep_time = min(sleep + jitter, 60)  # cap at 60s\n",
    "            logger.info(\"Backing off %0.2fs for %s (attempt %s)\", sleep_time, new_meta[\"url\"], new_meta[\"attempts\"])\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        return key, new_meta\n",
    "\n",
    "    # Main loop: do rounds where each round runs up to max_workers concurrent attempts on pending items.\n",
    "    pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "    # List comprehension selects k's to include, where k is the status and v the attempts, if status is pending and attempts below threshhld\n",
    "    round_idx = 0 # counter\n",
    "    while pending_keys:\n",
    "        round_idx += 1\n",
    "        logger.info(\"Download round %d: %d pending\", round_idx, len(pending_keys)) # How does %d work?\n",
    "\n",
    "        # Limit concurrency to not overload server\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futures = {ex.submit(_worker_task, key): key for key in pending_keys} # Futures represent the future result of the task - recall \n",
    "            # recall that threads designed to run tasks concerrently\n",
    "            # Note with... as ex is a context manager, opens/closes the thread pool\n",
    "            # submit passes a task (a certain key/guest name to try and download) to the thread pool\n",
    "            # futures = {} creates a dict where the future objects are keys and the values are the...keys, confusingly        \n",
    "            for fut in as_completed(futures): # as completed yields futuer obkects 1 by 1 as they're completed\n",
    "                key = futures[fut] # This accesses the future\n",
    "                try:\n",
    "                    k, new_meta = fut.result() # This makes k and new meta the results of the future (output of worker task, which is attempted download: a key and new meta which is the status entry for the key, inc. save path for html)\n",
    "                    status[k].update(new_meta)\n",
    "                except Exception as e:\n",
    "                    logger.exception(\"Unhandled exception for key %s: %s\", key, e)\n",
    "                    status[key][\"attempts\"] = status[key].get(\"attempts\", 0) + 1\n",
    "                    status[key][\"last_error\"] = repr(e)\n",
    "\n",
    "        # persist status to disk after every round\n",
    "        try:\n",
    "            with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(status, f, indent=2)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Failed to write status file: %s\", e)\n",
    "\n",
    "        # Prepare next round: only keys still pending and under attempts limit\n",
    "        pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "\n",
    "        # If there are pending keys, optionally small delay between rounds\n",
    "        if pending_keys:\n",
    "            logger.info(\"Sleeping 2s between rounds to be polite...\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    # final persist - saves the JSON again (unsure how dump method works)\n",
    "    with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(status, f, indent=2)\n",
    "\n",
    "    # return status mapping\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445304c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT scraper V2 slug use\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ---- Simple logger ----\n",
    "#logger = logging.getLogger(\"scraper\")\n",
    "if not logger.handlers:\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s %(message)s\"))\n",
    "    logger.addHandler(ch)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# ---- Helper: random-ish UA list (small) ----\n",
    "_SIMPLE_USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "]\n",
    "\n",
    "\n",
    "def _choose_headers():\n",
    "    return {\"User-Agent\": random.choice(_SIMPLE_USER_AGENTS)}\n",
    "\n",
    "\n",
    "def _sanitize_key(key: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert a key (expected to be a slug) into a safe filename slug:\n",
    "      - lowercase\n",
    "      - replace any sequence of characters NOT a-z0-9 or '-' or '_' with '-'\n",
    "      - collapse multiple '-' into one\n",
    "      - strip leading/trailing '-' or '_'\n",
    "    This ensures keys like \"Paul Rudd\" become \"paul-rudd\" and already-correct slugs remain stable.\n",
    "    \"\"\"\n",
    "    if not isinstance(key, str):\n",
    "        key = str(key)\n",
    "    s = key.strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\-_]+\", \"-\", s)\n",
    "    s = re.sub(r\"-{2,}\", \"-\", s)\n",
    "    return s.strip(\"-_\")\n",
    "\n",
    "\n",
    "# ---- Downloader with retries, backoff, persistence ----\n",
    "def download_transcripts(\n",
    "    url_map: Dict[str, str],  # mapping slug_or_filename -> url (keys should be your episode slugs)\n",
    "    out_dir: str,  # Directory to save html to\n",
    "    status_path: str,  # Path to status JSON file\n",
    "    max_attempts_per_url: int = 5,\n",
    "    backoff_base: float = 1.0,\n",
    "    max_workers: int = 3,  # Number of concurrent download workers\n",
    "    session: Optional[requests.Session] = None,  # Optional shared requests.Session\n",
    "    timeout: float = 12.0,\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Download a set of URLs and save the HTML files locally.\n",
    "\n",
    "    Args:\n",
    "        url_map: mapping slug_or_filename -> url. Keys should be the episode slugs you want to use as identifiers.\n",
    "        out_dir: directory to save files (created if missing).\n",
    "        status_path: path to JSON status file to persist attempts and outcomes.\n",
    "        max_attempts_per_url: maximum attempts per url before giving up.\n",
    "        backoff_base: base seconds for exponential backoff.\n",
    "        max_workers: number of concurrent download workers.\n",
    "        session: optional requests.Session() - if None a new one is created.\n",
    "        timeout: request timeout in seconds.\n",
    "\n",
    "    Returns:\n",
    "        status dict mapping key -> { \"url\", \"attempts\", \"status\", \"saved_path\", \"last_error\" }\n",
    "    \"\"\"\n",
    "\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    status_path = Path(status_path)\n",
    "\n",
    "    # Load existing status if present (allows resume)\n",
    "    if status_path.exists():\n",
    "        try:\n",
    "            with open(status_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                status = json.load(f)\n",
    "        except Exception:\n",
    "            status = {}\n",
    "    else:\n",
    "        status = {}\n",
    "\n",
    "    # Initialize status entries for any missing keys\n",
    "    for key, url in url_map.items():\n",
    "        if key not in status:\n",
    "            status[key] = {\n",
    "                \"url\": url,\n",
    "                \"attempts\": 0,\n",
    "                \"status\": \"pending\",  # pending | success | failed\n",
    "                \"saved_path\": None,\n",
    "                \"last_error\": None,\n",
    "            }\n",
    "\n",
    "    # Use a single session for pooling\n",
    "    session = session or requests.Session()\n",
    "\n",
    "    def _attempt_download(key: str, meta: Dict) -> Dict:\n",
    "        url = meta[\"url\"]\n",
    "        attempts = meta[\"attempts\"]\n",
    "        result = dict(meta)\n",
    "        # If already succeeded, skip\n",
    "        if meta.get(\"status\") == \"success\":\n",
    "            return result\n",
    "\n",
    "        # If we've already reached max attempts, mark failed and skip\n",
    "        if attempts >= max_attempts_per_url:\n",
    "            result[\"status\"] = \"failed\"\n",
    "            result[\"last_error\"] = \"max_attempts_reached\"\n",
    "            return result\n",
    "\n",
    "        try:\n",
    "            # Build headers and request\n",
    "            headers = _choose_headers()\n",
    "            resp = session.get(url, headers=headers, timeout=timeout)\n",
    "            # If success\n",
    "            if resp.status_code == 200:\n",
    "                # Save file using sanitized key (ensure filesystem-safe slug)\n",
    "                safe_key = _sanitize_key(key)\n",
    "                filename = f\"{safe_key}.html\"\n",
    "                saved_path = str(out_dir / filename)\n",
    "                with open(saved_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "                    fh.write(resp.text)\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"success\",\n",
    "                    \"saved_path\": saved_path,\n",
    "                    \"last_error\": None,\n",
    "                })\n",
    "                logger.info(\"Saved %s -> %s\", url, saved_path)\n",
    "                return result\n",
    "\n",
    "            # Retryable status codes (429 Too Many Requests, 5xx)\n",
    "            if resp.status_code in (429, 500, 502, 503, 504):\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"pending\",\n",
    "                    \"last_error\": f\"status_{resp.status_code}\"\n",
    "                })\n",
    "                logger.warning(\"Retryable HTTP %s for %s (attempt %s)\", resp.status_code, url, attempts + 1)\n",
    "                return result\n",
    "\n",
    "            # Non-retryable: mark failed with info\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"failed\",\n",
    "                \"last_error\": f\"status_{resp.status_code}\"\n",
    "            })\n",
    "            logger.error(\"Non-retryable HTTP %s for %s\", resp.status_code, url)\n",
    "            return result\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            # Network error: retryable\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"pending\",\n",
    "                \"last_error\": repr(e)\n",
    "            })\n",
    "            logger.warning(\"RequestException for %s (attempt %s): %s\", url, attempts + 1, e)\n",
    "            return result\n",
    "\n",
    "    # Worker function wraps attempts + backoff\n",
    "    def _worker_task(key):\n",
    "        meta = status[key]\n",
    "        # If already success or permanently failed, return\n",
    "        if meta.get(\"status\") == \"success\" or meta.get(\"attempts\", 0) >= max_attempts_per_url:\n",
    "            return key, meta\n",
    "\n",
    "        # attempt download\n",
    "        new_meta = _attempt_download(key, meta)\n",
    "\n",
    "        # If still pending (retry-worthy), sleep exponential backoff before returning\n",
    "        if new_meta[\"status\"] == \"pending\":\n",
    "            # compute sleep: base * 2^(attempts-1) + jitter\n",
    "            sleep = backoff_base * (2 ** (new_meta[\"attempts\"] - 1))\n",
    "            jitter = random.uniform(0, 1.0)\n",
    "            sleep_time = min(sleep + jitter, 60)  # cap at 60s\n",
    "            logger.info(\"Backing off %0.2fs for %s (attempt %s)\", sleep_time, new_meta[\"url\"], new_meta[\"attempts\"])\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        return key, new_meta\n",
    "\n",
    "    # Main loop: do rounds where each round runs up to max_workers concurrent attempts on pending items.\n",
    "    pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "    round_idx = 0\n",
    "    while pending_keys:\n",
    "        round_idx += 1\n",
    "        logger.info(\"Download round %d: %d pending\", round_idx, len(pending_keys))\n",
    "\n",
    "        # Limit concurrency to not overload server\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futures = {ex.submit(_worker_task, key): key for key in pending_keys}\n",
    "            for fut in as_completed(futures):\n",
    "                key = futures[fut]\n",
    "                try:\n",
    "                    k, new_meta = fut.result()\n",
    "                    status[k].update(new_meta)\n",
    "                except Exception as e:\n",
    "                    logger.exception(\"Unhandled exception for key %s: %s\", key, e)\n",
    "                    status[key][\"attempts\"] = status[key].get(\"attempts\", 0) + 1\n",
    "                    status[key][\"last_error\"] = repr(e)\n",
    "\n",
    "        # persist status to disk after every round\n",
    "        try:\n",
    "            with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(status, f, indent=2)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Failed to write status file: %s\", e)\n",
    "\n",
    "        # Prepare next round: only keys still pending and under attempts limit\n",
    "        pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "\n",
    "        # If there are pending keys, optionally small delay between rounds\n",
    "        if pending_keys:\n",
    "            logger.info(\"Sleeping 2s between rounds to be polite...\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    # final persist\n",
    "    with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(status, f, indent=2)\n",
    "\n",
    "    # return status mapping\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcda7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V3 scraper GPT (better log for debugging)\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Dict, Optional\n",
    "from email.utils import parsedate_to_datetime\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# -------------------\n",
    "# Logger configuration helper\n",
    "# -------------------\n",
    "# def configure_logger(log_file: Optional[str] = None, level: int = logging.DEBUG):\n",
    "    \"\"\"\n",
    "    Configure a compact logger for the scraper.\n",
    "    - Console handler always enabled.\n",
    "    - Optional file handler if log_file provided.\n",
    "    - Default level: DEBUG for maximum visibility while testing.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"scraper\")\n",
    "    logger.setLevel(level)\n",
    "\n",
    "    # Avoid adding handlers multiple times when running multiple times in a notebook\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    # Console handler (clear, one-line format)\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(level)\n",
    "    ch.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s: %(message)s\"))\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    # Optional file handler (rotating not necessary here — keep simple)\n",
    "    if log_file:\n",
    "        fh = logging.FileHandler(log_file, encoding=\"utf-8\")\n",
    "        fh.setLevel(level)\n",
    "        fh.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s: %(message)s\"))\n",
    "        logger.addHandler(fh)\n",
    "\n",
    "    return logger\n",
    "\n",
    "# Initialize logger (call this in your notebook before running download_transcripts)\n",
    "logger = configure_logger()  # or configure_logger(\"data/scraper.log\")\n",
    "\n",
    "# -------------------\n",
    "# small sanitize helper (same as before)\n",
    "# -------------------\n",
    "def _sanitize_key(key: str) -> str:\n",
    "    if not isinstance(key, str):\n",
    "        key = str(key)\n",
    "    s = key.strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\-_]+\", \"-\", s)\n",
    "    s = re.sub(r\"-{2,}\", \"-\", s)\n",
    "    return s.strip(\"-_\")\n",
    "\n",
    "# ---- Helper: random-ish UA list (small) ----\n",
    "_SIMPLE_USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "]\n",
    "\n",
    "def _choose_headers():\n",
    "    return {\"User-Agent\": random.choice(_SIMPLE_USER_AGENTS)}\n",
    "\n",
    "# ----- Helper to access retry limits from the server (for use in scraper)\n",
    "\n",
    "def _parse_retry_after(header_value: Optional[str]) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Parse Retry-After header. It can be:\n",
    "      - an integer number of seconds, e.g. \"120\"\n",
    "      - a HTTP-date string, e.g. \"Wed, 21 Oct 2015 07:28:00 GMT\"\n",
    "    Return number of seconds to wait (float), or None if not parseable.\n",
    "    \"\"\"\n",
    "    if not header_value:\n",
    "        return None\n",
    "    header_value = header_value.strip()\n",
    "    # try integer seconds\n",
    "    if header_value.isdigit():\n",
    "        return float(header_value)\n",
    "    # try HTTP-date\n",
    "    try:\n",
    "        dt = parsedate_to_datetime(header_value)\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        now = datetime.now(timezone.utc)\n",
    "        delta = (dt - now).total_seconds()\n",
    "        return max(0.0, float(delta))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# -------------------\n",
    "# download_transcripts with extra logging (no other behavioural changes)\n",
    "# -------------------\n",
    "def download_transcripts(\n",
    "    url_map: Dict[str, str],\n",
    "    out_dir: str,\n",
    "    status_path: str,\n",
    "    max_attempts_per_url: int = 5,\n",
    "    backoff_base: float = 1.0,\n",
    "    max_workers: int = 3,\n",
    "    session: Optional[requests.Session] = None,\n",
    "    timeout: float = 12.0,\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Download URLs to out_dir using url_map (keys are slugs used as filenames).\n",
    "    Added logging provides visibility into what the function does on each run.\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    status_path = Path(status_path)\n",
    "\n",
    "    logger.info(\"Starting download_transcripts: %d urls, out_dir=%s, status_path=%s\",\n",
    "                len(url_map), out_dir, status_path)\n",
    "\n",
    "    # Load existing status if present (allows resume)\n",
    "    if status_path.exists():\n",
    "        try:\n",
    "            with open(status_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                status = json.load(f)\n",
    "            logger.debug(\"Loaded existing status.json with %d entries\", len(status))\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Failed to load status.json (%s). Starting with empty status.\", e)\n",
    "            status = {}\n",
    "    else:\n",
    "        logger.debug(\"No status.json file found at %s. Starting fresh.\", status_path)\n",
    "        status = {}\n",
    "\n",
    "    # Initialize status entries for any missing keys (log each new init)\n",
    "    for key, url in url_map.items():\n",
    "        if key not in status:\n",
    "            status[key] = {\n",
    "                \"url\": url,\n",
    "                \"attempts\": 0,\n",
    "                \"status\": \"pending\",  # pending | success | failed\n",
    "                \"saved_path\": None,\n",
    "                \"last_error\": None,\n",
    "            }\n",
    "            logger.debug(\"Initialized status for key='%s' -> %s\", key, url)\n",
    "\n",
    "    # Use a single session for pooling\n",
    "    session = session or requests.Session()\n",
    "\n",
    "    def _attempt_download(key: str, meta: Dict) -> Dict:\n",
    "        url = meta[\"url\"]\n",
    "        attempts = meta[\"attempts\"]\n",
    "        result = dict(meta)\n",
    "\n",
    "        # If already succeeded, skip and log reason\n",
    "        if meta.get(\"status\") == \"success\":\n",
    "            logger.debug(\"Skipping key='%s' (already success, saved_path=%s)\", key, meta.get(\"saved_path\"))\n",
    "            return result\n",
    "\n",
    "        # If max attempts reached, log and skip\n",
    "        if attempts >= max_attempts_per_url:\n",
    "            result[\"status\"] = \"failed\"\n",
    "            result[\"last_error\"] = \"max_attempts_reached\"\n",
    "            logger.info(\"Key='%s' reached max attempts (%d). Marking failed.\", key, attempts)\n",
    "            return result\n",
    "\n",
    "        # Log the attempt about to be made\n",
    "        logger.debug(\"Attempting key='%s' (attempt %d) -> %s\", key, attempts + 1, url)\n",
    "        try:\n",
    "            headers = _choose_headers()\n",
    "            resp = session.get(url, headers=headers, timeout=timeout)\n",
    "\n",
    "            # If success (200)\n",
    "            if resp.status_code == 200:\n",
    "                safe_key = _sanitize_key(key)\n",
    "                filename = f\"{safe_key}.html\"\n",
    "                saved_path = str(out_dir / filename)\n",
    "\n",
    "                # If file already exists, log that we're overwriting (helps debug)\n",
    "                if Path(saved_path).exists():\n",
    "                    logger.debug(\"File %s already exists and will be overwritten by key='%s'\", saved_path, key)\n",
    "\n",
    "                with open(saved_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "                    fh.write(resp.text)\n",
    "\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"success\",\n",
    "                    \"saved_path\": saved_path,\n",
    "                    \"last_error\": None,\n",
    "                })\n",
    "                logger.info(\"Saved %s -> %s (key=%s)\", url, saved_path, key)\n",
    "                return result\n",
    "\n",
    "            # Retryable status codes\n",
    "            if resp.status_code in (429, 500, 502, 503, 504):\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"pending\",\n",
    "                    \"last_error\": f\"status_{resp.status_code}\"\n",
    "                })\n",
    "                logger.warning(\"Retryable HTTP %s for key='%s' url=%s (attempt %s)\",\n",
    "                               resp.status_code, key, url, attempts + 1)\n",
    "                # Log headers optionally for 429 to see Retry-After\n",
    "                if resp.status_code == 429:\n",
    "                    ra = resp.headers.get(\"Retry-After\")\n",
    "                    logger.debug(\"429 response headers for key='%s': Retry-After=%s\", key, ra)\n",
    "                return result\n",
    "\n",
    "            # Non-retryable\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"failed\",\n",
    "                \"last_error\": f\"status_{resp.status_code}\"\n",
    "            })\n",
    "            logger.error(\"Non-retryable HTTP %s for key='%s' url=%s\", resp.status_code, key, url)\n",
    "            return result\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            # Network error: retryable\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"pending\",\n",
    "                \"last_error\": repr(e)\n",
    "            })\n",
    "            logger.warning(\"RequestException for key='%s' url=%s (attempt %s): %s\", key, url, attempts + 1, e)\n",
    "            return result\n",
    "\n",
    "    # Worker wrapper with backoff\n",
    "    def _worker_task(key):\n",
    "        meta = status[key]\n",
    "        if meta.get(\"status\") == \"success\" or meta.get(\"attempts\", 0) >= max_attempts_per_url:\n",
    "            return key, meta\n",
    "\n",
    "        new_meta = _attempt_download(key, meta)\n",
    "\n",
    "        if new_meta[\"status\"] == \"pending\":\n",
    "            sleep = backoff_base * (2 ** (new_meta[\"attempts\"] - 1))\n",
    "            jitter = random.uniform(0, 1.0)\n",
    "            sleep_time = min(sleep + jitter, 60)\n",
    "            logger.debug(\"Backing off %0.2fs for key='%s' (attempt %s)\", sleep_time, key, new_meta[\"attempts\"])\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        return key, new_meta\n",
    "\n",
    "    # Main loop\n",
    "    pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "    round_idx = 0\n",
    "    while pending_keys:\n",
    "        round_idx += 1\n",
    "        logger.info(\"Download round %d: %d pending\", round_idx, len(pending_keys))\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futures = {ex.submit(_worker_task, key): key for key in pending_keys}\n",
    "            for fut in as_completed(futures):\n",
    "                key = futures[fut]\n",
    "                try:\n",
    "                    k, new_meta = fut.result()\n",
    "                    status[k].update(new_meta)\n",
    "                except Exception as e:\n",
    "                    logger.exception(\"Unhandled exception for key %s: %s\", key, e)\n",
    "                    status[key][\"attempts\"] = status[key].get(\"attempts\", 0) + 1\n",
    "                    status[key][\"last_error\"] = repr(e)\n",
    "\n",
    "        # persist status to disk after every round\n",
    "        try:\n",
    "            with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(status, f, indent=2)\n",
    "            logger.debug(\"Persisted status.json (round %d).\", round_idx)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Failed to write status file: %s\", e)\n",
    "\n",
    "        # Prepare next round\n",
    "        pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "\n",
    "        if pending_keys:\n",
    "            logger.info(\"Sleeping 2s between rounds to be polite...\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    # final persist and summary\n",
    "    with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(status, f, indent=2)\n",
    "\n",
    "    # Final summary counts\n",
    "    succ = sum(1 for v in status.values() if v.get(\"status\") == \"success\")\n",
    "    failed = sum(1 for v in status.values() if v.get(\"status\") == \"failed\")\n",
    "    pending = sum(1 for v in status.values() if v.get(\"status\") == \"pending\")\n",
    "    logger.info(\"Download finished. success=%d failed=%d pending=%d\", succ, failed, pending)\n",
    "\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ecf65b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V4 scraper GPT (collects server wait times from the server and uses these to prevent overload errors)\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Dict, Optional\n",
    "from email.utils import parsedate_to_datetime\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# -------------------\n",
    "# Logger configuration helper\n",
    "# -------------------\n",
    "def configure_logger(log_file: Optional[str] = None, level: int = logging.DEBUG):\n",
    "    \"\"\"\n",
    "    Configure a compact logger for the scraper.\n",
    "    - Console handler always enabled.\n",
    "    - Optional file handler if log_file provided.\n",
    "    - Default level: DEBUG for maximum visibility while testing.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"scraper\")\n",
    "    logger.setLevel(level)\n",
    "\n",
    "    # Avoid adding handlers multiple times when running multiple times in a notebook\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    # Console handler (clear, one-line format)\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(level)\n",
    "    ch.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s: %(message)s\"))\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    # Optional file handler (rotating not necessary here — keep simple)\n",
    "    if log_file:\n",
    "        fh = logging.FileHandler(log_file, encoding=\"utf-8\")\n",
    "        fh.setLevel(level)\n",
    "        fh.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s: %(message)s\"))\n",
    "        logger.addHandler(fh)\n",
    "\n",
    "    return logger\n",
    "\n",
    "# Initialize logger (call this in your notebook before running download_transcripts)\n",
    "logger = configure_logger()  # or configure_logger(\"data/scraper.log\")\n",
    "\n",
    "# -------------------\n",
    "# small sanitize helper (same as before)\n",
    "# -------------------\n",
    "def _sanitize_key(key: str) -> str:\n",
    "    if not isinstance(key, str):\n",
    "        key = str(key)\n",
    "    s = key.strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\-_]+\", \"-\", s)\n",
    "    s = re.sub(r\"-{2,}\", \"-\", s)\n",
    "    return s.strip(\"-_\")\n",
    "\n",
    "# ---- Helper: random-ish UA list (small) ----\n",
    "_SIMPLE_USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "]\n",
    "\n",
    "def _choose_headers():\n",
    "    return {\"User-Agent\": random.choice(_SIMPLE_USER_AGENTS)}\n",
    "\n",
    "# ----- Helper to access retry limits from the server (for use in scraper)\n",
    "def _parse_retry_after(header_value: Optional[str]) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Parse Retry-After header. It can be:\n",
    "      - an integer number of seconds, e.g. \"120\"\n",
    "      - a HTTP-date string, e.g. \"Wed, 21 Oct 2015 07:28:00 GMT\"\n",
    "    Return number of seconds to wait (float), or None if not parseable.\n",
    "    \"\"\"\n",
    "    if not header_value:\n",
    "        return None\n",
    "    header_value = header_value.strip()\n",
    "    # try integer seconds\n",
    "    if header_value.isdigit():\n",
    "        try:\n",
    "            return float(header_value)\n",
    "        except Exception:\n",
    "            return None\n",
    "    # try HTTP-date\n",
    "    try:\n",
    "        dt = parsedate_to_datetime(header_value)\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        now = datetime.now(timezone.utc)\n",
    "        delta = (dt - now).total_seconds()\n",
    "        return max(0.0, float(delta))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# -------------------\n",
    "# download_transcripts with extra logging (no other behavioural changes)\n",
    "# -------------------\n",
    "def download_transcripts(\n",
    "    url_map: Dict[str, str],\n",
    "    out_dir: str,\n",
    "    status_path: str,\n",
    "    max_attempts_per_url: int = 5,\n",
    "    backoff_base: float = 1.0,\n",
    "    max_workers: int = 3,\n",
    "    session: Optional[requests.Session] = None,\n",
    "    timeout: float = 12.0,\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Download URLs to out_dir using url_map (keys are slugs used as filenames).\n",
    "    Added logging provides visibility into what the function does on each run.\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    status_path = Path(status_path)\n",
    "\n",
    "    logger.info(\"Starting download_transcripts: %d urls, out_dir=%s, status_path=%s\",\n",
    "                len(url_map), out_dir, status_path)\n",
    "\n",
    "    # Load existing status if present (allows resume)\n",
    "    if status_path.exists():\n",
    "        try:\n",
    "            with open(status_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                status = json.load(f)\n",
    "            logger.debug(\"Loaded existing status.json with %d entries\", len(status))\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Failed to load status.json (%s). Starting with empty status.\", e)\n",
    "            status = {}\n",
    "    else:\n",
    "        logger.debug(\"No status.json file found at %s. Starting fresh.\", status_path)\n",
    "        status = {}\n",
    "\n",
    "    # Initialize status entries for any missing keys (log each new init)\n",
    "    for key, url in url_map.items():\n",
    "        if key not in status:\n",
    "            status[key] = {\n",
    "                \"url\": url,\n",
    "                \"attempts\": 0,\n",
    "                \"status\": \"pending\",  # pending | success | failed\n",
    "                \"saved_path\": None,\n",
    "                \"last_error\": None,\n",
    "            }\n",
    "            logger.debug(\"Initialized status for key='%s' -> %s\", key, url)\n",
    "\n",
    "    # Use a single session for pooling\n",
    "    session = session or requests.Session()\n",
    "\n",
    "    def _attempt_download(key: str, meta: Dict) -> Dict:\n",
    "        url = meta[\"url\"]\n",
    "        attempts = meta[\"attempts\"]\n",
    "        result = dict(meta)\n",
    "\n",
    "        # If already succeeded, skip and log reason\n",
    "        if meta.get(\"status\") == \"success\":\n",
    "            logger.debug(\"Skipping key='%s' (already success, saved_path=%s)\", key, meta.get(\"saved_path\"))\n",
    "            return result\n",
    "\n",
    "        # If max attempts reached, log and skip\n",
    "        if attempts >= max_attempts_per_url:\n",
    "            result[\"status\"] = \"failed\"\n",
    "            result[\"last_error\"] = \"max_attempts_reached\"\n",
    "            logger.info(\"Key='%s' reached max attempts (%d). Marking failed.\", key, attempts)\n",
    "            return result\n",
    "\n",
    "        # Log the attempt about to be made\n",
    "        logger.debug(\"Attempting key='%s' (attempt %d) -> %s\", key, attempts + 1, url)\n",
    "        try:\n",
    "            headers = _choose_headers()\n",
    "            resp = session.get(url, headers=headers, timeout=timeout)\n",
    "\n",
    "            # If success (200)\n",
    "            if resp.status_code == 200:\n",
    "                safe_key = _sanitize_key(key)\n",
    "                filename = f\"{safe_key}.html\"\n",
    "                saved_path = str(out_dir / filename)\n",
    "\n",
    "                # If file already exists, log that we're overwriting (helps debug)\n",
    "                if Path(saved_path).exists():\n",
    "                    logger.debug(\"File %s already exists and will be overwritten by key='%s'\", saved_path, key)\n",
    "\n",
    "                with open(saved_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "                    fh.write(resp.text)\n",
    "\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"success\",\n",
    "                    \"saved_path\": saved_path,\n",
    "                    \"last_error\": None,\n",
    "                })\n",
    "                logger.info(\"Saved %s -> %s (key=%s)\", url, saved_path, key)\n",
    "                return result\n",
    "\n",
    "            # Retryable status codes\n",
    "            if resp.status_code in (429, 500, 502, 503, 504):\n",
    "                # Parse Retry-After header if present and include in result\n",
    "                retry_after_raw = resp.headers.get(\"Retry-After\")\n",
    "                retry_after_seconds = _parse_retry_after(retry_after_raw)\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"pending\",\n",
    "                    \"last_error\": f\"status_{resp.status_code}\",\n",
    "                    \"retry_after_seconds\": retry_after_seconds,\n",
    "                })\n",
    "                logger.warning(\"Retryable HTTP %s for key='%s' url=%s (attempt %s)\",\n",
    "                               resp.status_code, key, url, attempts + 1)\n",
    "                # Log headers optionally for 429 to see Retry-After\n",
    "                if resp.status_code == 429:\n",
    "                    logger.debug(\"429 response headers for key='%s': Retry-After=%s\", key, retry_after_raw)\n",
    "                    logger.debug(\"Parsed Retry-After seconds for key='%s': %s\", key, retry_after_seconds)\n",
    "                return result\n",
    "\n",
    "            # Non-retryable\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"failed\",\n",
    "                \"last_error\": f\"status_{resp.status_code}\"\n",
    "            })\n",
    "            logger.error(\"Non-retryable HTTP %s for key='%s' url=%s\", resp.status_code, key, url)\n",
    "            return result\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            # Network error: retryable\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"pending\",\n",
    "                \"last_error\": repr(e)\n",
    "            })\n",
    "            logger.warning(\"RequestException for key='%s' url=%s (attempt %s): %s\", key, url, attempts + 1, e)\n",
    "            return result\n",
    "\n",
    "    # Worker wrapper with backoff\n",
    "    def _worker_task(key):\n",
    "        meta = status[key]\n",
    "        if meta.get(\"status\") == \"success\" or meta.get(\"attempts\", 0) >= max_attempts_per_url:\n",
    "            return key, meta\n",
    "\n",
    "        new_meta = _attempt_download(key, meta)\n",
    "\n",
    "        if new_meta[\"status\"] == \"pending\":\n",
    "            # computed exponential backoff (what we would do)\n",
    "            comp_sleep = backoff_base * (2 ** (new_meta[\"attempts\"] - 1))\n",
    "            jitter = random.uniform(0, 1.0)\n",
    "            computed_sleep = comp_sleep + jitter\n",
    "\n",
    "            # server-provided advice (if any)\n",
    "            retry_after = new_meta.get(\"retry_after_seconds\")\n",
    "            if retry_after is not None:\n",
    "                # use the server's suggestion if it's longer than our computed wait\n",
    "                sleep_time = max(computed_sleep, float(retry_after))\n",
    "            else:\n",
    "                sleep_time = computed_sleep\n",
    "\n",
    "            # cap to avoid runaway sleeps (adjust cap as desired)\n",
    "            sleep_time = min(sleep_time, 600.0)\n",
    "\n",
    "            logger.info(\"Backing off %0.2fs for key='%s' (attempt %s) [computed=%0.2fs, server=%s]\",\n",
    "                        sleep_time, key, new_meta[\"attempts\"], computed_sleep, retry_after)\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        return key, new_meta\n",
    "\n",
    "    # Main loop\n",
    "    pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "    round_idx = 0\n",
    "    while pending_keys:\n",
    "        round_idx += 1\n",
    "        logger.info(\"Download round %d: %d pending\", round_idx, len(pending_keys))\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futures = {ex.submit(_worker_task, key): key for key in pending_keys}\n",
    "            for fut in as_completed(futures):\n",
    "                key = futures[fut]\n",
    "                try:\n",
    "                    k, new_meta = fut.result()\n",
    "                    status[k].update(new_meta)\n",
    "                except Exception as e:\n",
    "                    logger.exception(\"Unhandled exception for key %s: %s\", key, e)\n",
    "                    status[key][\"attempts\"] = status[key].get(\"attempts\", 0) + 1\n",
    "                    status[key][\"last_error\"] = repr(e)\n",
    "\n",
    "        # persist status to disk after every round\n",
    "        try:\n",
    "            with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(status, f, indent=2)\n",
    "            logger.debug(\"Persisted status.json (round %d).\", round_idx)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Failed to write status file: %s\", e)\n",
    "\n",
    "        # Prepare next round\n",
    "        pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "\n",
    "        if pending_keys:\n",
    "            logger.info(\"Sleeping 2s between rounds to be polite...\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    # final persist and summary\n",
    "    with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(status, f, indent=2)\n",
    "\n",
    "    # Final summary counts\n",
    "    succ = sum(1 for v in status.values() if v.get(\"status\") == \"success\")\n",
    "    failed = sum(1 for v in status.values() if v.get(\"status\") == \"failed\")\n",
    "    pending = sum(1 for v in status.values() if v.get(\"status\") == \"pending\")\n",
    "    logger.info(\"Download finished. success=%d failed=%d pending=%d\", succ, failed, pending)\n",
    "\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "64bd5ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,\n",
       " [('kunal-nayyar',\n",
       "   'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/kunal-nayyar'),\n",
       "  ('mawaan-rizwan',\n",
       "   'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/mawaan-rizwan'),\n",
       "  ('ep-298-james-norton-in-partnership-with-dexcom',\n",
       "   'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-298-james-norton-in-partnership-with-dexcom')])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First Test Batch new scraper\n",
    "\n",
    "\n",
    "# Cell A — Build url_map\n",
    "# Replace base_url below if you need to construct URLs from slugs:\n",
    "base_url = \"https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster\"\n",
    "\n",
    "# If sliced_df has a 'url' column already:\n",
    "if \"url\" in sliced_df.columns:\n",
    "    url_map = {row[\"slug\"]: row[\"url\"] for _, row in sliced_df.iterrows()}\n",
    "else:\n",
    "    # build urls by joining base_url and slug (only do this if that matches the website)\n",
    "    url_map = {row[\"slug\"]: base_url.rstrip(\"/\") + \"/\" + row[\"slug\"].lstrip(\"/\") for _, row in sliced_df.iterrows()}\n",
    "\n",
    "len(url_map), list(url_map.items())[:3]  # quick check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1c133e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 11:57:28,920 INFO: Starting download_transcripts: 11 urls, out_dir=c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts, status_path=c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\status.json\n",
      "2025-11-26 11:57:28,925 DEBUG: Loaded existing status.json with 11 entries\n",
      "2025-11-26 11:57:28,935 INFO: Download finished. success=11 failed=0 pending=0\n"
     ]
    }
   ],
   "source": [
    "# First Test Batch new scraper - Cell B — run download_transcripts\n",
    "out_dir = os.path.join(V2_tests_dir, \"test_transcripts\")      # where HTMLs will be saved \n",
    "status_path = os.path.join(out_dir, \"status.json\")\n",
    "\n",
    "# tune these for a polite test run\n",
    "max_attempts_per_url = 8\n",
    "backoff_base = 2.0\n",
    "max_workers = 2   # start low while testing\n",
    "\n",
    "# call the function (assumes download_transcripts is in scope)\n",
    "status = download_transcripts(\n",
    "    url_map=url_map,\n",
    "    out_dir=out_dir,\n",
    "    status_path=status_path,\n",
    "    max_attempts_per_url=max_attempts_per_url,\n",
    "    backoff_base=backoff_base,\n",
    "    max_workers=max_workers,\n",
    "    timeout=12.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1dc56ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,\n",
       " [('ep-217-ross-noble-christmas-special',\n",
       "   'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-217-ross-noble-christmas-special'),\n",
       "  ('ep-207-nick-frost',\n",
       "   'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-207-nick-frost'),\n",
       "  ('ep-197-jenny-eclair',\n",
       "   'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-197-jenny-eclair')])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second test batch new scraper\n",
    "\n",
    "# Cell A — Build url_map\n",
    "# Replace base_url below if you need to construct URLs from slugs:\n",
    "base_url = \"https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster\"\n",
    "\n",
    "# If second_ten_test_data_df has a 'url' column already:\n",
    "if \"url\" in second_ten_test_data_df.columns:\n",
    "    url_map_2 = {row[\"slug\"]: row[\"url\"] for _, row in second_ten_test_data_df.iterrows()}\n",
    "else:\n",
    "    # build urls by joining base_url and slug (only do this if that matches the website)\n",
    "    url_map_2 = {row[\"slug\"]: base_url.rstrip(\"/\") + \"/\" + row[\"slug\"].lstrip(\"/\") for _, row in second_ten_test_data_df.iterrows()}\n",
    "\n",
    "len(url_map_2), list(url_map_2.items())[:3]  # quick check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fda992d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 11:57:42,535 INFO: Starting download_transcripts: 11 urls, out_dir=c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts, status_path=c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\status.json\n",
      "2025-11-26 11:57:42,538 DEBUG: Loaded existing status.json with 11 entries\n",
      "2025-11-26 11:57:42,539 DEBUG: Initialized status for key='ep-207-nick-frost' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-207-nick-frost\n",
      "2025-11-26 11:57:42,541 DEBUG: Initialized status for key='ep-197-jenny-eclair' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-197-jenny-eclair\n",
      "2025-11-26 11:57:42,541 DEBUG: Initialized status for key='ep-187-lily-allen' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-187-lily-allen\n",
      "2025-11-26 11:57:42,542 DEBUG: Initialized status for key='ep-178-fern-brady' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-178-fern-brady\n",
      "2025-11-26 11:57:42,542 DEBUG: Initialized status for key='ep-169-ania-magliano' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-169-ania-magliano\n",
      "2025-11-26 11:57:42,543 DEBUG: Initialized status for key='ep-159-felicity-ward' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-159-felicity-ward\n",
      "2025-11-26 11:57:42,543 DEBUG: Initialized status for key='ep-149-adam-buxton' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-149-adam-buxton\n",
      "2025-11-26 11:57:42,547 DEBUG: Initialized status for key='ep-139-nadiya-hussain' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-139-nadiya-hussain\n",
      "2025-11-26 11:57:42,547 DEBUG: Initialized status for key='ep-129-jason-reitman' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-129-jason-reitman\n",
      "2025-11-26 11:57:42,547 DEBUG: Initialized status for key='ep-119-jamie-oliver' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-119-jamie-oliver\n",
      "2025-11-26 11:57:42,547 INFO: Download round 1: 10 pending\n",
      "2025-11-26 11:57:42,590 DEBUG: Attempting key='ep-207-nick-frost' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-207-nick-frost\n",
      "2025-11-26 11:57:42,602 DEBUG: Attempting key='ep-197-jenny-eclair' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-197-jenny-eclair\n",
      "2025-11-26 11:57:43,886 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-207-nick-frost -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-207-nick-frost.html (key=ep-207-nick-frost)\n",
      "2025-11-26 11:57:43,887 DEBUG: Attempting key='ep-187-lily-allen' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-187-lily-allen\n",
      "2025-11-26 11:57:43,956 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-197-jenny-eclair -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-197-jenny-eclair.html (key=ep-197-jenny-eclair)\n",
      "2025-11-26 11:57:43,957 DEBUG: Attempting key='ep-178-fern-brady' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-178-fern-brady\n",
      "2025-11-26 11:57:44,652 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-178-fern-brady -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-178-fern-brady.html (key=ep-178-fern-brady)\n",
      "2025-11-26 11:57:44,668 DEBUG: Attempting key='ep-169-ania-magliano' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-169-ania-magliano\n",
      "2025-11-26 11:57:45,569 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-169-ania-magliano -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-169-ania-magliano.html (key=ep-169-ania-magliano)\n",
      "2025-11-26 11:57:45,569 DEBUG: Attempting key='ep-159-felicity-ward' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-159-felicity-ward\n",
      "2025-11-26 11:57:45,807 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-187-lily-allen -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-187-lily-allen.html (key=ep-187-lily-allen)\n",
      "2025-11-26 11:57:45,815 DEBUG: Attempting key='ep-149-adam-buxton' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-149-adam-buxton\n",
      "2025-11-26 11:57:46,485 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-159-felicity-ward -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-159-felicity-ward.html (key=ep-159-felicity-ward)\n",
      "2025-11-26 11:57:46,487 DEBUG: Attempting key='ep-139-nadiya-hussain' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-139-nadiya-hussain\n",
      "2025-11-26 11:57:46,541 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-149-adam-buxton -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-149-adam-buxton.html (key=ep-149-adam-buxton)\n",
      "2025-11-26 11:57:46,544 DEBUG: Attempting key='ep-129-jason-reitman' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-129-jason-reitman\n",
      "2025-11-26 11:57:47,197 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-139-nadiya-hussain -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-139-nadiya-hussain.html (key=ep-139-nadiya-hussain)\n",
      "2025-11-26 11:57:47,198 DEBUG: Attempting key='ep-119-jamie-oliver' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-119-jamie-oliver\n",
      "2025-11-26 11:57:47,672 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-129-jason-reitman -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-129-jason-reitman.html (key=ep-129-jason-reitman)\n",
      "2025-11-26 11:57:48,074 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-119-jamie-oliver -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-119-jamie-oliver.html (key=ep-119-jamie-oliver)\n",
      "2025-11-26 11:57:48,090 DEBUG: Persisted status.json (round 1).\n",
      "2025-11-26 11:57:48,090 INFO: Download finished. success=21 failed=0 pending=0\n"
     ]
    }
   ],
   "source": [
    "# Second Test Batch new scraper - Cell B — run download_transcripts\n",
    "out_dir = os.path.join(V2_tests_dir, \"test_transcripts\")      # where HTMLs will be saved \n",
    "status_path = os.path.join(out_dir, \"status.json\")\n",
    "\n",
    "# tune these for a polite test run\n",
    "max_attempts_per_url = 8\n",
    "backoff_base = 2.0\n",
    "max_workers = 2   # start low while testing\n",
    "\n",
    "# call the function (assumes download_transcripts is in scope)\n",
    "status = download_transcripts(\n",
    "    url_map=url_map_2,\n",
    "    out_dir=out_dir,\n",
    "    status_path=status_path,\n",
    "    max_attempts_per_url=max_attempts_per_url,\n",
    "    backoff_base=backoff_base,\n",
    "    max_workers=max_workers,\n",
    "    timeout=12.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orchestration function for the scraper - notes (everything we need to do first)\n",
    "\n",
    "logger = configure_logger()\n",
    "\n",
    "# If sliced_df has a 'url' column already:\n",
    "if \"url\" in sliced_df.columns:\n",
    "    url_map = {row[\"slug\"]: row[\"url\"] for _, row in sliced_df.iterrows()}\n",
    "else:\n",
    "    # build urls by joining base_url and slug (only do this if that matches the website)\n",
    "    url_map = {row[\"slug\"]: base_url.rstrip(\"/\") + \"/\" + row[\"slug\"].lstrip(\"/\") for _, row in sliced_df.iterrows()}\n",
    "\n",
    "out_dir = os.path.join(V2_tests_dir, \"test_transcripts\")      # where HTMLs will be saved \n",
    "status_path = os.path.join(out_dir, \"status.json\")\n",
    "max_attempts_per_url = 8\n",
    "backoff_base = 2.0\n",
    "max_workers = 2   # start low while testing\n",
    "\n",
    "# call the function (assumes download_transcripts is in scope)\n",
    "status = download_transcripts(\n",
    "    url_map=url_map_2,\n",
    "    out_dir=out_dir,\n",
    "    status_path=status_path,\n",
    "    max_attempts_per_url=max_attempts_per_url,\n",
    "    backoff_base=backoff_base,\n",
    "    max_workers=max_workers,\n",
    "    timeout=12.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "54b2b5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT orchestration function\n",
    "\n",
    "def orchestrate_scraper(\n",
    "    df,                     # DataFrame with 'slug' and optionally 'url'\n",
    "    base_url,               # base URL for constructing URLs if df has no 'url' column\n",
    "    out_dir,                # folder to save HTML transcripts\n",
    "    max_attempts_per_url=5,\n",
    "    backoff_base=1.0,\n",
    "    max_workers=3,\n",
    "    timeout=12.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Orchestrates the scraping process:\n",
    "      1. Prepares a slug → URL map\n",
    "      2. Ensures output folder exists\n",
    "      3. Calls download_transcripts() with sensible defaults\n",
    "      4. Returns the status dict for all downloads\n",
    "    \"\"\"\n",
    "    # ---------------------\n",
    "    # Setup logger for this run\n",
    "    # ---------------------\n",
    "    logger = configure_logger()\n",
    "    logger.info(\"Starting scraper orchestration for %d episodes\", len(df))\n",
    "\n",
    "    # ---------------------\n",
    "    # Prepare URL map\n",
    "    # ---------------------\n",
    "    if \"url\" in df.columns:\n",
    "        url_map = {row[\"slug\"]: row[\"url\"] for _, row in df.iterrows()}\n",
    "        logger.info(\"Using existing URLs from DataFrame\")\n",
    "    else:\n",
    "        url_map = {row[\"slug\"]: base_url.rstrip(\"/\") + \"/\" + row[\"slug\"].lstrip(\"/\") for _, row in df.iterrows()}\n",
    "        logger.info(\"Constructed URLs from base_url and slugs\")\n",
    "\n",
    "    # ---------------------\n",
    "    # Ensure output folder exists\n",
    "    # ---------------------\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    status_path = out_dir / \"status.json\"\n",
    "\n",
    "    # ---------------------\n",
    "    # Call the scraper\n",
    "    # ---------------------\n",
    "    logger.info(\"Running download_transcripts with %d URLs\", len(url_map))\n",
    "    status = download_transcripts(\n",
    "        url_map=url_map,\n",
    "        out_dir=out_dir,\n",
    "        status_path=status_path,\n",
    "        max_attempts_per_url=max_attempts_per_url,\n",
    "        backoff_base=backoff_base,\n",
    "        max_workers=max_workers,\n",
    "        timeout=timeout\n",
    "    )\n",
    "\n",
    "    logger.info(\"Scraper orchestration finished\")\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "20e8635b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 12:24:23,863 INFO: Starting scraper orchestration for 11 episodes\n",
      "2025-11-26 12:24:23,866 INFO: Using existing URLs from DataFrame\n",
      "2025-11-26 12:24:23,869 INFO: Running download_transcripts with 11 URLs\n",
      "2025-11-26 12:24:23,870 INFO: Starting download_transcripts: 11 urls, out_dir=c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts, status_path=c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\status.json\n",
      "2025-11-26 12:24:23,893 DEBUG: Loaded existing status.json with 21 entries\n",
      "2025-11-26 12:24:23,893 INFO: Download finished. success=21 failed=0 pending=0\n",
      "2025-11-26 12:24:23,893 INFO: Scraper orchestration finished\n"
     ]
    }
   ],
   "source": [
    "# Test orchstrator\n",
    "\n",
    "status = orchestrate_scraper(\n",
    "    df=second_ten_test_data_df,\n",
    "    base_url=\"https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster\",\n",
    "    out_dir=os.path.join(V2_tests_dir, \"test_transcripts\"),\n",
    "    max_attempts_per_url=8,\n",
    "    backoff_base=2.0,\n",
    "    max_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15374254",
   "metadata": {},
   "source": [
    "## New extracting clean text and timestamps ; combining into dataframe\n",
    "\n",
    "### Do we need to change any prior functions for the slug change? No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4900179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_timestamps_as_list_of_dicts(\n",
    "    transcript_str: str, slug: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Finds all 'starting point is HH:MM:SS' timestamps in a transcript string.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of dictionaries, where each dict contains the episode\n",
    "                               number, timestamp string, and its starting index.\n",
    "    \"\"\"\n",
    "    timestamp_pattern = re.compile(r\"starting point is (\\d{2}:\\d{2}:\\d{2})\")\n",
    "    all_timestamps_in_transcript = []\n",
    "    for match in timestamp_pattern.finditer(transcript_str):\n",
    "        # Get the captured timestamp string (e.g., \"00:00:05\")\n",
    "        actual_time_string = match.group(1)\n",
    "        # We use group(1) because that's our (HH:MM:SS) part, group(0) refers to the whole string by default\n",
    "\n",
    "        # Get the starting index of the entire match\n",
    "        start_position_in_text = match.start()\n",
    "        # Store this as a dict with episode_slug as key\n",
    "        stamp_dict = {\n",
    "            \"slug\": slug,\n",
    "            \"timestamp\": actual_time_string,\n",
    "            \"start_index\": start_position_in_text,\n",
    "        }\n",
    "        # Store this extracted data (the timestamp string and its position)\n",
    "        all_timestamps_in_transcript.append(stamp_dict)\n",
    "    return all_timestamps_in_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f27907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_save_clean_text_and_periodic_timestamps(\n",
    "    full_episodes_metadata_path: str, transcripts_dir: str, output_filepath: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Takes the full episodes metadata filepath, the transcripts html directory, and an output filepath, and iterates\n",
    "    through the episodes, processing the html into clean transcript text and collating the periodic timestamps.\n",
    "\n",
    "    These transcripts and periodic timestamps are saved in a dataframe, which is saves as a parquet file to the\n",
    "    output filepath.\n",
    "\n",
    "    Args:\n",
    "        full_episodes_metadata_path (str): The full episodes metadata dataframe filepath\n",
    "        transcripts_dir (str): The directory containing the html of each episode.\n",
    "        output_filepath (str): The filepath the output df is saved to.\n",
    "    Returns:\n",
    "        None: A dataframe containing the clean text and the timestamps (a list of Dicts) is saved to the\n",
    "        output filepath as a parquet.\n",
    "    \"\"\"\n",
    "    # 1. Load episodes meta_data\n",
    "    episodes_df = try_read_parquet(full_episodes_metadata_path)\n",
    "    if episodes_df is None or episodes_df.empty:\n",
    "        print(\n",
    "            \"  ERROR: Input episode metadata is missing or empty. Cannot process transcripts.\"\n",
    "        )\n",
    "        raise ValueError(\"No episodes to process.\")\n",
    "\n",
    "    processed_records = []  # To store data for the final DataFrame\n",
    "\n",
    "    # 2. Iterate through each episode's metadata\n",
    "    for index, row in episodes_df.iterrows():\n",
    "        episode_slug = row.get(\"slug\")\n",
    "        guest_name = row.get(\"guest_name\")\n",
    "        transcript_filename = f\"{episode_slug}.html\"\n",
    "        transcript_filepath = os.path.join(transcripts_dir, transcript_filename)\n",
    "\n",
    "        # Confirm file exists and skip if not\n",
    "        if not os.path.exists(transcript_filepath):\n",
    "            print(\n",
    "                f\"  WARNING: Transcript file not found for Episode {guest_name}, slug: {episode_slug} at {transcript_filepath}. Skipping.\"\n",
    "            )\n",
    "            continue  # Skip to the next episode\n",
    "        try:\n",
    "            clean_transcript_str = _clean_transcript_str_from_html(transcript_filepath)\n",
    "            timestamps = _extract_timestamps_as_list_of_dicts(\n",
    "                clean_transcript_str, episode_slug\n",
    "            )\n",
    "\n",
    "            processed_records.append(\n",
    "                {\n",
    "                    \"slug\": episode_slug,\n",
    "                    \"guest_name\": guest_name,\n",
    "                    \"clean_transcript_text\": clean_transcript_str,\n",
    "                    \"periodic_timestamps\": timestamps,  # This will be a list of dictionaries\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"  Processed Episode {episode_slug} ({guest_name}): Extracted text and {len(timestamps)} timestamps.\"\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"  ERROR: Failed to process transcript for Episode {episode_slug} ({guest_name}) from {transcript_filepath}: {e}\"\n",
    "            )\n",
    "            continue  # For MVP, just skip and warn\n",
    "\n",
    "        if processed_records:\n",
    "            result_df = pd.DataFrame(processed_records)\n",
    "            result_df.to_parquet(output_filepath, index=False)\n",
    "            print(\n",
    "                f\"Successfully saved clean transcripts and timestamps for {len(result_df)} episodes to {output_filepath}\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                \"No transcripts were successfully processed. Output DataFrame will be empty.\"\n",
    "            )\n",
    "            pd.DataFrame().to_parquet(output_filepath, index=False)  # Save an empty DF\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Combining episode metadata with transcripts and timestamps\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def combine_timestamps_and_metadata(\n",
    "    transcripts_timestamps_filepath: str, metadata_filepath: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads and combines the transcripts and timestamps dataframe with the metadata dataframe.\n",
    "\n",
    "    Args:\n",
    "        transcripts_timestamps_filepath(str)\n",
    "        metadata_filepath (str)\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe containing episode slug, restaurants mentioned, clean transcript,\n",
    "        and timestamps.\n",
    "    \"\"\"\n",
    "    metadata_df = try_read_parquet(metadata_filepath)\n",
    "    transcripts_timestamps_df = try_read_parquet(transcripts_timestamps_filepath)\n",
    "    combined_df = transcripts_timestamps_df.merge(\n",
    "        metadata_df[[\"slug\", \"restaurants_mentioned\"]],\n",
    "        on=\"slug\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd8936a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed Episode kunal-nayyar (Kunal Nayyar): Extracted text and 257 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 1 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\ten_test_timestamps.parquet\n",
      "  Processed Episode mawaan-rizwan (Mawaan Rizwan): Extracted text and 294 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 2 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\ten_test_timestamps.parquet\n",
      "  Processed Episode ep-298-james-norton-in-partnership-with-dexcom (James Norton): Extracted text and 241 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 3 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\ten_test_timestamps.parquet\n",
      "  Processed Episode ep-287-santiago-lastra (Santiago Lastra): Extracted text and 175 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 4 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\ten_test_timestamps.parquet\n",
      "  Processed Episode ep-277-mo-gilligan (Mo Gilligan): Extracted text and 229 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 5 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\ten_test_timestamps.parquet\n",
      "  Processed Episode ep-267-danny-dyer (Danny Dyer): Extracted text and 200 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 6 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\ten_test_timestamps.parquet\n",
      "  Processed Episode ep-257-amy-annette (Amy Annette): Extracted text and 211 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 7 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\ten_test_timestamps.parquet\n",
      "  Processed Episode ep-247-ardal-ohanlon-live-in-dublin (Ardal O'Hanlon): Extracted text and 193 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 8 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\ten_test_timestamps.parquet\n",
      "  Processed Episode ep-237-lucy-beaumont-live-in-manchester (Lucy Beaumont): Extracted text and 163 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 9 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\ten_test_timestamps.parquet\n",
      "  Processed Episode ep-227-john-robins-live-in-bristol (John Robins): Extracted text and 183 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 10 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\ten_test_timestamps.parquet\n",
      "  Processed Episode ep-217-ross-noble-christmas-special (Ross Noble): Extracted text and 270 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 11 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\ten_test_timestamps.parquet\n"
     ]
    }
   ],
   "source": [
    "test_timestamps_out_path = os.path.join(V2_tests_dir, \"ten_test_timestamps.parquet\")\n",
    "\n",
    "extract_save_clean_text_and_periodic_timestamps(ten_test_episodes_metadata_output_path, out_dir, test_timestamps_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15da589f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_slug</th>\n",
       "      <th>guest_name</th>\n",
       "      <th>clean_transcript_text</th>\n",
       "      <th>periodic_timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kunal-nayyar</td>\n",
       "      <td>Kunal Nayyar</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "      <td>[{'slug': 'kunal-nayyar', 'start_index': 0, 't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mawaan-rizwan</td>\n",
       "      <td>Mawaan Rizwan</td>\n",
       "      <td>starting point is 00:00:00 james, huge news fr...</td>\n",
       "      <td>[{'slug': 'mawaan-rizwan', 'start_index': 0, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ep-298-james-norton-in-partnership-with-dexcom</td>\n",
       "      <td>James Norton</td>\n",
       "      <td>starting point is 00:00:00 welcome to the off-...</td>\n",
       "      <td>[{'slug': 'ep-298-james-norton-in-partnership-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ep-287-santiago-lastra</td>\n",
       "      <td>Santiago Lastra</td>\n",
       "      <td>starting point is 00:00:00 huge news from off-...</td>\n",
       "      <td>[{'slug': 'ep-287-santiago-lastra', 'start_ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ep-277-mo-gilligan</td>\n",
       "      <td>Mo Gilligan</td>\n",
       "      <td>starting point is 00:00:00 today's episode of ...</td>\n",
       "      <td>[{'slug': 'ep-277-mo-gilligan', 'start_index':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ep-267-danny-dyer</td>\n",
       "      <td>Danny Dyer</td>\n",
       "      <td>starting point is 00:00:00 hello, i'm amy gled...</td>\n",
       "      <td>[{'slug': 'ep-267-danny-dyer', 'start_index': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ep-257-amy-annette</td>\n",
       "      <td>Amy Annette</td>\n",
       "      <td>starting point is 00:00:00 welcome to the off ...</td>\n",
       "      <td>[{'slug': 'ep-257-amy-annette', 'start_index':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ep-247-ardal-ohanlon-live-in-dublin</td>\n",
       "      <td>Ardal O'Hanlon</td>\n",
       "      <td>starting point is 00:00:00 acas powers the wor...</td>\n",
       "      <td>[{'slug': 'ep-247-ardal-ohanlon-live-in-dublin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ep-237-lucy-beaumont-live-in-manchester</td>\n",
       "      <td>Lucy Beaumont</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "      <td>[{'slug': 'ep-237-lucy-beaumont-live-in-manche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ep-227-john-robins-live-in-bristol</td>\n",
       "      <td>John Robins</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "      <td>[{'slug': 'ep-227-john-robins-live-in-bristol'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ep-217-ross-noble-christmas-special</td>\n",
       "      <td>Ross Noble</td>\n",
       "      <td>starting point is 00:00:00 greetings off menu ...</td>\n",
       "      <td>[{'slug': 'ep-217-ross-noble-christmas-special...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      episode_slug  ...                                periodic_timestamps\n",
       "0                                     kunal-nayyar  ...  [{'slug': 'kunal-nayyar', 'start_index': 0, 't...\n",
       "1                                    mawaan-rizwan  ...  [{'slug': 'mawaan-rizwan', 'start_index': 0, '...\n",
       "2   ep-298-james-norton-in-partnership-with-dexcom  ...  [{'slug': 'ep-298-james-norton-in-partnership-...\n",
       "3                           ep-287-santiago-lastra  ...  [{'slug': 'ep-287-santiago-lastra', 'start_ind...\n",
       "4                               ep-277-mo-gilligan  ...  [{'slug': 'ep-277-mo-gilligan', 'start_index':...\n",
       "5                                ep-267-danny-dyer  ...  [{'slug': 'ep-267-danny-dyer', 'start_index': ...\n",
       "6                               ep-257-amy-annette  ...  [{'slug': 'ep-257-amy-annette', 'start_index':...\n",
       "7              ep-247-ardal-ohanlon-live-in-dublin  ...  [{'slug': 'ep-247-ardal-ohanlon-live-in-dublin...\n",
       "8          ep-237-lucy-beaumont-live-in-manchester  ...  [{'slug': 'ep-237-lucy-beaumont-live-in-manche...\n",
       "9               ep-227-john-robins-live-in-bristol  ...  [{'slug': 'ep-227-john-robins-live-in-bristol'...\n",
       "10             ep-217-ross-noble-christmas-special  ...  [{'slug': 'ep-217-ross-noble-christmas-special...\n",
       "\n",
       "[11 rows x 4 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspectinf timestamps df\n",
    "\n",
    "timestamps_transcripts_test_df = try_read_parquet(test_timestamps_out_path)\n",
    "timestamps_transcripts_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ce13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slug</th>\n",
       "      <th>guest_name</th>\n",
       "      <th>clean_transcript_text</th>\n",
       "      <th>periodic_timestamps</th>\n",
       "      <th>restaurants_mentioned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kunal-nayyar</td>\n",
       "      <td>Kunal Nayyar</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "      <td>[{'slug': 'kunal-nayyar', 'start_index': 0, 't...</td>\n",
       "      <td>[Moti Mahal, The Tamil Prince, The Dover, Kutir]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mawaan-rizwan</td>\n",
       "      <td>Mawaan Rizwan</td>\n",
       "      <td>starting point is 00:00:00 james, huge news fr...</td>\n",
       "      <td>[{'slug': 'mawaan-rizwan', 'start_index': 0, '...</td>\n",
       "      <td>[Ambala]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ep-298-james-norton-in-partnership-with-dexcom</td>\n",
       "      <td>James Norton</td>\n",
       "      <td>starting point is 00:00:00 welcome to the off-...</td>\n",
       "      <td>[{'slug': 'ep-298-james-norton-in-partnership-...</td>\n",
       "      <td>[Goldeneye, Belindas, The Ham Yard Hotel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ep-287-santiago-lastra</td>\n",
       "      <td>Santiago Lastra</td>\n",
       "      <td>starting point is 00:00:00 huge news from off-...</td>\n",
       "      <td>[{'slug': 'ep-287-santiago-lastra', 'start_ind...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ep-277-mo-gilligan</td>\n",
       "      <td>Mo Gilligan</td>\n",
       "      <td>starting point is 00:00:00 today's episode of ...</td>\n",
       "      <td>[{'slug': 'ep-277-mo-gilligan', 'start_index':...</td>\n",
       "      <td>[Roka, Bagel King]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ep-267-danny-dyer</td>\n",
       "      <td>Danny Dyer</td>\n",
       "      <td>starting point is 00:00:00 hello, i'm amy gled...</td>\n",
       "      <td>[{'slug': 'ep-267-danny-dyer', 'start_index': ...</td>\n",
       "      <td>[Wimpy, Eastenders Kebab, Wilsons Fish and Chips]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ep-257-amy-annette</td>\n",
       "      <td>Amy Annette</td>\n",
       "      <td>starting point is 00:00:00 welcome to the off ...</td>\n",
       "      <td>[{'slug': 'ep-257-amy-annette', 'start_index':...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ep-247-ardal-ohanlon-live-in-dublin</td>\n",
       "      <td>Ardal O'Hanlon</td>\n",
       "      <td>starting point is 00:00:00 acas powers the wor...</td>\n",
       "      <td>[{'slug': 'ep-247-ardal-ohanlon-live-in-dublin...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ep-237-lucy-beaumont-live-in-manchester</td>\n",
       "      <td>Lucy Beaumont</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "      <td>[{'slug': 'ep-237-lucy-beaumont-live-in-manche...</td>\n",
       "      <td>[Restaurant Story]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ep-227-john-robins-live-in-bristol</td>\n",
       "      <td>John Robins</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "      <td>[{'slug': 'ep-227-john-robins-live-in-bristol'...</td>\n",
       "      <td>[Schwartzs, Heritage]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ep-217-ross-noble-christmas-special</td>\n",
       "      <td>Ross Noble</td>\n",
       "      <td>starting point is 00:00:00 greetings off menu ...</td>\n",
       "      <td>[{'slug': 'ep-217-ross-noble-christmas-special...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              slug  ...                              restaurants_mentioned\n",
       "0                                     kunal-nayyar  ...   [Moti Mahal, The Tamil Prince, The Dover, Kutir]\n",
       "1                                    mawaan-rizwan  ...                                           [Ambala]\n",
       "2   ep-298-james-norton-in-partnership-with-dexcom  ...          [Goldeneye, Belindas, The Ham Yard Hotel]\n",
       "3                           ep-287-santiago-lastra  ...                                                 []\n",
       "4                               ep-277-mo-gilligan  ...                                 [Roka, Bagel King]\n",
       "5                                ep-267-danny-dyer  ...  [Wimpy, Eastenders Kebab, Wilsons Fish and Chips]\n",
       "6                               ep-257-amy-annette  ...                                                 []\n",
       "7              ep-247-ardal-ohanlon-live-in-dublin  ...                                                 []\n",
       "8          ep-237-lucy-beaumont-live-in-manchester  ...                                 [Restaurant Story]\n",
       "9               ep-227-john-robins-live-in-bristol  ...                              [Schwartzs, Heritage]\n",
       "10             ep-217-ross-noble-christmas-special  ...                                                 []\n",
       "\n",
       "[11 rows x 5 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining with metadata and inspecting\n",
    "\n",
    "combined_timestamps_metadata_df = combine_timestamps_and_metadata(test_timestamps_out_path, ten_test_episodes_metadata_output_path)\n",
    "combined_timestamps_metadata_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c00816b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuzzy functions\n",
    "\n",
    "def _create_list_tuple_clean_sen_og_sen_og_index(\n",
    "    text: str,\n",
    ") -> List[Tuple[str, str, int]]:\n",
    "    \"\"\"\n",
    "    Takes in a clean transcript string, and creates a list of tuples containing cleaned sentences\n",
    "    for fuzzymatching, original sentences and starting index for locating quotes.\n",
    "\n",
    "    Splits text using delimiter \". \". Assumes no sentences start with puntuation (leading spaces are the only shift from the start of the original to the start\n",
    "    of the cleaned sentence).\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, str, int]]: a list containing a tuple, with cleaned sentence, original\n",
    "                                    stripped sentence, and true start index (the start index of the original sentence,\n",
    "                                    in the original text).\n",
    "\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    current_idx_in_original = 0  # This tracks our position in the original 'text'\n",
    "\n",
    "    # Split into 'segments' (what will become sentences) by full stop/space.\n",
    "    segments = text.split(\". \")\n",
    "\n",
    "    for i, segment in enumerate(\n",
    "        segments\n",
    "    ):  # Note enumerate is a way to loop and get index (rather than a manual counter)\n",
    "        original_full_sentence_segment = segment\n",
    "        # Calculate the actual start index of the content within the segment itself (after stripping leading/trailing spaces)\n",
    "        # It asssumes the start index (in processes sentence) will only move due to leading spaces\n",
    "        # So, it calculates the original (assuming none start with punctuation), and retains it\n",
    "        # Later, we will use this original index to compare against timestamps\n",
    "        leading_spaces_count = len(original_full_sentence_segment) - len(\n",
    "            original_full_sentence_segment.lstrip()\n",
    "        )\n",
    "        true_start_index = current_idx_in_original + leading_spaces_count\n",
    "\n",
    "        original_sentence_stripped = original_full_sentence_segment.strip()\n",
    "\n",
    "        # Only process if the sentence is not empty after stripping\n",
    "        if original_sentence_stripped:\n",
    "            # Apply original cleaning, explicitly converting to lowercase for fuzzy matching\n",
    "            cleaned_sentence = re.sub(\n",
    "                r\"[^\\w\\s]\", \"\", original_sentence_stripped\n",
    "            ).lower()\n",
    "\n",
    "            # Store cleaned, original, and start index\n",
    "            results.append(\n",
    "                (cleaned_sentence, original_sentence_stripped, true_start_index)\n",
    "            )\n",
    "\n",
    "        # Update current_idx_in_original for the next segment.\n",
    "        # Add the length of the current segment and the delimiter length (2 for \". \").\n",
    "        # This assumes all segments (except possibly the last) were followed by \". \".\n",
    "        current_idx_in_original += len(original_full_sentence_segment)\n",
    "        if (\n",
    "            i < len(segments) - 1\n",
    "        ):  # Only add delimiter length if it's not the last segment\n",
    "            current_idx_in_original += len(\". \")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def _find_timestamp(\n",
    "    original_sentence_start_index: int, transcript_timestamps: List[dict]\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds the nearest timestamp occurring before or at a given sentence index.\n",
    "\n",
    "    This function searches through a list of timestamp dictionaries (which should\n",
    "    be pre-sorted by `start_index`) to find the timestamp that immediately\n",
    "    precedes or is at the start of a matched sentence.\n",
    "\n",
    "    Args:\n",
    "        original_sentence_start_index (int): The starting index of the sentence\n",
    "            in the full transcript string.\n",
    "        transcript_timestamps (List[dict]): A list of dictionaries, where each dict\n",
    "            contains 'start_index' and 'timestamp' for a periodic timestamp.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: The timestamp string (e.g., '00:01:23') if a match is found,\n",
    "                       otherwise returns None.\n",
    "    \"\"\"\n",
    "    if original_sentence_start_index is None:\n",
    "        return None\n",
    "    # Could sort timestamps here for good practice, but should be sorted already\n",
    "    # Reverse-iterate over timestamps to find the \"nearest before or at\"\n",
    "    for timestamp_dict in reversed(transcript_timestamps):\n",
    "        if timestamp_dict[\"start_index\"] <= original_sentence_start_index:\n",
    "            return timestamp_dict[\"timestamp\"]\n",
    "\n",
    "    return None  # If no timestamp found before the quote's starting position (all eps start \"Starting point is 00:00:00\")\n",
    "\n",
    "\n",
    "def _matches_by_res_name_from_list_of_res_names(\n",
    "    restaurant_names: List[str], searchable_sentences: List[str], min_score: int\n",
    ") -> Dict[str, List[Tuple[str, int, int]]]:\n",
    "    \"\"\"\n",
    "    Finds fuzzy matches for a list of restaurant names within a list of cleaned sentences.\n",
    "\n",
    "    This function iterates through each restaurant name and uses fuzzy matching to find\n",
    "    sentences that are a close match. Matches are filtered based on a minimum score.\n",
    "\n",
    "    Args:\n",
    "        restaurant_names (List[str]): A list of restaurant names to search for.\n",
    "        searchable_sentences (List[str]): A list of pre-cleaned sentences to search within.\n",
    "        min_score (int): The minimum fuzzy match score (from 0-100) to consider\n",
    "                         a match valid.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[Tuple[str, int, int]]]: A dictionary where:\n",
    "            - Keys are the restaurant names from `restaurant_names`.\n",
    "            - Values are a list of filtered matches for that restaurant.\n",
    "            - Each match is a tuple containing:\n",
    "                - str: The matched sentence text.\n",
    "                - int: The fuzzy matching score.\n",
    "                - int: The index of the matched sentence in the `searchable_sentences` list.\n",
    "    \"\"\"\n",
    "    filtered_matches_by_string = {}\n",
    "    for res_name in restaurant_names:\n",
    "        matches = process.extract(\n",
    "            res_name, searchable_sentences, scorer=fuzz.partial_ratio, limit=20\n",
    "        )\n",
    "\n",
    "        filtered_matches = []\n",
    "        # --- FIX: Unpack the tuple of 2 items correctly ---\n",
    "        for match_text, score in matches:\n",
    "            if score >= min_score:\n",
    "                # Find the index of the matched sentence in the original list\n",
    "                # We use a try-except block for robustness in case of unexpected data.\n",
    "                try:\n",
    "                    original_sentence_index = searchable_sentences.index(match_text)\n",
    "                    # Append all three pieces of information\n",
    "                    filtered_matches.append(\n",
    "                        (match_text, score, original_sentence_index)\n",
    "                    )\n",
    "                except ValueError:\n",
    "                    # This will happen if the match text isn't found in the list,\n",
    "                    # e.g., due to slight string differences not captured by .index()\n",
    "                    continue\n",
    "\n",
    "        filtered_matches_by_string[res_name] = filtered_matches\n",
    "\n",
    "    return filtered_matches_by_string\n",
    "\n",
    "def find_top_match_and_timestamps(\n",
    "    combined_df: pd.DataFrame, min_match_score: int = 90\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Finds fuzzy matches for restaurant mentions in episode transcripts and associates them with timestamps.\n",
    "\n",
    "    This function iterates through each episode's metadata and transcript data. For each mentioned\n",
    "    restaurant, it performs a fuzzy search within the transcript. It then returns a DataFrame\n",
    "    of the top matches and their corresponding timestamps, or notes if no match was found.\n",
    "\n",
    "    Args:\n",
    "        combined_df (pd.DataFrame): A DataFrame containing episode metadata, cleaned transcripts,\n",
    "                                    and periodic timestamps.\n",
    "        min_match_score (int): The minimum fuzzy match score (0-100) required to consider\n",
    "                               a match valid.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame where each row represents a restaurant mention. It contains\n",
    "                      the following columns:\n",
    "                          - 'slug': The episode slug e.g. ep-217-ross-noble or elle-fanning\n",
    "                          - 'Restaurant': The name of the restaurant mentioned.\n",
    "                          - 'Mention text': The original sentence where the mention was found.\n",
    "                          - 'Match Score': The fuzzy match score.\n",
    "                          - 'Match Type': The type of match (e.g., 'full, over 90' or 'No match found').\n",
    "                          - 'Timestamp': The nearest preceding timestamp for the mention.\n",
    "                          - 'Transcript sample': A short sample of the transcript text.\n",
    "    \"\"\"\n",
    "    all_mentions_collected = []\n",
    "\n",
    "    for index, combined_row in combined_df.iterrows():\n",
    "        slug = combined_row.get(\"slug\")\n",
    "        guest_name = combined_row.get(\"guest_name\")\n",
    "        clean_transcript_text = combined_row.get(\"clean_transcript_text\")\n",
    "        periodic_timestamps = combined_row.get(\"periodic_timestamps\")\n",
    "\n",
    "        restaurants_data = combined_row.get(\"restaurants_mentioned\", [])\n",
    "        transcript_sample = (\n",
    "            clean_transcript_text[:200]\n",
    "            if isinstance(clean_transcript_text, str)\n",
    "            else \"No Transcript Found\"\n",
    "        )\n",
    "\n",
    "        # Unsure what data type the res mentions are, hence need for this\n",
    "        restaurants_list = []\n",
    "        if isinstance(restaurants_data, list):\n",
    "            restaurants_list = restaurants_data\n",
    "        elif isinstance(restaurants_data, np.ndarray) and restaurants_data.size > 0:\n",
    "            # Flatten the array and convert it to a standard Python list of strings\n",
    "            restaurants_raw_list = restaurants_data.flatten().tolist()\n",
    "            restaurants_list = [\n",
    "                name.strip().lower() for name in restaurants_raw_list if name.strip()\n",
    "            ]\n",
    "        elif isinstance(restaurants_data, str):\n",
    "            restaurants_list = [\n",
    "                name.strip() for name in restaurants_data.split(\",\") if name.strip()\n",
    "            ]\n",
    "\n",
    "        if restaurants_list:\n",
    "            episode_sentences_data = _create_list_tuple_clean_sen_og_sen_og_index(\n",
    "                clean_transcript_text\n",
    "            )\n",
    "            searchable_sentences = [\n",
    "                item[0] for item in episode_sentences_data\n",
    "            ]  # This is to select the cleaned sentence from the list of tuple\n",
    "            # of cleaned sentence, original, and true start index that create_sentence_list creates\n",
    "\n",
    "            all_matches_for_episode = _matches_by_res_name_from_list_of_res_names(\n",
    "                restaurants_list, searchable_sentences, 90\n",
    "            )\n",
    "            # --- all_matches_for_episode is a dict with key res_name and value lists of matches (matches r tuples of quote, score)\n",
    "            for (\n",
    "                restaurant_name_query,\n",
    "                match_list_for_query,\n",
    "            ) in all_matches_for_episode.items():\n",
    "                if match_list_for_query:\n",
    "                    top_match = match_list_for_query[0]\n",
    "                    # Unpack the top match's data\n",
    "                    matched_cleaned_text, score, matched_sentence_index = top_match\n",
    "                    original_sentence_data = episode_sentences_data[\n",
    "                        matched_sentence_index\n",
    "                    ]  # This takes you back to episode sentences data for the sentence index\n",
    "                    # Which is a tuple of clean sentence, original, and index of sentence within sen list\n",
    "                    original_sentence_text = original_sentence_data[\n",
    "                        1\n",
    "                    ]  # The og sentence is at index 1 in this tuple\n",
    "                    original_start_index = original_sentence_data[\n",
    "                        2\n",
    "                    ]  # The og start index is at index 2 in this tuple\n",
    "\n",
    "                    timestamp = _find_timestamp(\n",
    "                        original_start_index, periodic_timestamps\n",
    "                    )\n",
    "\n",
    "                    mention = {\n",
    "                        \"Episode ID\": slug,\n",
    "                        \"Restaurant\": restaurant_name_query,\n",
    "                        \"Mention text\": original_sentence_text,\n",
    "                        \"Match Score\": score,\n",
    "                        \"Match Type\": f\"full, over {min_match_score}\",\n",
    "                        \"Timestamp\": timestamp,\n",
    "                        \"transcript_sample\": transcript_sample,\n",
    "                    }\n",
    "                    all_mentions_collected.append(mention)\n",
    "                else:\n",
    "                    null_mention = {\n",
    "                        \"Episode ID\": slug,\n",
    "                        \"Restaurant\": restaurant_name_query,\n",
    "                        \"Mention text\": None,\n",
    "                        \"Match Score\": 0,\n",
    "                        \"Match Type\": \"No match found\",\n",
    "                        \"Timestamp\": None,\n",
    "                        \"transcript_sample\": transcript_sample,\n",
    "                    }\n",
    "                    all_mentions_collected.append(null_mention)\n",
    "        else:\n",
    "            print(\n",
    "                f\"  No raw mentions found in 'restaurants_mentioned' list for Episode {slug}. Skipping\"\n",
    "            )\n",
    "    combined_df = pd.DataFrame(all_mentions_collected)\n",
    "    return combined_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "288d840e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-287-santiago-lastra. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-257-amy-annette. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-247-ardal-ohanlon-live-in-dublin. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-217-ross-noble-christmas-special. Skipping\n",
      "\n",
      "--- TOP COLLECTED ---\n",
      "Top Mentions DataFrame created with 16 rows.\n"
     ]
    }
   ],
   "source": [
    "# Test fuzzymatching\n",
    "\n",
    "# --- Run top matches on the test data ---\n",
    "top_mentions_df = find_top_match_and_timestamps(combined_timestamps_metadata_df , 90)\n",
    "\n",
    "# --- Convert list into dataframe, print output ---\n",
    "\n",
    "print(f\"\\n--- TOP COLLECTED ---\")\n",
    "print(f\"Top Mentions DataFrame created with {len(top_mentions_df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4dbe56b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Episode ID</th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Mention text</th>\n",
       "      <th>Match Score</th>\n",
       "      <th>Match Type</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>transcript_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kunal-nayyar</td>\n",
       "      <td>moti mahal</td>\n",
       "      <td>ah</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>01:00:27</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kunal-nayyar</td>\n",
       "      <td>the tamil prince</td>\n",
       "      <td>there's a pub, an indian pub called the tamil ...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:32:33</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kunal-nayyar</td>\n",
       "      <td>the dover</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>No match found</td>\n",
       "      <td>None</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kunal-nayyar</td>\n",
       "      <td>kutir</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>No match found</td>\n",
       "      <td>None</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mawaan-rizwan</td>\n",
       "      <td>ambala</td>\n",
       "      <td>is there a place where you've had the best fal...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:56:05</td>\n",
       "      <td>starting point is 00:00:00 james, huge news fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ep-298-james-norton-in-partnership-with-dexcom</td>\n",
       "      <td>goldeneye</td>\n",
       "      <td>go goldeneye, honestly, it's the best place in...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:55:49</td>\n",
       "      <td>starting point is 00:00:00 welcome to the off-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ep-298-james-norton-in-partnership-with-dexcom</td>\n",
       "      <td>belindas</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>No match found</td>\n",
       "      <td>None</td>\n",
       "      <td>starting point is 00:00:00 welcome to the off-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ep-298-james-norton-in-partnership-with-dexcom</td>\n",
       "      <td>the ham yard hotel</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>No match found</td>\n",
       "      <td>None</td>\n",
       "      <td>starting point is 00:00:00 welcome to the off-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ep-277-mo-gilligan</td>\n",
       "      <td>roka</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>No match found</td>\n",
       "      <td>None</td>\n",
       "      <td>starting point is 00:00:00 today's episode of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ep-277-mo-gilligan</td>\n",
       "      <td>bagel king</td>\n",
       "      <td>one of my favorites though, is from a place ca...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>01:19:18</td>\n",
       "      <td>starting point is 00:00:00 today's episode of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ep-267-danny-dyer</td>\n",
       "      <td>wimpy</td>\n",
       "      <td>and it would have to be a wimpy's knickerbocke...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>01:01:37</td>\n",
       "      <td>starting point is 00:00:00 hello, i'm amy gled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ep-267-danny-dyer</td>\n",
       "      <td>eastenders kebab</td>\n",
       "      <td>and then there was also another one called eas...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:49:17</td>\n",
       "      <td>starting point is 00:00:00 hello, i'm amy gled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ep-267-danny-dyer</td>\n",
       "      <td>wilsons fish and chips</td>\n",
       "      <td>so</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:16:39</td>\n",
       "      <td>starting point is 00:00:00 hello, i'm amy gled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ep-237-lucy-beaumont-live-in-manchester</td>\n",
       "      <td>restaurant story</td>\n",
       "      <td>now, i think the issue might be, and i might b...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:31:22</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ep-227-john-robins-live-in-bristol</td>\n",
       "      <td>schwartzs</td>\n",
       "      <td>and i went on my own and he took me to schwart...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:55:22</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ep-227-john-robins-live-in-bristol</td>\n",
       "      <td>heritage</td>\n",
       "      <td>it's a coffee shop called heritage</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>01:11:42</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Episode ID  ...                                  transcript_sample\n",
       "0                                     kunal-nayyar  ...  starting point is 00:00:00 oh no, it's james a...\n",
       "1                                     kunal-nayyar  ...  starting point is 00:00:00 oh no, it's james a...\n",
       "2                                     kunal-nayyar  ...  starting point is 00:00:00 oh no, it's james a...\n",
       "3                                     kunal-nayyar  ...  starting point is 00:00:00 oh no, it's james a...\n",
       "4                                    mawaan-rizwan  ...  starting point is 00:00:00 james, huge news fr...\n",
       "5   ep-298-james-norton-in-partnership-with-dexcom  ...  starting point is 00:00:00 welcome to the off-...\n",
       "6   ep-298-james-norton-in-partnership-with-dexcom  ...  starting point is 00:00:00 welcome to the off-...\n",
       "7   ep-298-james-norton-in-partnership-with-dexcom  ...  starting point is 00:00:00 welcome to the off-...\n",
       "8                               ep-277-mo-gilligan  ...  starting point is 00:00:00 today's episode of ...\n",
       "9                               ep-277-mo-gilligan  ...  starting point is 00:00:00 today's episode of ...\n",
       "10                               ep-267-danny-dyer  ...  starting point is 00:00:00 hello, i'm amy gled...\n",
       "11                               ep-267-danny-dyer  ...  starting point is 00:00:00 hello, i'm amy gled...\n",
       "12                               ep-267-danny-dyer  ...  starting point is 00:00:00 hello, i'm amy gled...\n",
       "13         ep-237-lucy-beaumont-live-in-manchester  ...  starting point is 00:00:00 hello, it's ed gamb...\n",
       "14              ep-227-john-robins-live-in-bristol  ...  starting point is 00:00:00 hello, it's ed gamb...\n",
       "15              ep-227-john-robins-live-in-bristol  ...  starting point is 00:00:00 hello, it's ed gamb...\n",
       "\n",
       "[16 rows x 7 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_mentions_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Off Menu Project Venv",
   "language": "python",
   "name": "off_menu_project_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
