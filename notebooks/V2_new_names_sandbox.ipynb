{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "460c7477",
   "metadata": {},
   "source": [
    "## Creating episodes metadata \n",
    "### Metadata consists of:\n",
    " - Formerly: numbers, names, urls\n",
    " - Presently: raw title, slug, guest name, url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bfd3f0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root Set to: c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\n",
      "V2 Test Directory Set to: c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Project Root and Imports ---\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import os.path \n",
    "from pathlib import Path\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz, process # <--- THE CRITICAL FIX\n",
    "import requests\n",
    "\n",
    "# Get the path of the directory containing this notebook (e.g., /project/notebooks)\n",
    "# os.getcwd() typically works well in notebooks for this purpose.\n",
    "notebook_dir = os.getcwd() \n",
    "\n",
    "# Go UP one directory level to find the Project Root (e.g., /project)\n",
    "# NOTE: If your notebook is deeper, you might need another '../'\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "\n",
    "# Add the Project Root to Python's search path (sys.path)\n",
    "# This allows Python to find and import modules like 'utils' and 'off_menu'\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Now, imports should work\n",
    "from off_menu.utils import try_read_html_string_from_filepath, try_read_parquet, extract_html, save_text_to_file\n",
    "from off_menu.config import episodes_list_url, transcript_base_url, restaurants_url\n",
    "from off_menu.data_extraction import extract_and_save_html\n",
    "from off_menu.data_processing import create_mentions_by_res_name_dict, create_return_exploded_res_mentions_df, _clean_transcript_str_from_html\n",
    "\n",
    "# --- 2. Define Data Paths ---\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, \"raw\")\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, \"processed\")\n",
    "ANALYTICS_DATA_DIR = os.path.join(DATA_DIR, \"analytics\")\n",
    "\n",
    "# --- 3. Define and Create Test Temp Directory (V2_tests) ---\n",
    "Test_data_dir = os.path.join(DATA_DIR, \"test_temp\")\n",
    "new_test_folder = \"V2_tests\"\n",
    "V2_tests_dir = os.path.join(Test_data_dir, new_test_folder)\n",
    "\n",
    "# Create the directory structure, avoiding errors if it already exists\n",
    "os.makedirs(V2_tests_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Project Root Set to: {PROJECT_ROOT}\")\n",
    "print(f\"V2 Test Directory Set to: {V2_tests_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea43b26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test HTML data downloaded.\n"
     ]
    }
   ],
   "source": [
    "# --- Access and save test data (all episodes html, all restaurants html) ---\n",
    "    \n",
    "test_episodes_html_filepath = os.path.join(V2_tests_dir, \"episodes.html\")\n",
    "test_restaurants_html_filepath = os.path.join(V2_tests_dir, \"restaurants.html\")\n",
    "\n",
    "extract_and_save_html(episodes_list_url, test_episodes_html_filepath)\n",
    "extract_and_save_html(restaurants_url, test_restaurants_html_filepath)\n",
    "\n",
    "print(\"Test HTML data downloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e3d809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Converting \"create_numbers_names_dict_from_html\" into => create_ep_names_slugs_list_from_html ---\n",
    "# Note: this function calls _create_epnumber_epname_dict, which will also need editing\n",
    "\n",
    "# -------------------------\n",
    "# 1 slugify helper\n",
    "# -------------------------\n",
    "def slugify(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert text to a simple dash-separated, lowercase slug.\n",
    "    Example: \"Richard Herring (Bonus Episode)\" -> \"richard-herring-bonus-episode\"\n",
    "    \"\"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", text or \"\")\n",
    "    # remove parentheses but keep their content separated by space\n",
    "    s = s.replace(\"(\", \" \").replace(\")\", \" \")\n",
    "    # remove all characters except word chars, whitespace and hyphen\n",
    "    s = re.sub(r\"[^\\w\\s-]\", \"\", s)\n",
    "    # collapse whitespace to single dash and strip leading/trailing dashes\n",
    "    s = re.sub(r\"\\s+\", \"-\", s).strip(\"-\")\n",
    "    return s.lower()\n",
    "\n",
    "# -------------------------\n",
    "# 2 extract guest name\n",
    "# -------------------------\n",
    "def extract_guest_name(raw_title: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract guest name using the simple rule:\n",
    "      - split on first colon ':'\n",
    "      - take the right hand side if a separator exists\n",
    "      - remove any trailing parenthetical content e.g. ' (Bonus Episode)'\n",
    "      - strip whitespace\n",
    "    \"\"\"\n",
    "    if not raw_title:\n",
    "        return \"\"\n",
    "\n",
    "    s = raw_title.strip()\n",
    "\n",
    "    # Split on the first recognized separator in the remaining string.\n",
    "    # We prefer colon first as your original method did; then hyphens or em-dash.\n",
    "    if \":\" in s:\n",
    "        parts = s.split(\":\", 1)\n",
    "        candidate = parts[1].strip()\n",
    "    else:\n",
    "        # no separator found: either the whole string *is* the guest (as for new episodes)\n",
    "        candidate = s\n",
    "\n",
    "    # remove any parenthetical content at end or inside e.g \"Name (Live) extra\"\n",
    "    candidate = re.sub(r\"\\(.*?\\)\", \"\", candidate).strip()\n",
    "\n",
    "    # final clean: collapse multiple spaces\n",
    "    candidate = re.sub(r\"\\s+\", \" \", candidate).strip()\n",
    "\n",
    "    return candidate\n",
    "\n",
    "\n",
    "def create_tuple_inc_ep_slugs_guests_list_from_html(html_string: str) -> Tuple[List[Dict[str, Any]], List[str]]:\n",
    "    \"\"\"\n",
    "    Parse episodes HTML and return a tuple:\n",
    "      (\n",
    "        [list of valid episode records],\n",
    "        [list of raw_titles for excluded 'Best of' episodes]\n",
    "      )\n",
    "    \"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html_string, \"html.parser\")\n",
    "    episode_divs = soup.find_all(\"div\", class_=\"image-slide-title\")\n",
    "\n",
    "    # 1. Initialize two separate lists\n",
    "    records: List[Dict[str, Any]] = []\n",
    "    exceptions: List[str] = [] \n",
    "\n",
    "    for div in episode_divs:\n",
    "        raw_title = div.get_text(separator=\" \", strip=True)\n",
    "        \n",
    "        # 2. Check the condition using the string method\n",
    "        if raw_title.startswith(\"Best of\"):\n",
    "            # 3. If it is a \"Best of\" episode, append the title to the exceptions list\n",
    "            exceptions.append(raw_title)\n",
    "            # Skip the rest of the loop for this title and move to the next 'div'\n",
    "            continue\n",
    "        # menus to be buried with exception?\n",
    "        # christmas dinner party exception?\n",
    "            \n",
    "        # If the 'if' condition was false (i.e., it's a regular episode), the code continues here:\n",
    "        \n",
    "        guest_name = extract_guest_name(raw_title)\n",
    "        slug_full = slugify(raw_title)\n",
    "\n",
    "        records.append({\n",
    "            \"raw_title\": raw_title,\n",
    "            \"slug\": slug_full,\n",
    "            \"guest_name\": guest_name\n",
    "        })\n",
    "\n",
    "    # 4. Return both lists as a tuple\n",
    "    return records, exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4be0b537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exceptions list\n",
      "['Best of 2024: Live', 'Best of 2024: Part 2', 'Best of 2024: Part 1', 'Best of 2023: Part 2', 'Best of 2023: Part 1', 'Best of 2022: Part 2', 'Best of 2022: Part 1', 'Best of 2021: Part 2', 'Best of 2021: Part 1', 'Best of 2020', 'Best of 2019']\n",
      "Names List\n",
      "[{'raw_title': 'John Early', 'slug': 'john-early', 'guest_name': 'John Early'}, {'raw_title': 'Kunal Nayyar', 'slug': 'kunal-nayyar', 'guest_name': 'Kunal Nayyar'}, {'raw_title': 'Joy Crookes', 'slug': 'joy-crookes', 'guest_name': 'Joy Crookes'}, {'raw_title': 'Elle Fanning', 'slug': 'elle-fanning', 'guest_name': 'Elle Fanning'}, {'raw_title': 'Lucia Keskin', 'slug': 'lucia-keskin', 'guest_name': 'Lucia Keskin'}, {'raw_title': 'Ian Smith', 'slug': 'ian-smith', 'guest_name': 'Ian Smith'}, {'raw_title': 'Jen Brister (Tasting Menu)', 'slug': 'jen-brister-tasting-menu', 'guest_name': 'Jen Brister'}, {'raw_title': 'Gillian Anderson', 'slug': 'gillian-anderson', 'guest_name': 'Gillian Anderson'}, {'raw_title': 'Greg James', 'slug': 'greg-james', 'guest_name': 'Greg James'}, {'raw_title': 'Rhys James', 'slug': 'rhys-james', 'guest_name': 'Rhys James'}]\n",
      "[{'raw_title': 'Ep 218: Jada Pinkett Smith', 'slug': 'ep-218-jada-pinkett-smith', 'guest_name': 'Jada Pinkett Smith'}, {'raw_title': 'Ep 217: Ross Noble (Christmas Special)', 'slug': 'ep-217-ross-noble-christmas-special', 'guest_name': 'Ross Noble'}, {'raw_title': 'Ep 216: Dawn French (Christmas Special)', 'slug': 'ep-216-dawn-french-christmas-special', 'guest_name': 'Dawn French'}, {'raw_title': 'Ep 215: Paul Rudd', 'slug': 'ep-215-paul-rudd', 'guest_name': 'Paul Rudd'}, {'raw_title': 'Ep 214: Steve-O', 'slug': 'ep-214-steve-o', 'guest_name': 'Steve-O'}, {'raw_title': 'Ep 213: Harriet Kemsley', 'slug': 'ep-213-harriet-kemsley', 'guest_name': 'Harriet Kemsley'}, {'raw_title': 'Ep 212: Garth Marenghi', 'slug': 'ep-212-garth-marenghi', 'guest_name': 'Garth Marenghi'}, {'raw_title': 'Ep 211: Steve Coogan', 'slug': 'ep-211-steve-coogan', 'guest_name': 'Steve Coogan'}, {'raw_title': 'Ep 210: Paapa Essiedu', 'slug': 'ep-210-paapa-essiedu', 'guest_name': 'Paapa Essiedu'}, {'raw_title': 'Ep 209: Dr Maggie Aderin-Pocock', 'slug': 'ep-209-dr-maggie-aderin-pocock', 'guest_name': 'Dr Maggie Aderin-Pocock'}]\n"
     ]
    }
   ],
   "source": [
    "# --- Test: Creating raw title, slug, guest name list of dicts using create_tuple_inc_ep_slugs_guests_list_from_html --- \n",
    "\n",
    "episodes_html_str = try_read_html_string_from_filepath(test_episodes_html_filepath)\n",
    "\n",
    "test_episodes_list = create_tuple_inc_ep_slugs_guests_list_from_html(episodes_html_str)\n",
    "\n",
    "print(\"Exceptions list\")\n",
    "print(test_episodes_list[1])\n",
    "\n",
    "print(\"Names List\")\n",
    "print(test_episodes_list[0][:10])\n",
    "print(test_episodes_list[0][100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d73392a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_title</th>\n",
       "      <th>slug</th>\n",
       "      <th>guest_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Early</td>\n",
       "      <td>john-early</td>\n",
       "      <td>John Early</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kunal Nayyar</td>\n",
       "      <td>kunal-nayyar</td>\n",
       "      <td>Kunal Nayyar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Joy Crookes</td>\n",
       "      <td>joy-crookes</td>\n",
       "      <td>Joy Crookes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elle Fanning</td>\n",
       "      <td>elle-fanning</td>\n",
       "      <td>Elle Fanning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lucia Keskin</td>\n",
       "      <td>lucia-keskin</td>\n",
       "      <td>Lucia Keskin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Ep 5: Aisling Bea</td>\n",
       "      <td>ep-5-aisling-bea</td>\n",
       "      <td>Aisling Bea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Ep 4: Nish Kumar</td>\n",
       "      <td>ep-4-nish-kumar</td>\n",
       "      <td>Nish Kumar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>Ep 3: Richard Osman</td>\n",
       "      <td>ep-3-richard-osman</td>\n",
       "      <td>Richard Osman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Ep 2: Grace Dent</td>\n",
       "      <td>ep-2-grace-dent</td>\n",
       "      <td>Grace Dent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>Ep 1: Scroobius Pip</td>\n",
       "      <td>ep-1-scroobius-pip</td>\n",
       "      <td>Scroobius Pip</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>322 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               raw_title                slug     guest_name\n",
       "0             John Early          john-early     John Early\n",
       "1           Kunal Nayyar        kunal-nayyar   Kunal Nayyar\n",
       "2            Joy Crookes         joy-crookes    Joy Crookes\n",
       "3           Elle Fanning        elle-fanning   Elle Fanning\n",
       "4           Lucia Keskin        lucia-keskin   Lucia Keskin\n",
       "..                   ...                 ...            ...\n",
       "317    Ep 5: Aisling Bea    ep-5-aisling-bea    Aisling Bea\n",
       "318     Ep 4: Nish Kumar     ep-4-nish-kumar     Nish Kumar\n",
       "319  Ep 3: Richard Osman  ep-3-richard-osman  Richard Osman\n",
       "320     Ep 2: Grace Dent     ep-2-grace-dent     Grace Dent\n",
       "321  Ep 1: Scroobius Pip  ep-1-scroobius-pip  Scroobius Pip\n",
       "\n",
       "[322 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Function converting list of dicts (of raw title, slug, guest name) into dataframe + test\n",
    "\n",
    "def create_slugs_guests_df_from_list_of_dict(titles_list: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes the list of dicts of raw titles, slugs and guest names and returns a dataframe\n",
    "    \"\"\"\n",
    "    df_episodes_metadata = pd.DataFrame(titles_list)\n",
    "    return df_episodes_metadata\n",
    "\n",
    "# --- test generating all episodes raw title, slug, guest name dataframe\n",
    "test_eps_metadata_df = create_slugs_guests_df_from_list_of_dict(test_episodes_list[0])\n",
    "test_eps_metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195fbb7d",
   "metadata": {},
   "source": [
    "### Checking for duplicate names (e.g. Ed and James have multiple eps; 100, 200, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "156a1c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             raw_title  \\\n",
      "19   Ep 300: Ed Gamble and James Acaster (with spec...   \n",
      "220  Ep 100: Ed Gamble and James Acaster (with Gues...   \n",
      "\n",
      "                                                  slug  \\\n",
      "19   ep-300-ed-gamble-and-james-acaster-with-specia...   \n",
      "220  ep-100-ed-gamble-and-james-acaster-with-guest-...   \n",
      "\n",
      "                      guest_name  \n",
      "19   Ed Gamble and James Acaster  \n",
      "220  Ed Gamble and James Acaster  \n"
     ]
    }
   ],
   "source": [
    "# --- Test to check for duplicate Ed and James GUEST NAME, confirmed duplicate. Use slugs as unique IDs ---\n",
    "\n",
    "filter_condition = test_eps_metadata_df['guest_name'] == \"Ed Gamble and James Acaster\"\n",
    "\n",
    "# 2. Apply the Filter to the DataFrame\n",
    "# When you pass the filter_condition to the DataFrame, \n",
    "# Pandas only returns the rows where the condition is True.\n",
    "specific_guest_rows = test_eps_metadata_df[filter_condition]\n",
    "\n",
    "# 3. View the results\n",
    "print(specific_guest_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b823a184",
   "metadata": {},
   "source": [
    "## Creating and adding URLs to metadata dataframe (currently consisting of raw title, slug, guest name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6fab82ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Functions to create a URL from a given row, and to orchestrate doing this for each row ---\n",
    "\n",
    "def _create_url_from_row(row: pd.Series) -> str:\n",
    "    \"\"\"Creates a podscripts transcript URL from an episode's metadata.\"\"\"\n",
    "    slug = row[\"slug\"]\n",
    "    url = f\"{transcript_base_url}{slug}\"\n",
    "    return url\n",
    "\n",
    "def create_urls_and_save_to_slugs_guests_df(\n",
    "    input_dataframe: pd.DataFrame, output_filepath: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates transcript URLs for a DataFrame of episode metadata and saves it.\n",
    "\n",
    "    This function adds a new column 'url' to the input DataFrame by applying\n",
    "    a helper function to each row. The modified DataFrame is then saved as a\n",
    "    Parquet file to the specified path.\n",
    "\n",
    "    Args:\n",
    "        input_dataframe (pd.DataFrame): The DataFrame containing episode metadata\n",
    "                                        with 'episode_number' and 'guest_name' columns.\n",
    "        output_filepath (str): The full file path where the resulting DataFrame\n",
    "                               will be saved in Parquet format.\n",
    "\n",
    "    Returns:\n",
    "        None: The function modifies the input DataFrame and saves a file to disk,\n",
    "              but does not return a value.\n",
    "    \"\"\"\n",
    "    df = input_dataframe\n",
    "    df[\"url\"] = df.apply(_create_url_from_row, axis=1)\n",
    "    df.to_parquet(output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "50c8284d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_title</th>\n",
       "      <th>slug</th>\n",
       "      <th>guest_name</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Early</td>\n",
       "      <td>john-early</td>\n",
       "      <td>John Early</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kunal Nayyar</td>\n",
       "      <td>kunal-nayyar</td>\n",
       "      <td>Kunal Nayyar</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Joy Crookes</td>\n",
       "      <td>joy-crookes</td>\n",
       "      <td>Joy Crookes</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elle Fanning</td>\n",
       "      <td>elle-fanning</td>\n",
       "      <td>Elle Fanning</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lucia Keskin</td>\n",
       "      <td>lucia-keskin</td>\n",
       "      <td>Lucia Keskin</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Ep 5: Aisling Bea</td>\n",
       "      <td>ep-5-aisling-bea</td>\n",
       "      <td>Aisling Bea</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Ep 4: Nish Kumar</td>\n",
       "      <td>ep-4-nish-kumar</td>\n",
       "      <td>Nish Kumar</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>Ep 3: Richard Osman</td>\n",
       "      <td>ep-3-richard-osman</td>\n",
       "      <td>Richard Osman</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Ep 2: Grace Dent</td>\n",
       "      <td>ep-2-grace-dent</td>\n",
       "      <td>Grace Dent</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>Ep 1: Scroobius Pip</td>\n",
       "      <td>ep-1-scroobius-pip</td>\n",
       "      <td>Scroobius Pip</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>322 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               raw_title                slug     guest_name  \\\n",
       "0             John Early          john-early     John Early   \n",
       "1           Kunal Nayyar        kunal-nayyar   Kunal Nayyar   \n",
       "2            Joy Crookes         joy-crookes    Joy Crookes   \n",
       "3           Elle Fanning        elle-fanning   Elle Fanning   \n",
       "4           Lucia Keskin        lucia-keskin   Lucia Keskin   \n",
       "..                   ...                 ...            ...   \n",
       "317    Ep 5: Aisling Bea    ep-5-aisling-bea    Aisling Bea   \n",
       "318     Ep 4: Nish Kumar     ep-4-nish-kumar     Nish Kumar   \n",
       "319  Ep 3: Richard Osman  ep-3-richard-osman  Richard Osman   \n",
       "320     Ep 2: Grace Dent     ep-2-grace-dent     Grace Dent   \n",
       "321  Ep 1: Scroobius Pip  ep-1-scroobius-pip  Scroobius Pip   \n",
       "\n",
       "                                                   url  \n",
       "0    https://podscripts.co/podcasts/off-menu-with-e...  \n",
       "1    https://podscripts.co/podcasts/off-menu-with-e...  \n",
       "2    https://podscripts.co/podcasts/off-menu-with-e...  \n",
       "3    https://podscripts.co/podcasts/off-menu-with-e...  \n",
       "4    https://podscripts.co/podcasts/off-menu-with-e...  \n",
       "..                                                 ...  \n",
       "317  https://podscripts.co/podcasts/off-menu-with-e...  \n",
       "318  https://podscripts.co/podcasts/off-menu-with-e...  \n",
       "319  https://podscripts.co/podcasts/off-menu-with-e...  \n",
       "320  https://podscripts.co/podcasts/off-menu-with-e...  \n",
       "321  https://podscripts.co/podcasts/off-menu-with-e...  \n",
       "\n",
       "[322 rows x 4 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Test: Adding URLS to full epiosdes metadata (raw title, slug, guest name) ---\n",
    "\n",
    "# Create filepath, run function\n",
    "test_processed_metadata_filepath_for_saving = os.path.join(\n",
    "            V2_tests_dir, \"test_metadata.parquet\")\n",
    "\n",
    "create_urls_and_save_to_slugs_guests_df(test_eps_metadata_df, test_processed_metadata_filepath_for_saving)\n",
    "\n",
    "# Read dataframe, display dataframe\n",
    "test_eps_metadata_urls = try_read_parquet(test_processed_metadata_filepath_for_saving)\n",
    "test_eps_metadata_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be749730",
   "metadata": {},
   "source": [
    "## Merging restaurant mentions with episodes metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39c122a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurant_name</th>\n",
       "      <th>guest_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Red Chilli</td>\n",
       "      <td>Sophie Duker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Orana</td>\n",
       "      <td>Ian Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barbacoa El Primo</td>\n",
       "      <td>Finn Wolfhard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>La Taberna Del Gourmet</td>\n",
       "      <td>Rhod Gilbert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Gastrobar</td>\n",
       "      <td>James Acaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>Estelle Manor</td>\n",
       "      <td>AJ Odudu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>Partisan</td>\n",
       "      <td>CMAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>The Black Swan</td>\n",
       "      <td>Maisie Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>The Black Swan</td>\n",
       "      <td>Ed Gamble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>Bettys Cafe Tea Rooms</td>\n",
       "      <td>Elis James</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>844 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            restaurant_name     guest_name\n",
       "0                Red Chilli   Sophie Duker\n",
       "1                     Orana      Ian Smith\n",
       "2         Barbacoa El Primo  Finn Wolfhard\n",
       "3    La Taberna Del Gourmet   Rhod Gilbert\n",
       "4             Ron Gastrobar  James Acaster\n",
       "..                      ...            ...\n",
       "742           Estelle Manor       AJ Odudu\n",
       "743                Partisan           CMAT\n",
       "744          The Black Swan    Maisie Adam\n",
       "744          The Black Swan      Ed Gamble\n",
       "745   Bettys Cafe Tea Rooms     Elis James\n",
       "\n",
       "[844 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Generate restaurant mentions dataframe ready for new merging function test --- \n",
    "\n",
    "# Create dict of mentions with res name as keys and list of guests who mention as values\n",
    "guests_who_mention_res_by_res_name_dict = create_mentions_by_res_name_dict(\n",
    "            test_restaurants_html_filepath\n",
    ")\n",
    "# Convert dict of res names and guests who mention into exploded dataframe (one line per guest who mentions the restaurant)\n",
    "exploded_res_mentions_df = create_return_exploded_res_mentions_df(\n",
    "    guests_who_mention_res_by_res_name_dict\n",
    ")\n",
    "\n",
    "exploded_res_mentions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79d0cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Creating new merging function (merging mentions with guest name, url, slug ; slug based) ---\n",
    "\n",
    "def combine_save_mentions_and_ep_metadata_dfs(\n",
    "    exploded_restaurants_guest_df: pd.DataFrame,\n",
    "    ep_metadata_filepath: str,\n",
    "    output_df_filepath: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Takes in exploded (one line per guest/mention) mentions/guest df, and ep metadata (numbers, names, url) dataframe\n",
    "    filepath, and output filepath, and combines the dataframes. The combined dataframe is then saved as a\n",
    "    Parquet file to the specified path.\n",
    "\n",
    "    Args:\n",
    "        exploded_restaurants_guest_df (pd.DataFrame): A dataframe with 1 row for each mention of a restaurant (exploded)\n",
    "        ep_metadata_filepath (str): String filepath for the episode metadata dataframe\n",
    "        output_df_filepath (str): String filepath for where to save the combined dataframe\n",
    "\n",
    "    Returns:\n",
    "        None: The function combines the dataframes, and saves to a parquet.\n",
    "    \"\"\"\n",
    "    # Fetch metadata filepath\n",
    "    df_episodes_metadata = try_read_parquet(ep_metadata_filepath)\n",
    "    # Left merge on guest, with numbers, names, url (df_episodes_metadata)\n",
    "    merged_df = pd.merge(\n",
    "        df_episodes_metadata, exploded_restaurants_guest_df, on=\"guest_name\", how=\"left\"\n",
    "    )\n",
    "    # Aggregating rows so we have one row per episode, with a list of restaurant mentions\n",
    "    # Note groupby creates groups based on the args (three identical in this case). as_index False means also have an index col (don't use first col as index)\n",
    "    # Note .agg aggregates the data, it creates a new col called restaurants mentioned, from the col 'restaurant_name', applying the method 'dropna' to each group (restuarants that were in the restaurant_name cell), dropna gets rid of the NaN's\n",
    "    # Note NaN's are placeholders for missing data (means ilterally not a number, which is confusing as it could be text...)\n",
    "    ep_meta_and_mentions_df = (\n",
    "        merged_df.groupby([\"guest_name\", \"url\", \"slug\"], as_index=False, sort=False)\n",
    "        .agg(restaurants_mentioned=(\"restaurant_name\", lambda x: list(x.dropna())))\n",
    "        .rename(columns={\"restaurant_name\": \"restaurants_mentioned\"})\n",
    "    )\n",
    "    # Save the dataframe\n",
    "    ep_meta_and_mentions_df.to_parquet(output_df_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1684a0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guest_name</th>\n",
       "      <th>url</th>\n",
       "      <th>slug</th>\n",
       "      <th>restaurants_mentioned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Early</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>john-early</td>\n",
       "      <td>[Princes Hot Chicken, Hattie Bs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kunal Nayyar</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>kunal-nayyar</td>\n",
       "      <td>[Moti Mahal, The Tamil Prince, The Dover, Kutir]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Joy Crookes</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>joy-crookes</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elle Fanning</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>elle-fanning</td>\n",
       "      <td>[Lady M, Red Lobster, Popeyes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lucia Keskin</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>lucia-keskin</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Aisling Bea</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-5-aisling-bea</td>\n",
       "      <td>[Cafe Gratitude, Burger and Lobster]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Nish Kumar</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-4-nish-kumar</td>\n",
       "      <td>[Bademiya, The Owl and The Pussycat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>Richard Osman</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-3-richard-osman</td>\n",
       "      <td>[Five Guys, Cora Pearl, Berners Tavern]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Grace Dent</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-2-grace-dent</td>\n",
       "      <td>[Little Owl, Trullo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>Scroobius Pip</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-1-scroobius-pip</td>\n",
       "      <td>[Oli Babas Kerb Camden]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>322 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        guest_name                                                url  \\\n",
       "0       John Early  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "1     Kunal Nayyar  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "2      Joy Crookes  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "3     Elle Fanning  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "4     Lucia Keskin  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "..             ...                                                ...   \n",
       "317    Aisling Bea  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "318     Nish Kumar  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "319  Richard Osman  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "320     Grace Dent  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "321  Scroobius Pip  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "\n",
       "                   slug                             restaurants_mentioned  \n",
       "0            john-early                  [Princes Hot Chicken, Hattie Bs]  \n",
       "1          kunal-nayyar  [Moti Mahal, The Tamil Prince, The Dover, Kutir]  \n",
       "2           joy-crookes                                                []  \n",
       "3          elle-fanning                    [Lady M, Red Lobster, Popeyes]  \n",
       "4          lucia-keskin                                                []  \n",
       "..                  ...                                               ...  \n",
       "317    ep-5-aisling-bea              [Cafe Gratitude, Burger and Lobster]  \n",
       "318     ep-4-nish-kumar              [Bademiya, The Owl and The Pussycat]  \n",
       "319  ep-3-richard-osman           [Five Guys, Cora Pearl, Berners Tavern]  \n",
       "320     ep-2-grace-dent                              [Little Owl, Trullo]  \n",
       "321  ep-1-scroobius-pip                           [Oli Babas Kerb Camden]  \n",
       "\n",
       "[322 rows x 4 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- GENERATE TEST DATA: All episodes metadata combined with res mentions, later slices into test batches --- \n",
    "# --- Test: New merging function - merging full res mentions with full episodes metadata (raw title, slug, guest name, urls)\n",
    "\n",
    "test_full_episodes_metadata_path = os.path.join(V2_tests_dir, \"test_episodes_metadata_full.parquet\")\n",
    "\n",
    "combine_save_mentions_and_ep_metadata_dfs(\n",
    "        exploded_res_mentions_df,\n",
    "        test_processed_metadata_filepath_for_saving,\n",
    "        test_full_episodes_metadata_path,\n",
    "    )\n",
    "\n",
    "full_episodes_metadata_test_df = try_read_parquet(test_full_episodes_metadata_path)\n",
    "full_episodes_metadata_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec127737",
   "metadata": {},
   "source": [
    "## New web scraper\n",
    "### Needed to generate test data from the above html + to improve functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c9c29e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced DataFrame created with 11 rows and saved to: c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_eps_metadata.parquet\n",
      "Sliced DataFrame created with 11 rows and saved to: c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\second_ten_eps_metadata.parquet\n",
      "\n",
      "First ten eps metadata:\n",
      "             guest_name                                                url  \\\n",
      "0            John Early  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "10           Nina Conti  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "20  Katherine Parkinson  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "30     Bridget Christie  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "40          John Kearns  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "\n",
      "                                         slug  \\\n",
      "0                                  john-early   \n",
      "10                                 nina-conti   \n",
      "20  ep-299-katherine-parkinson-live-in-london   \n",
      "30       ep-288-bridget-christie-tasting-menu   \n",
      "40            ep-278-john-kearns-tasting-menu   \n",
      "\n",
      "               restaurants_mentioned  \n",
      "0   [Princes Hot Chicken, Hattie Bs]  \n",
      "10                        [Di Palos]  \n",
      "20                                []  \n",
      "30           [Soho Hotel Refuel Bar]  \n",
      "40                                []  \n",
      "\n",
      "Second ten eps metadata:\n",
      "             guest_name                                                url  \\\n",
      "100  Jada Pinkett Smith  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "110         Izuka Hoyle  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "120        Graham Coxon  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "130          Alex Jones  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "140    Yotam Ottolenghi  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "\n",
      "                          slug  \\\n",
      "100  ep-218-jada-pinkett-smith   \n",
      "110         ep-208-izuka-hoyle   \n",
      "120        ep-198-graham-coxon   \n",
      "130          ep-188-alex-jones   \n",
      "140    ep-179-yotam-ottolenghi   \n",
      "\n",
      "                                 restaurants_mentioned  \n",
      "100                                                 []  \n",
      "110                                                 []  \n",
      "120  [Cannons, Toffs of Muswell Hill, Sitwell Fish ...  \n",
      "130                                                 []  \n",
      "140                                [The Dusty Knuckle]  \n"
     ]
    }
   ],
   "source": [
    "# --- GENERATE TEST DATA: Full episodes metadata selected rows (inc. guest name, url, slug, MENTIONS (already combined)) ---\n",
    "# --- Batch 1: rows 0, 10, 20, 30, 40, 50...100 ---\n",
    "# --- Batch 2: \n",
    "\n",
    "# --- Batch 1: rows 0, 10, 20, 30, 40, 50...100 ---\n",
    "ten_test_episodes_metadata_output_path = os.path.join(V2_tests_dir, \"first_ten_eps_metadata.parquet\")\n",
    "\n",
    "indices_to_slice = range(0, 101, 10)\n",
    "\n",
    "# 2. Slice the DataFrame by position using .iloc\n",
    "# .iloc stands for 'integer location' and is used for positional indexing.\n",
    "first_ten_eps_metadata_df = full_episodes_metadata_test_df.iloc[indices_to_slice]\n",
    "\n",
    "# 3. Save the sliced DataFrame to a Parquet file\n",
    "# index=False ensures the default Pandas index (0, 1, 2, ...) is not saved as a column\n",
    "first_ten_eps_metadata_df.to_parquet(ten_test_episodes_metadata_output_path, index=False)\n",
    "\n",
    "print(f\"Sliced DataFrame created with {len(first_ten_eps_metadata_df)} rows and saved to: {ten_test_episodes_metadata_output_path}\")\n",
    "first_ten_eps_metadata_df\n",
    "\n",
    "# --- Batch 2: Rows 200, 210, ... 300 ---\n",
    "\n",
    "second_ten_test_episodes_metadata_output_path = os.path.join(V2_tests_dir, \"second_ten_eps_metadata.parquet\")\n",
    "\n",
    "indices_to_slice_2 = range(100, 201, 10)\n",
    "\n",
    "second_ten_eps_metadata_df = full_episodes_metadata_test_df.iloc[indices_to_slice_2]\n",
    "\n",
    "second_ten_eps_metadata_df.to_parquet(second_ten_test_episodes_metadata_output_path, index=False)\n",
    "\n",
    "print(f\"Sliced DataFrame created with {len(second_ten_eps_metadata_df)} rows and saved to: {second_ten_test_episodes_metadata_output_path}\")\n",
    "\n",
    "print(\"\\nFirst ten eps metadata:\")\n",
    "print(first_ten_eps_metadata_df.head())\n",
    "\n",
    "print(\"\\nSecond ten eps metadata:\")\n",
    "print(second_ten_eps_metadata_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ffd5940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---Original scraper with some edits (unsuccessful) ---\n",
    "\n",
    "def _save_transcripts_html(eps_dataframe, directory):\n",
    "    \"\"\"\n",
    "    Iterates through a DataFrame of episodes, downloads the HTML content from\n",
    "    the episode URL, and saves it to a specified directory.\n",
    "\n",
    "    Skips files that already exist and includes a random delay to be\n",
    "    polite to the server.\n",
    "\n",
    "    Args:\n",
    "        eps_dataframe (pd.DataFrame): DataFrame containing episode metadata\n",
    "                                      (including 'episode_number' and 'url').\n",
    "        directory (str): The directory to save the HTML files to.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    for index, row in eps_dataframe.iterrows():\n",
    "        guest_name = row[\"guest_name\"]\n",
    "        episode_url = row[\"url\"]\n",
    "        filename = f\"{guest_name}.html\"\n",
    "        filepath = os.path.join(directory, filename)\n",
    "\n",
    "        # Skip episodes that already exist\n",
    "        if os.path.exists(filepath):\n",
    "            print(\n",
    "                f\"  Skipping Episode {guest_name}, at index{index}: File already exists at {filepath}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Delay to be polite to the server and avoid 429 errors\n",
    "        sleep_time = random.uniform(1, 3)  # Sleep for 1 to 3 seconds\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "        html_content_str = extract_html(episode_url)\n",
    "\n",
    "        # Check for None before attempting to save\n",
    "        # The extract_html function returns None on failure (like a 429 error)\n",
    "        if html_content_str:\n",
    "            save_text_to_file(html_content_str, filename, directory)\n",
    "        else:\n",
    "            print(\n",
    "                f\"  Skipping save for Episode {episode_num} due to failed extraction.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa82cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GPT scraper V1 annotated --- \n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ---- Simple logger ----\n",
    "# logger = logging.getLogger(\"scraper\")\n",
    "if not logger.handlers:\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s %(message)s\"))\n",
    "    logger.addHandler(ch)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# ---- Helper: random-ish UA list (small) ----\n",
    "_SIMPLE_USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "]\n",
    "\n",
    "\n",
    "def _choose_headers():\n",
    "    return {\"User-Agent\": random.choice(_SIMPLE_USER_AGENTS)}\n",
    "\n",
    "\n",
    "# ---- Downloader with retries, backoff, persistence ----\n",
    "def download_transcripts(\n",
    "    url_map: Dict[str, str], # links the url to the guest name via a dict, because it uses name as filename (note needs to use slug as some names repeate.g. ed & james)\n",
    "    out_dir: str, # Directory to save html to\n",
    "    status_path: str, # Path to status JSON file\n",
    "    max_attempts_per_url: int = 5,\n",
    "    backoff_base: float = 1.0,\n",
    "    max_workers: int = 3, # Number of \"workers\" (threads , things that try to run concurrently in a single overarching process)\n",
    "    session: Optional[requests.Session] = None, # The session if we have one open for some reason (single session = more effieicnt)\n",
    "    timeout: float = 12.0,\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Download a set of URLs and save the HTML files locally.\n",
    "\n",
    "    Args:\n",
    "        url_map: mapping slug_or_filename -> url. Eg {\"paul-rudd\": \"https://.../ep-215-paul-rudd\"}\n",
    "                 Or you can map guest_name -> url.\n",
    "        out_dir: directory to save files (will be created).\n",
    "        status_path: path to JSON status file to persist attempts and outcomes.\n",
    "        max_attempts_per_url: maximum attempts per url before giving up.\n",
    "        backoff_base: base seconds for exponential backoff (1.0 is a reasonable default).\n",
    "        max_workers: number of concurrent download workers (1..6 recommended).\n",
    "        session: optional requests.Session() - if None a new one is created.\n",
    "        timeout: request timeout in seconds.\n",
    "\n",
    "    Returns:\n",
    "        status dict mapping key -> { \"url\", \"attempts\", \"status\", \"saved_path\", \"last_error\" }\n",
    "    \"\"\"\n",
    "\n",
    "    out_dir = Path(out_dir) \n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    status_path = Path(status_path) # Turns strings into paths for use saving/reading\n",
    "\n",
    "    # Load existing status if present (allows resume)\n",
    "    if status_path.exists():\n",
    "        try:\n",
    "            with open(status_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                status = json.load(f)\n",
    "        except Exception:\n",
    "            status = {}\n",
    "    else:\n",
    "        status = {}\n",
    "\n",
    "    # Initialize status entries for any missing keys\n",
    "    # Makes statuses for all \"keys\" (guest names/episodes)\n",
    "    for key, url in url_map.items():\n",
    "        if key not in status:\n",
    "            status[key] = {\n",
    "                \"url\": url,\n",
    "                \"attempts\": 0,\n",
    "                \"status\": \"pending\",  # pending | success | failed\n",
    "                \"saved_path\": None,\n",
    "                \"last_error\": None,\n",
    "            }\n",
    "\n",
    "    # Use a single session for pooling - more efficient that starting multiple sessions apparently\n",
    "    session = session or requests.Session()\n",
    "\n",
    "    def _attempt_download(key: str, meta: Dict) -> Dict:\n",
    "        url = meta[\"url\"]\n",
    "        attempts = meta[\"attempts\"]\n",
    "        result = dict(meta)\n",
    "        # If already succeeded, skip\n",
    "        if meta.get(\"status\") == \"success\":\n",
    "            return result\n",
    "\n",
    "        # If we've already reached max attempts, mark failed and skip\n",
    "        if attempts >= max_attempts_per_url:\n",
    "            result[\"status\"] = \"failed\"\n",
    "            result[\"last_error\"] = \"max_attempts_reached\"\n",
    "            return result\n",
    "\n",
    "        try:\n",
    "            # Build headers and request - headers are in the request and say what browser I'm using, we're faking three diff ones to rotate between\n",
    "            # To look less bot like\n",
    "            # Resp is a response object which is what comes back from the request, and contains the html text among other things\n",
    "            headers = _choose_headers()\n",
    "            resp = session.get(url, headers=headers, timeout=timeout)\n",
    "            # If success\n",
    "            if resp.status_code == 200:\n",
    "                # Save file (deterministic name using key)\n",
    "                # note needs changing to slug\n",
    "                filename = f\"{key}.html\"\n",
    "                saved_path = str(out_dir / filename)\n",
    "                with open(saved_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "                    fh.write(resp.text)\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"success\",\n",
    "                    \"saved_path\": saved_path,\n",
    "                    \"last_error\": None,\n",
    "                })\n",
    "                logger.info(\"Saved %s -> %s\", url, saved_path) # The logger lets us know whats going on, better than prints as level of detail can be\n",
    "                # changed dynamically\n",
    "                return result\n",
    "\n",
    "            # Retryable status codes (429 Too Many Requests, 5xx)\n",
    "            if resp.status_code in (429, 500, 502, 503, 504):\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"pending\",\n",
    "                    \"last_error\": f\"status_{resp.status_code}\"\n",
    "                })\n",
    "                logger.warning(\"Retryable HTTP %s for %s (attempt %s)\", resp.status_code, url, attempts + 1)\n",
    "                return result\n",
    "\n",
    "            # Non-retryable: mark failed with info\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"failed\",\n",
    "                \"last_error\": f\"status_{resp.status_code}\"\n",
    "            })\n",
    "            logger.error(\"Non-retryable HTTP %s for %s\", resp.status_code, url)\n",
    "            return result\n",
    "\n",
    "        except requests.RequestException as e: # network level errors, considered retryable\n",
    "            # Network error: retryable\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"pending\",\n",
    "                \"last_error\": repr(e)\n",
    "            })\n",
    "            logger.warning(\"RequestException for %s (attempt %s): %s\", url, attempts + 1, e)\n",
    "            return result\n",
    "\n",
    "    # Worker function wraps attempts + backoff\n",
    "    # Note meta is the status for this key, and it may be changed throuhg attemotin downloads to new meta\n",
    "    def _worker_task(key):\n",
    "        meta = status[key]\n",
    "        # If already success or permanently failed, return\n",
    "        if meta.get(\"status\") == \"success\" or meta.get(\"attempts\", 0) >= max_attempts_per_url:\n",
    "            return key, meta\n",
    "\n",
    "        # attempt download\n",
    "        new_meta = _attempt_download(key, meta)\n",
    "\n",
    "        # If still pending (retry-worthy), sleep exponential backoff before returning\n",
    "        if new_meta[\"status\"] == \"pending\":\n",
    "            # compute sleep: base * 2^(attempts-1) + jitter\n",
    "            sleep = backoff_base * (2 ** (new_meta[\"attempts\"] - 1))\n",
    "            jitter = random.uniform(0, 1.0)\n",
    "            sleep_time = min(sleep + jitter, 60)  # cap at 60s\n",
    "            logger.info(\"Backing off %0.2fs for %s (attempt %s)\", sleep_time, new_meta[\"url\"], new_meta[\"attempts\"])\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        return key, new_meta\n",
    "\n",
    "    # Main loop: do rounds where each round runs up to max_workers concurrent attempts on pending items.\n",
    "    pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "    # List comprehension selects k's to include, where k is the status and v the attempts, if status is pending and attempts below threshhld\n",
    "    round_idx = 0 # counter\n",
    "    while pending_keys:\n",
    "        round_idx += 1\n",
    "        logger.info(\"Download round %d: %d pending\", round_idx, len(pending_keys)) # How does %d work?\n",
    "\n",
    "        # Limit concurrency to not overload server\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futures = {ex.submit(_worker_task, key): key for key in pending_keys} # Futures represent the future result of the task - recall \n",
    "            # recall that threads designed to run tasks concerrently\n",
    "            # Note with... as ex is a context manager, opens/closes the thread pool\n",
    "            # submit passes a task (a certain key/guest name to try and download) to the thread pool\n",
    "            # futures = {} creates a dict where the future objects are keys and the values are the...keys, confusingly        \n",
    "            for fut in as_completed(futures): # as completed yields futuer obkects 1 by 1 as they're completed\n",
    "                key = futures[fut] # This accesses the future\n",
    "                try:\n",
    "                    k, new_meta = fut.result() # This makes k and new meta the results of the future (output of worker task, which is attempted download: a key and new meta which is the status entry for the key, inc. save path for html)\n",
    "                    status[k].update(new_meta)\n",
    "                except Exception as e:\n",
    "                    logger.exception(\"Unhandled exception for key %s: %s\", key, e)\n",
    "                    status[key][\"attempts\"] = status[key].get(\"attempts\", 0) + 1\n",
    "                    status[key][\"last_error\"] = repr(e)\n",
    "\n",
    "        # persist status to disk after every round\n",
    "        try:\n",
    "            with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(status, f, indent=2)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Failed to write status file: %s\", e)\n",
    "\n",
    "        # Prepare next round: only keys still pending and under attempts limit\n",
    "        pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "\n",
    "        # If there are pending keys, optionally small delay between rounds\n",
    "        if pending_keys:\n",
    "            logger.info(\"Sleeping 2s between rounds to be polite...\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    # final persist - saves the JSON again (unsure how dump method works)\n",
    "    with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(status, f, indent=2)\n",
    "\n",
    "    # return status mapping\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445304c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GPT scraper V2 using slugs ---\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ---- Simple logger ----\n",
    "#logger = logging.getLogger(\"scraper\")\n",
    "if not logger.handlers:\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s %(message)s\"))\n",
    "    logger.addHandler(ch)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# ---- Helper: random-ish UA list (small) ----\n",
    "_SIMPLE_USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "]\n",
    "\n",
    "\n",
    "def _choose_headers():\n",
    "    return {\"User-Agent\": random.choice(_SIMPLE_USER_AGENTS)}\n",
    "\n",
    "\n",
    "def _sanitize_key(key: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert a key (expected to be a slug) into a safe filename slug:\n",
    "      - lowercase\n",
    "      - replace any sequence of characters NOT a-z0-9 or '-' or '_' with '-'\n",
    "      - collapse multiple '-' into one\n",
    "      - strip leading/trailing '-' or '_'\n",
    "    This ensures keys like \"Paul Rudd\" become \"paul-rudd\" and already-correct slugs remain stable.\n",
    "    \"\"\"\n",
    "    if not isinstance(key, str):\n",
    "        key = str(key)\n",
    "    s = key.strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\-_]+\", \"-\", s)\n",
    "    s = re.sub(r\"-{2,}\", \"-\", s)\n",
    "    return s.strip(\"-_\")\n",
    "\n",
    "\n",
    "# ---- Downloader with retries, backoff, persistence ----\n",
    "def download_transcripts(\n",
    "    url_map: Dict[str, str],  # mapping slug_or_filename -> url (keys should be your episode slugs)\n",
    "    out_dir: str,  # Directory to save html to\n",
    "    status_path: str,  # Path to status JSON file\n",
    "    max_attempts_per_url: int = 5,\n",
    "    backoff_base: float = 1.0,\n",
    "    max_workers: int = 3,  # Number of concurrent download workers\n",
    "    session: Optional[requests.Session] = None,  # Optional shared requests.Session\n",
    "    timeout: float = 12.0,\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Download a set of URLs and save the HTML files locally.\n",
    "\n",
    "    Args:\n",
    "        url_map: mapping slug_or_filename -> url. Keys should be the episode slugs you want to use as identifiers.\n",
    "        out_dir: directory to save files (created if missing).\n",
    "        status_path: path to JSON status file to persist attempts and outcomes.\n",
    "        max_attempts_per_url: maximum attempts per url before giving up.\n",
    "        backoff_base: base seconds for exponential backoff.\n",
    "        max_workers: number of concurrent download workers.\n",
    "        session: optional requests.Session() - if None a new one is created.\n",
    "        timeout: request timeout in seconds.\n",
    "\n",
    "    Returns:\n",
    "        status dict mapping key -> { \"url\", \"attempts\", \"status\", \"saved_path\", \"last_error\" }\n",
    "    \"\"\"\n",
    "\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    status_path = Path(status_path)\n",
    "\n",
    "    # Load existing status if present (allows resume)\n",
    "    if status_path.exists():\n",
    "        try:\n",
    "            with open(status_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                status = json.load(f)\n",
    "        except Exception:\n",
    "            status = {}\n",
    "    else:\n",
    "        status = {}\n",
    "\n",
    "    # Initialize status entries for any missing keys\n",
    "    for key, url in url_map.items():\n",
    "        if key not in status:\n",
    "            status[key] = {\n",
    "                \"url\": url,\n",
    "                \"attempts\": 0,\n",
    "                \"status\": \"pending\",  # pending | success | failed\n",
    "                \"saved_path\": None,\n",
    "                \"last_error\": None,\n",
    "            }\n",
    "\n",
    "    # Use a single session for pooling\n",
    "    session = session or requests.Session()\n",
    "\n",
    "    def _attempt_download(key: str, meta: Dict) -> Dict:\n",
    "        url = meta[\"url\"]\n",
    "        attempts = meta[\"attempts\"]\n",
    "        result = dict(meta)\n",
    "        # If already succeeded, skip\n",
    "        if meta.get(\"status\") == \"success\":\n",
    "            return result\n",
    "\n",
    "        # If we've already reached max attempts, mark failed and skip\n",
    "        if attempts >= max_attempts_per_url:\n",
    "            result[\"status\"] = \"failed\"\n",
    "            result[\"last_error\"] = \"max_attempts_reached\"\n",
    "            return result\n",
    "\n",
    "        try:\n",
    "            # Build headers and request\n",
    "            headers = _choose_headers()\n",
    "            resp = session.get(url, headers=headers, timeout=timeout)\n",
    "            # If success\n",
    "            if resp.status_code == 200:\n",
    "                # Save file using sanitized key (ensure filesystem-safe slug)\n",
    "                safe_key = _sanitize_key(key)\n",
    "                filename = f\"{safe_key}.html\"\n",
    "                saved_path = str(out_dir / filename)\n",
    "                with open(saved_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "                    fh.write(resp.text)\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"success\",\n",
    "                    \"saved_path\": saved_path,\n",
    "                    \"last_error\": None,\n",
    "                })\n",
    "                logger.info(\"Saved %s -> %s\", url, saved_path)\n",
    "                return result\n",
    "\n",
    "            # Retryable status codes (429 Too Many Requests, 5xx)\n",
    "            if resp.status_code in (429, 500, 502, 503, 504):\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"pending\",\n",
    "                    \"last_error\": f\"status_{resp.status_code}\"\n",
    "                })\n",
    "                logger.warning(\"Retryable HTTP %s for %s (attempt %s)\", resp.status_code, url, attempts + 1)\n",
    "                return result\n",
    "\n",
    "            # Non-retryable: mark failed with info\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"failed\",\n",
    "                \"last_error\": f\"status_{resp.status_code}\"\n",
    "            })\n",
    "            logger.error(\"Non-retryable HTTP %s for %s\", resp.status_code, url)\n",
    "            return result\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            # Network error: retryable\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"pending\",\n",
    "                \"last_error\": repr(e)\n",
    "            })\n",
    "            logger.warning(\"RequestException for %s (attempt %s): %s\", url, attempts + 1, e)\n",
    "            return result\n",
    "\n",
    "    # Worker function wraps attempts + backoff\n",
    "    def _worker_task(key):\n",
    "        meta = status[key]\n",
    "        # If already success or permanently failed, return\n",
    "        if meta.get(\"status\") == \"success\" or meta.get(\"attempts\", 0) >= max_attempts_per_url:\n",
    "            return key, meta\n",
    "\n",
    "        # attempt download\n",
    "        new_meta = _attempt_download(key, meta)\n",
    "\n",
    "        # If still pending (retry-worthy), sleep exponential backoff before returning\n",
    "        if new_meta[\"status\"] == \"pending\":\n",
    "            # compute sleep: base * 2^(attempts-1) + jitter\n",
    "            sleep = backoff_base * (2 ** (new_meta[\"attempts\"] - 1))\n",
    "            jitter = random.uniform(0, 1.0)\n",
    "            sleep_time = min(sleep + jitter, 60)  # cap at 60s\n",
    "            logger.info(\"Backing off %0.2fs for %s (attempt %s)\", sleep_time, new_meta[\"url\"], new_meta[\"attempts\"])\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        return key, new_meta\n",
    "\n",
    "    # Main loop: do rounds where each round runs up to max_workers concurrent attempts on pending items.\n",
    "    pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "    round_idx = 0\n",
    "    while pending_keys:\n",
    "        round_idx += 1\n",
    "        logger.info(\"Download round %d: %d pending\", round_idx, len(pending_keys))\n",
    "\n",
    "        # Limit concurrency to not overload server\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futures = {ex.submit(_worker_task, key): key for key in pending_keys}\n",
    "            for fut in as_completed(futures):\n",
    "                key = futures[fut]\n",
    "                try:\n",
    "                    k, new_meta = fut.result()\n",
    "                    status[k].update(new_meta)\n",
    "                except Exception as e:\n",
    "                    logger.exception(\"Unhandled exception for key %s: %s\", key, e)\n",
    "                    status[key][\"attempts\"] = status[key].get(\"attempts\", 0) + 1\n",
    "                    status[key][\"last_error\"] = repr(e)\n",
    "\n",
    "        # persist status to disk after every round\n",
    "        try:\n",
    "            with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(status, f, indent=2)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Failed to write status file: %s\", e)\n",
    "\n",
    "        # Prepare next round: only keys still pending and under attempts limit\n",
    "        pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "\n",
    "        # If there are pending keys, optionally small delay between rounds\n",
    "        if pending_keys:\n",
    "            logger.info(\"Sleeping 2s between rounds to be polite...\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    # final persist\n",
    "    with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(status, f, indent=2)\n",
    "\n",
    "    # return status mapping\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ecf65b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GPT scraper V4 (collects server wait times from the server and uses these to prevent overload errors)\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Dict, Optional\n",
    "from email.utils import parsedate_to_datetime\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# -------------------\n",
    "# Logger configuration helper\n",
    "# -------------------\n",
    "def configure_logger(log_file: Optional[str] = None, level: int = logging.DEBUG):\n",
    "    \"\"\"\n",
    "    Configure a compact logger for the scraper.\n",
    "    - Console handler always enabled.\n",
    "    - Optional file handler if log_file provided.\n",
    "    - Default level: DEBUG for maximum visibility while testing.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"scraper\")\n",
    "    logger.setLevel(level)\n",
    "\n",
    "    # Avoid adding handlers multiple times when running multiple times in a notebook\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    # Console handler (clear, one-line format)\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(level)\n",
    "    ch.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s: %(message)s\"))\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    # Optional file handler (rotating not necessary here — keep simple)\n",
    "    if log_file:\n",
    "        fh = logging.FileHandler(log_file, encoding=\"utf-8\")\n",
    "        fh.setLevel(level)\n",
    "        fh.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s: %(message)s\"))\n",
    "        logger.addHandler(fh)\n",
    "\n",
    "    return logger\n",
    "\n",
    "# Initialize logger (call this in your notebook before running download_transcripts)\n",
    "logger = configure_logger()  # or configure_logger(\"data/scraper.log\")\n",
    "\n",
    "# -------------------\n",
    "# small sanitize helper (same as before)\n",
    "# -------------------\n",
    "def _sanitize_key(key: str) -> str:\n",
    "    if not isinstance(key, str):\n",
    "        key = str(key)\n",
    "    s = key.strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\-_]+\", \"-\", s)\n",
    "    s = re.sub(r\"-{2,}\", \"-\", s)\n",
    "    return s.strip(\"-_\")\n",
    "\n",
    "# ---- Helper: random-ish UA list (small) ----\n",
    "_SIMPLE_USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "]\n",
    "\n",
    "def _choose_headers():\n",
    "    return {\"User-Agent\": random.choice(_SIMPLE_USER_AGENTS)}\n",
    "\n",
    "# ----- Helper to access retry limits from the server (for use in scraper)\n",
    "def _parse_retry_after(header_value: Optional[str]) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Parse Retry-After header. It can be:\n",
    "      - an integer number of seconds, e.g. \"120\"\n",
    "      - a HTTP-date string, e.g. \"Wed, 21 Oct 2015 07:28:00 GMT\"\n",
    "    Return number of seconds to wait (float), or None if not parseable.\n",
    "    \"\"\"\n",
    "    if not header_value:\n",
    "        return None\n",
    "    header_value = header_value.strip()\n",
    "    # try integer seconds\n",
    "    if header_value.isdigit():\n",
    "        try:\n",
    "            return float(header_value)\n",
    "        except Exception:\n",
    "            return None\n",
    "    # try HTTP-date\n",
    "    try:\n",
    "        dt = parsedate_to_datetime(header_value)\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        now = datetime.now(timezone.utc)\n",
    "        delta = (dt - now).total_seconds()\n",
    "        return max(0.0, float(delta))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# -------------------\n",
    "# download_transcripts with extra logging (no other behavioural changes)\n",
    "# -------------------\n",
    "def download_transcripts(\n",
    "    url_map: Dict[str, str],\n",
    "    out_dir: str,\n",
    "    status_path: str,\n",
    "    max_attempts_per_url: int = 5,\n",
    "    backoff_base: float = 1.0,\n",
    "    max_workers: int = 3,\n",
    "    session: Optional[requests.Session] = None,\n",
    "    timeout: float = 12.0,\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Download URLs to out_dir using url_map (keys are slugs used as filenames).\n",
    "    Added logging provides visibility into what the function does on each run.\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    status_path = Path(status_path)\n",
    "\n",
    "    logger.info(\"Starting download_transcripts: %d urls, out_dir=%s, status_path=%s\",\n",
    "                len(url_map), out_dir, status_path)\n",
    "\n",
    "    # Load existing status if present (allows resume)\n",
    "    if status_path.exists():\n",
    "        try:\n",
    "            with open(status_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                status = json.load(f)\n",
    "            logger.debug(\"Loaded existing status.json with %d entries\", len(status))\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Failed to load status.json (%s). Starting with empty status.\", e)\n",
    "            status = {}\n",
    "    else:\n",
    "        logger.debug(\"No status.json file found at %s. Starting fresh.\", status_path)\n",
    "        status = {}\n",
    "\n",
    "    # Initialize status entries for any missing keys (log each new init)\n",
    "    for key, url in url_map.items():\n",
    "        if key not in status:\n",
    "            status[key] = {\n",
    "                \"url\": url,\n",
    "                \"attempts\": 0,\n",
    "                \"status\": \"pending\",  # pending | success | failed\n",
    "                \"saved_path\": None,\n",
    "                \"last_error\": None,\n",
    "            }\n",
    "            logger.debug(\"Initialized status for key='%s' -> %s\", key, url)\n",
    "\n",
    "    # Use a single session for pooling\n",
    "    session = session or requests.Session()\n",
    "\n",
    "    def _attempt_download(key: str, meta: Dict) -> Dict:\n",
    "        url = meta[\"url\"]\n",
    "        attempts = meta[\"attempts\"]\n",
    "        result = dict(meta)\n",
    "\n",
    "        # If already succeeded, skip and log reason\n",
    "        if meta.get(\"status\") == \"success\":\n",
    "            logger.debug(\"Skipping key='%s' (already success, saved_path=%s)\", key, meta.get(\"saved_path\"))\n",
    "            return result\n",
    "\n",
    "        # If max attempts reached, log and skip\n",
    "        if attempts >= max_attempts_per_url:\n",
    "            result[\"status\"] = \"failed\"\n",
    "            result[\"last_error\"] = \"max_attempts_reached\"\n",
    "            logger.info(\"Key='%s' reached max attempts (%d). Marking failed.\", key, attempts)\n",
    "            return result\n",
    "\n",
    "        # Log the attempt about to be made\n",
    "        logger.debug(\"Attempting key='%s' (attempt %d) -> %s\", key, attempts + 1, url)\n",
    "        try:\n",
    "            headers = _choose_headers()\n",
    "            resp = session.get(url, headers=headers, timeout=timeout)\n",
    "\n",
    "            # If success (200)\n",
    "            if resp.status_code == 200:\n",
    "                safe_key = _sanitize_key(key)\n",
    "                filename = f\"{safe_key}.html\"\n",
    "                saved_path = str(out_dir / filename)\n",
    "\n",
    "                # If file already exists, log that we're overwriting (helps debug)\n",
    "                if Path(saved_path).exists():\n",
    "                    logger.debug(\"File %s already exists and will be overwritten by key='%s'\", saved_path, key)\n",
    "\n",
    "                with open(saved_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "                    fh.write(resp.text)\n",
    "\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"success\",\n",
    "                    \"saved_path\": saved_path,\n",
    "                    \"last_error\": None,\n",
    "                })\n",
    "                logger.info(\"Saved %s -> %s (key=%s)\", url, saved_path, key)\n",
    "                return result\n",
    "\n",
    "            # Retryable status codes\n",
    "            if resp.status_code in (429, 500, 502, 503, 504):\n",
    "                # Parse Retry-After header if present and include in result\n",
    "                retry_after_raw = resp.headers.get(\"Retry-After\")\n",
    "                retry_after_seconds = _parse_retry_after(retry_after_raw)\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"pending\",\n",
    "                    \"last_error\": f\"status_{resp.status_code}\",\n",
    "                    \"retry_after_seconds\": retry_after_seconds,\n",
    "                })\n",
    "                logger.warning(\"Retryable HTTP %s for key='%s' url=%s (attempt %s)\",\n",
    "                               resp.status_code, key, url, attempts + 1)\n",
    "                # Log headers optionally for 429 to see Retry-After\n",
    "                if resp.status_code == 429:\n",
    "                    logger.debug(\"429 response headers for key='%s': Retry-After=%s\", key, retry_after_raw)\n",
    "                    logger.debug(\"Parsed Retry-After seconds for key='%s': %s\", key, retry_after_seconds)\n",
    "                return result\n",
    "\n",
    "            # Non-retryable\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"failed\",\n",
    "                \"last_error\": f\"status_{resp.status_code}\"\n",
    "            })\n",
    "            logger.error(\"Non-retryable HTTP %s for key='%s' url=%s\", resp.status_code, key, url)\n",
    "            return result\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            # Network error: retryable\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"pending\",\n",
    "                \"last_error\": repr(e)\n",
    "            })\n",
    "            logger.warning(\"RequestException for key='%s' url=%s (attempt %s): %s\", key, url, attempts + 1, e)\n",
    "            return result\n",
    "\n",
    "    # Worker wrapper with backoff\n",
    "    def _worker_task(key):\n",
    "        meta = status[key]\n",
    "        if meta.get(\"status\") == \"success\" or meta.get(\"attempts\", 0) >= max_attempts_per_url:\n",
    "            return key, meta\n",
    "\n",
    "        new_meta = _attempt_download(key, meta)\n",
    "\n",
    "        if new_meta[\"status\"] == \"pending\":\n",
    "            # computed exponential backoff (what we would do)\n",
    "            comp_sleep = backoff_base * (2 ** (new_meta[\"attempts\"] - 1))\n",
    "            jitter = random.uniform(0, 1.0)\n",
    "            computed_sleep = comp_sleep + jitter\n",
    "\n",
    "            # server-provided advice (if any)\n",
    "            retry_after = new_meta.get(\"retry_after_seconds\")\n",
    "            if retry_after is not None:\n",
    "                # use the server's suggestion if it's longer than our computed wait\n",
    "                sleep_time = max(computed_sleep, float(retry_after))\n",
    "            else:\n",
    "                sleep_time = computed_sleep\n",
    "\n",
    "            # cap to avoid runaway sleeps (adjust cap as desired)\n",
    "            sleep_time = min(sleep_time, 600.0)\n",
    "\n",
    "            logger.info(\"Backing off %0.2fs for key='%s' (attempt %s) [computed=%0.2fs, server=%s]\",\n",
    "                        sleep_time, key, new_meta[\"attempts\"], computed_sleep, retry_after)\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        return key, new_meta\n",
    "\n",
    "    # Main loop\n",
    "    pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "    round_idx = 0\n",
    "    while pending_keys:\n",
    "        round_idx += 1\n",
    "        logger.info(\"Download round %d: %d pending\", round_idx, len(pending_keys))\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futures = {ex.submit(_worker_task, key): key for key in pending_keys}\n",
    "            for fut in as_completed(futures):\n",
    "                key = futures[fut]\n",
    "                try:\n",
    "                    k, new_meta = fut.result()\n",
    "                    status[k].update(new_meta)\n",
    "                except Exception as e:\n",
    "                    logger.exception(\"Unhandled exception for key %s: %s\", key, e)\n",
    "                    status[key][\"attempts\"] = status[key].get(\"attempts\", 0) + 1\n",
    "                    status[key][\"last_error\"] = repr(e)\n",
    "\n",
    "        # persist status to disk after every round\n",
    "        try:\n",
    "            with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(status, f, indent=2)\n",
    "            logger.debug(\"Persisted status.json (round %d).\", round_idx)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Failed to write status file: %s\", e)\n",
    "\n",
    "        # Prepare next round\n",
    "        pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "\n",
    "        if pending_keys:\n",
    "            logger.info(\"Sleeping 2s between rounds to be polite...\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    # final persist and summary\n",
    "    with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(status, f, indent=2)\n",
    "\n",
    "    # Final summary counts\n",
    "    succ = sum(1 for v in status.values() if v.get(\"status\") == \"success\")\n",
    "    failed = sum(1 for v in status.values() if v.get(\"status\") == \"failed\")\n",
    "    pending = sum(1 for v in status.values() if v.get(\"status\") == \"pending\")\n",
    "    logger.info(\"Download finished. success=%d failed=%d pending=%d\", succ, failed, pending)\n",
    "\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "64bd5ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,\n",
       " [('john-early',\n",
       "   'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/john-early'),\n",
       "  ('nina-conti',\n",
       "   'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/nina-conti'),\n",
       "  ('ep-299-katherine-parkinson-live-in-london',\n",
       "   'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-299-katherine-parkinson-live-in-london')])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- First ten eps test batch, using metadata to scrape. Cell A, build URL map ---\n",
    "# URL map is a dict of slugs and urls, which feels a bit unnecessary given they're already linked by row\n",
    "\n",
    "# Replace base_url below if you need to construct URLs from slugs:\n",
    "base_url = \"https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster\"\n",
    "\n",
    "# If sliced_df has a 'url' column already:\n",
    "if \"url\" in first_ten_eps_metadata_df.columns:\n",
    "    url_map = {row[\"slug\"]: row[\"url\"] for _, row in first_ten_eps_metadata_df.iterrows()}\n",
    "else:\n",
    "    # build urls by joining base_url and slug (only do this if that matches the website)\n",
    "    url_map = {row[\"slug\"]: base_url.rstrip(\"/\") + \"/\" + row[\"slug\"].lstrip(\"/\") for _, row in first_ten_eps_metadata_df.iterrows()}\n",
    "\n",
    "len(url_map), list(url_map.items())[:3]  # quick check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1c133e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 11:24:51,430 INFO: Starting download_transcripts: 11 urls, out_dir=c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts, status_path=c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\status.json\n",
      "2025-12-03 11:24:51,460 DEBUG: Loaded existing status.json with 42 entries\n",
      "2025-12-03 11:24:51,460 INFO: Download finished. success=42 failed=0 pending=0\n"
     ]
    }
   ],
   "source": [
    "# --- Test: First ten eps test batch, using metadata to scrape. Cell B, run download_transcripts ---\n",
    "test_transcripts_dir = os.path.join(V2_tests_dir, \"test_transcripts\")      # where HTMLs will be saved \n",
    "status_path = os.path.join(test_transcripts_dir, \"status.json\")\n",
    "\n",
    "# tune these for a polite test run\n",
    "max_attempts_per_url = 8\n",
    "backoff_base = 2.0\n",
    "max_workers = 2   # start low while testing\n",
    "\n",
    "# call the function (assumes download_transcripts is in scope)\n",
    "status = download_transcripts(\n",
    "    url_map=url_map,\n",
    "    out_dir=test_transcripts_dir,\n",
    "    status_path=status_path,\n",
    "    max_attempts_per_url=max_attempts_per_url,\n",
    "    backoff_base=backoff_base,\n",
    "    max_workers=max_workers,\n",
    "    timeout=12.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1dc56ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,\n",
       " [('ep-218-jada-pinkett-smith',\n",
       "   'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-218-jada-pinkett-smith'),\n",
       "  ('ep-208-izuka-hoyle',\n",
       "   'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-208-izuka-hoyle'),\n",
       "  ('ep-198-graham-coxon',\n",
       "   'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-198-graham-coxon')])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Test: Second ten eps test batch, using metadata to scrape. Cell A, build URL map ---\n",
    "\n",
    "# Cell A — Build url_map\n",
    "# Replace base_url below if you need to construct URLs from slugs:\n",
    "base_url = \"https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster\"\n",
    "\n",
    "# If second_ten_test_data_df has a 'url' column already:\n",
    "if \"url\" in second_ten_eps_metadata_df.columns:\n",
    "    url_map_2 = {row[\"slug\"]: row[\"url\"] for _, row in second_ten_eps_metadata_df.iterrows()}\n",
    "else:\n",
    "    # build urls by joining base_url and slug (only do this if that matches the website)\n",
    "    url_map_2 = {row[\"slug\"]: base_url.rstrip(\"/\") + \"/\" + row[\"slug\"].lstrip(\"/\") for _, row in second_ten_eps_metadata_df.iterrows()}\n",
    "\n",
    "len(url_map_2), list(url_map_2.items())[:3]  # quick check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fda992d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 11:24:56,946 INFO: Starting download_transcripts: 11 urls, out_dir=c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts, status_path=c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\status.json\n",
      "2025-12-03 11:24:56,962 DEBUG: Loaded existing status.json with 42 entries\n",
      "2025-12-03 11:24:56,964 INFO: Download finished. success=42 failed=0 pending=0\n"
     ]
    }
   ],
   "source": [
    "# --- Test: Second ten eps test batch, using metadata to scrape. Cell B, run download_transcripts ---\n",
    "\n",
    "test_transcripts_dir = os.path.join(V2_tests_dir, \"test_transcripts\")      # where HTMLs will be saved \n",
    "status_path = os.path.join(test_transcripts_dir, \"status.json\")\n",
    "\n",
    "# tune these for a polite test run\n",
    "max_attempts_per_url = 8\n",
    "backoff_base = 2.0\n",
    "max_workers = 2   # start low while testing\n",
    "\n",
    "# call the function (assumes download_transcripts is in scope)\n",
    "status = download_transcripts(\n",
    "    url_map=url_map_2,\n",
    "    out_dir=test_transcripts_dir,\n",
    "    status_path=status_path,\n",
    "    max_attempts_per_url=max_attempts_per_url,\n",
    "    backoff_base=backoff_base,\n",
    "    max_workers=max_workers,\n",
    "    timeout=12.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1dc9d2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 11:25:02,115 INFO: Starting download_transcripts: 11 urls, out_dir=c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts, status_path=c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\status.json\n",
      "2025-12-03 11:25:02,131 DEBUG: Loaded existing status.json with 42 entries\n",
      "2025-12-03 11:25:02,133 INFO: Download finished. success=42 failed=0 pending=0\n"
     ]
    }
   ],
   "source": [
    "# --- Scraper orchestration function notes (everything we need to do first) ---\n",
    "\n",
    "logger = configure_logger()\n",
    "\n",
    "# If sliced_df has a 'url' column already:\n",
    "if \"url\" in first_ten_eps_metadata_df.columns:\n",
    "    url_map = {row[\"slug\"]: row[\"url\"] for _, row in first_ten_eps_metadata_df.iterrows()}\n",
    "else:\n",
    "    # build urls by joining base_url and slug (only do this if that matches the website)\n",
    "    url_map = {row[\"slug\"]: base_url.rstrip(\"/\") + \"/\" + row[\"slug\"].lstrip(\"/\") for _, row in first_ten_eps_metadata_df.iterrows()}\n",
    "\n",
    "out_dir = os.path.join(V2_tests_dir, \"test_transcripts\")      # where HTMLs will be saved \n",
    "status_path = os.path.join(out_dir, \"status.json\")\n",
    "max_attempts_per_url = 8\n",
    "backoff_base = 2.0\n",
    "max_workers = 2   # start low while testing\n",
    "\n",
    "# call the function (assumes download_transcripts is in scope)\n",
    "status = download_transcripts(\n",
    "    url_map=url_map_2,\n",
    "    out_dir=out_dir,\n",
    "    status_path=status_path,\n",
    "    max_attempts_per_url=max_attempts_per_url,\n",
    "    backoff_base=backoff_base,\n",
    "    max_workers=max_workers,\n",
    "    timeout=12.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "54b2b5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GPT scraper orchestration function ---\n",
    "\n",
    "def orchestrate_scraper(\n",
    "    df,                     # DataFrame with 'slug' and optionally 'url'\n",
    "    base_url,               # base URL for constructing URLs if df has no 'url' column\n",
    "    out_dir,                # folder to save HTML transcripts\n",
    "    max_attempts_per_url=5,\n",
    "    backoff_base=1.0,\n",
    "    max_workers=3,\n",
    "    timeout=12.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Orchestrates the scraping process:\n",
    "      1. Prepares a slug → URL map\n",
    "      2. Ensures output folder exists\n",
    "      3. Calls download_transcripts() with sensible defaults\n",
    "      4. Returns the status dict for all downloads\n",
    "    \"\"\"\n",
    "    # ---------------------\n",
    "    # Setup logger for this run\n",
    "    # ---------------------\n",
    "    logger = configure_logger()\n",
    "    logger.info(\"Starting scraper orchestration for %d episodes\", len(df))\n",
    "\n",
    "    # ---------------------\n",
    "    # Prepare URL map\n",
    "    # ---------------------\n",
    "    if \"url\" in df.columns:\n",
    "        url_map = {row[\"slug\"]: row[\"url\"] for _, row in df.iterrows()}\n",
    "        logger.info(\"Using existing URLs from DataFrame\")\n",
    "    else:\n",
    "        url_map = {row[\"slug\"]: base_url.rstrip(\"/\") + \"/\" + row[\"slug\"].lstrip(\"/\") for _, row in df.iterrows()}\n",
    "        logger.info(\"Constructed URLs from base_url and slugs\")\n",
    "\n",
    "    # ---------------------\n",
    "    # Ensure output folder exists\n",
    "    # ---------------------\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    status_path = out_dir / \"status.json\"\n",
    "\n",
    "    # ---------------------\n",
    "    # Call the scraper\n",
    "    # ---------------------\n",
    "    logger.info(\"Running download_transcripts with %d URLs\", len(url_map))\n",
    "    status = download_transcripts(\n",
    "        url_map=url_map,\n",
    "        out_dir=out_dir,\n",
    "        status_path=status_path,\n",
    "        max_attempts_per_url=max_attempts_per_url,\n",
    "        backoff_base=backoff_base,\n",
    "        max_workers=max_workers,\n",
    "        timeout=timeout\n",
    "    )\n",
    "\n",
    "    logger.info(\"Scraper orchestration finished\")\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e8635b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstatus = orchestrate_scraper(\\n    df=second_ten_test_data_df,\\n    base_url=\"https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster\",\\n    out_dir=os.path.join(V2_tests_dir, \"test_transcripts\"),\\n    max_attempts_per_url=8,\\n    backoff_base=2.0,\\n    max_workers=2\\n)\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Testing orchstrator (COMMENTED OUT) ---\n",
    "\"\"\"\n",
    "status = orchestrate_scraper(\n",
    "    df=second_ten_test_data_df,\n",
    "    base_url=\"https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster\",\n",
    "    out_dir=os.path.join(V2_tests_dir, \"test_transcripts\"),\n",
    "    max_attempts_per_url=8,\n",
    "    backoff_base=2.0,\n",
    "    max_workers=2\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15374254",
   "metadata": {},
   "source": [
    "## New extracting clean text and timestamps + combining into dataframe\n",
    "\n",
    "### Do we need to change any prior functions for the slug change? No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f4900179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function extracting timestamps from one ep, as list of dicts ---\n",
    "\n",
    "def _extract_timestamps_as_list_of_dicts(\n",
    "    transcript_str: str, slug: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Finds all 'starting point is HH:MM:SS' timestamps in a transcript string.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of dictionaries, where each dict contains the episode\n",
    "                               number, timestamp string, and its starting index.\n",
    "    \"\"\"\n",
    "    timestamp_pattern = re.compile(r\"starting point is (\\d{2}:\\d{2}:\\d{2})\")\n",
    "    all_timestamps_in_transcript = []\n",
    "    for match in timestamp_pattern.finditer(transcript_str):\n",
    "        # Get the captured timestamp string (e.g., \"00:00:05\")\n",
    "        actual_time_string = match.group(1)\n",
    "        # We use group(1) because that's our (HH:MM:SS) part, group(0) refers to the whole string by default\n",
    "\n",
    "        # Get the starting index of the entire match\n",
    "        start_position_in_text = match.start()\n",
    "        # Store this as a dict with episode_slug as key\n",
    "        stamp_dict = {\n",
    "            \"slug\": slug,\n",
    "            \"timestamp\": actual_time_string,\n",
    "            \"start_index\": start_position_in_text,\n",
    "        }\n",
    "        # Store this extracted data (the timestamp string and its position)\n",
    "        all_timestamps_in_transcript.append(stamp_dict)\n",
    "    return all_timestamps_in_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "47f27907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function orchestrating extration and saving of clean transcripts and timestamps from metadata df ---\n",
    "# Takes in episodes dataframe path, directory of transcripts, and output directory to save to\n",
    "\n",
    "def extract_save_clean_text_and_periodic_timestamps(\n",
    "    full_episodes_metadata_path: str, transcripts_dir: str, output_filepath: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Takes the full episodes metadata filepath, the transcripts html directory, and an output filepath, and iterates\n",
    "    through the episodes, processing the html into clean transcript text and collating the periodic timestamps.\n",
    "\n",
    "    These transcripts and periodic timestamps are saved in a dataframe, which is saves as a parquet file to the\n",
    "    output filepath.\n",
    "\n",
    "    Args:\n",
    "        full_episodes_metadata_path (str): The full episodes metadata dataframe filepath\n",
    "        transcripts_dir (str): The directory containing the html of each episode.\n",
    "        output_filepath (str): The filepath the output df is saved to.\n",
    "    Returns:\n",
    "        None: A dataframe containing the clean text and the timestamps (a list of Dicts) is saved to the\n",
    "        output filepath as a parquet.\n",
    "    \"\"\"\n",
    "    # 1. Load episodes meta_data\n",
    "    episodes_df = try_read_parquet(full_episodes_metadata_path)\n",
    "    if episodes_df is None or episodes_df.empty:\n",
    "        print(\n",
    "            \"INFO: No unprocessed episodes found. Nothing to do. (Either all episodes processed or metadata empty.)\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    processed_records = []  # To store data for the final DataFrame\n",
    "\n",
    "    # 2. Iterate through each episode's metadata\n",
    "    for index, row in episodes_df.iterrows():\n",
    "        episode_slug = row.get(\"slug\")\n",
    "        print(f\"Episode slug is {episode_slug}\") # DEBUGGING\n",
    "        guest_name = row.get(\"guest_name\")\n",
    "        transcript_filename = f\"{episode_slug}.html\"\n",
    "        transcript_filepath = os.path.join(transcripts_dir, transcript_filename)\n",
    "        restaurants_mentioned = row.get(\"restaurants_mentioned\")\n",
    "        # Confirm file exists and skip if not\n",
    "        if not os.path.exists(transcript_filepath):\n",
    "            print(\n",
    "                f\"  WARNING: Transcript file not found for Episode {guest_name}, slug: {episode_slug} at {transcript_filepath}. Skipping.\"\n",
    "            )\n",
    "            continue  # Skip to the next episode\n",
    "        try:\n",
    "            clean_transcript_str = _clean_transcript_str_from_html(transcript_filepath)\n",
    "            timestamps = _extract_timestamps_as_list_of_dicts(\n",
    "                clean_transcript_str, episode_slug\n",
    "            )\n",
    "\n",
    "            processed_records.append(\n",
    "                {\n",
    "                    \"slug\": episode_slug,\n",
    "                    \"guest_name\": guest_name,\n",
    "                    \"restaurants_mentioned\": restaurants_mentioned,\n",
    "                    \"clean_transcript_text\": clean_transcript_str,\n",
    "                    \"periodic_timestamps\": timestamps,  # This will be a list of dictionaries\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"  Processed Episode {episode_slug} ({guest_name}): Extracted text and {len(timestamps)} timestamps.\"\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"  ERROR: Failed to process transcript for Episode {episode_slug} ({guest_name}) from {transcript_filepath}: {e}\"\n",
    "            )\n",
    "            continue  # For MVP, just skip and warn\n",
    "\n",
    "        if processed_records:\n",
    "            result_df = pd.DataFrame(processed_records)\n",
    "            result_df.to_parquet(output_filepath, index=False)\n",
    "            print(\n",
    "                f\"Successfully saved clean transcripts and timestamps for {len(result_df)} episodes to {output_filepath}\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                \"No transcripts were successfully processed. Output DataFrame will be empty.\"\n",
    "            )\n",
    "            pd.DataFrame().to_parquet(output_filepath, index=False)  # Save an empty DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e817fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function combining episode metadata df with transcripts and timestamps df---\n",
    "# Takes args transcripts/timestamps filepath, metadata filepath\n",
    "\n",
    "\n",
    "def combine_timestamps_and_metadata(\n",
    "    transcripts_timestamps_filepath: str, metadata_filepath: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads and combines the transcripts and timestamps dataframe with the metadata dataframe.\n",
    "\n",
    "    Args:\n",
    "        transcripts_timestamps_filepath(str)\n",
    "        metadata_filepath (str)\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe containing episode slug, restaurants mentioned, clean transcript,\n",
    "        and timestamps.\n",
    "    \"\"\"\n",
    "    metadata_df = try_read_parquet(metadata_filepath)\n",
    "    transcripts_timestamps_df = try_read_parquet(transcripts_timestamps_filepath)\n",
    "    combined_df = transcripts_timestamps_df.merge(\n",
    "        metadata_df[[\"slug\", \"restaurants_mentioned\"]],\n",
    "        on=\"slug\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1cd8936a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode slug is john-early\n",
      "  Processed Episode john-early (John Early): Extracted text and 324 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 1 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_test_timestamps.parquet\n",
      "Episode slug is nina-conti\n",
      "  Processed Episode nina-conti (Nina Conti): Extracted text and 272 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 2 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_test_timestamps.parquet\n",
      "Episode slug is ep-299-katherine-parkinson-live-in-london\n",
      "  Processed Episode ep-299-katherine-parkinson-live-in-london (Katherine Parkinson): Extracted text and 152 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 3 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_test_timestamps.parquet\n",
      "Episode slug is ep-288-bridget-christie-tasting-menu\n",
      "  Processed Episode ep-288-bridget-christie-tasting-menu (Bridget Christie): Extracted text and 168 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 4 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_test_timestamps.parquet\n",
      "Episode slug is ep-278-john-kearns-tasting-menu\n",
      "  Processed Episode ep-278-john-kearns-tasting-menu (John Kearns): Extracted text and 232 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 5 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_test_timestamps.parquet\n",
      "Episode slug is ep-268-jessica-hynes\n",
      "  Processed Episode ep-268-jessica-hynes (Jessica Hynes): Extracted text and 203 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 6 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_test_timestamps.parquet\n",
      "Episode slug is ep-258-phil-dunster\n",
      "  Processed Episode ep-258-phil-dunster (Phil Dunster): Extracted text and 182 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 7 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_test_timestamps.parquet\n",
      "Episode slug is ep-248-huge-davies\n",
      "  Processed Episode ep-248-huge-davies (Huge Davies): Extracted text and 206 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 8 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_test_timestamps.parquet\n",
      "Episode slug is ep-238-katy-wix\n",
      "  Processed Episode ep-238-katy-wix (Katy Wix): Extracted text and 161 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 9 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_test_timestamps.parquet\n",
      "Episode slug is ep-228-ray-winstone\n",
      "  Processed Episode ep-228-ray-winstone (Ray Winstone): Extracted text and 198 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 10 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_test_timestamps.parquet\n",
      "Episode slug is ep-218-jada-pinkett-smith\n",
      "  Processed Episode ep-218-jada-pinkett-smith (Jada Pinkett Smith): Extracted text and 198 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 11 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_test_timestamps.parquet\n"
     ]
    }
   ],
   "source": [
    "# --- GENERATE TEST DATA: first ten eps and their timestamps and transcripts ---\n",
    "# --- Tests: extract and save clean transcripts and timestamps, using first 10 eps metadata ---\n",
    "\n",
    "# Timestamps/transcripts saved to first_ten_metadata_timestamps_out_path\n",
    "\n",
    "# Define path\n",
    "first_ten_metadata_timestamps_out_path = os.path.join(V2_tests_dir, \"first_ten_test_timestamps.parquet\")\n",
    "\n",
    "# Extract timestamps, save df inc. metadata and transcrtips/timestamps\n",
    "extract_save_clean_text_and_periodic_timestamps(ten_test_episodes_metadata_output_path, out_dir, first_ten_metadata_timestamps_out_path)\n",
    "\n",
    "# Read dataframe for use in fuzzymatching (requires df not path)\n",
    "first_ten_metadata_timestamps_df = try_read_parquet(first_ten_metadata_timestamps_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "15da589f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slug</th>\n",
       "      <th>guest_name</th>\n",
       "      <th>restaurants_mentioned</th>\n",
       "      <th>clean_transcript_text</th>\n",
       "      <th>periodic_timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>john-early</td>\n",
       "      <td>John Early</td>\n",
       "      <td>[Princes Hot Chicken, Hattie Bs]</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "      <td>[{'slug': 'john-early', 'start_index': 0, 'tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nina-conti</td>\n",
       "      <td>Nina Conti</td>\n",
       "      <td>[Di Palos]</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "      <td>[{'slug': 'nina-conti', 'start_index': 0, 'tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ep-299-katherine-parkinson-live-in-london</td>\n",
       "      <td>Katherine Parkinson</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 oh boy, oh boy, the...</td>\n",
       "      <td>[{'slug': 'ep-299-katherine-parkinson-live-in-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ep-288-bridget-christie-tasting-menu</td>\n",
       "      <td>Bridget Christie</td>\n",
       "      <td>[Soho Hotel Refuel Bar]</td>\n",
       "      <td>starting point is 00:00:00 huge news from off-...</td>\n",
       "      <td>[{'slug': 'ep-288-bridget-christie-tasting-men...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ep-278-john-kearns-tasting-menu</td>\n",
       "      <td>John Kearns</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 welcome to the off-...</td>\n",
       "      <td>[{'slug': 'ep-278-john-kearns-tasting-menu', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ep-268-jessica-hynes</td>\n",
       "      <td>Jessica Hynes</td>\n",
       "      <td>[Everest Cash and Carry]</td>\n",
       "      <td>starting point is 00:00:00 we get it. life get...</td>\n",
       "      <td>[{'slug': 'ep-268-jessica-hynes', 'start_index...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ep-258-phil-dunster</td>\n",
       "      <td>Phil Dunster</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 i am charlotte casa...</td>\n",
       "      <td>[{'slug': 'ep-258-phil-dunster', 'start_index'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ep-248-huge-davies</td>\n",
       "      <td>Huge Davies</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 welcome to the off ...</td>\n",
       "      <td>[{'slug': 'ep-248-huge-davies', 'start_index':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ep-238-katy-wix</td>\n",
       "      <td>Katy Wix</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "      <td>[{'slug': 'ep-238-katy-wix', 'start_index': 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ep-228-ray-winstone</td>\n",
       "      <td>Ray Winstone</td>\n",
       "      <td>[Clock Tower Cafe, Scotts, Smiths]</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "      <td>[{'slug': 'ep-228-ray-winstone', 'start_index'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ep-218-jada-pinkett-smith</td>\n",
       "      <td>Jada Pinkett Smith</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 ed. yeah, man. i'm ...</td>\n",
       "      <td>[{'slug': 'ep-218-jada-pinkett-smith', 'start_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         slug           guest_name  \\\n",
       "0                                  john-early           John Early   \n",
       "1                                  nina-conti           Nina Conti   \n",
       "2   ep-299-katherine-parkinson-live-in-london  Katherine Parkinson   \n",
       "3        ep-288-bridget-christie-tasting-menu     Bridget Christie   \n",
       "4             ep-278-john-kearns-tasting-menu          John Kearns   \n",
       "5                        ep-268-jessica-hynes        Jessica Hynes   \n",
       "6                         ep-258-phil-dunster         Phil Dunster   \n",
       "7                          ep-248-huge-davies          Huge Davies   \n",
       "8                             ep-238-katy-wix             Katy Wix   \n",
       "9                         ep-228-ray-winstone         Ray Winstone   \n",
       "10                  ep-218-jada-pinkett-smith   Jada Pinkett Smith   \n",
       "\n",
       "                 restaurants_mentioned  \\\n",
       "0     [Princes Hot Chicken, Hattie Bs]   \n",
       "1                           [Di Palos]   \n",
       "2                                   []   \n",
       "3              [Soho Hotel Refuel Bar]   \n",
       "4                                   []   \n",
       "5             [Everest Cash and Carry]   \n",
       "6                                   []   \n",
       "7                                   []   \n",
       "8                                   []   \n",
       "9   [Clock Tower Cafe, Scotts, Smiths]   \n",
       "10                                  []   \n",
       "\n",
       "                                clean_transcript_text  \\\n",
       "0   starting point is 00:00:00 oh no, it's james a...   \n",
       "1   starting point is 00:00:00 hello, it's ed gamb...   \n",
       "2   starting point is 00:00:00 oh boy, oh boy, the...   \n",
       "3   starting point is 00:00:00 huge news from off-...   \n",
       "4   starting point is 00:00:00 welcome to the off-...   \n",
       "5   starting point is 00:00:00 we get it. life get...   \n",
       "6   starting point is 00:00:00 i am charlotte casa...   \n",
       "7   starting point is 00:00:00 welcome to the off ...   \n",
       "8   starting point is 00:00:00 hello, it's ed gamb...   \n",
       "9   starting point is 00:00:00 hello, it's ed gamb...   \n",
       "10  starting point is 00:00:00 ed. yeah, man. i'm ...   \n",
       "\n",
       "                                  periodic_timestamps  \n",
       "0   [{'slug': 'john-early', 'start_index': 0, 'tim...  \n",
       "1   [{'slug': 'nina-conti', 'start_index': 0, 'tim...  \n",
       "2   [{'slug': 'ep-299-katherine-parkinson-live-in-...  \n",
       "3   [{'slug': 'ep-288-bridget-christie-tasting-men...  \n",
       "4   [{'slug': 'ep-278-john-kearns-tasting-menu', '...  \n",
       "5   [{'slug': 'ep-268-jessica-hynes', 'start_index...  \n",
       "6   [{'slug': 'ep-258-phil-dunster', 'start_index'...  \n",
       "7   [{'slug': 'ep-248-huge-davies', 'start_index':...  \n",
       "8   [{'slug': 'ep-238-katy-wix', 'start_index': 0,...  \n",
       "9   [{'slug': 'ep-228-ray-winstone', 'start_index'...  \n",
       "10  [{'slug': 'ep-218-jada-pinkett-smith', 'start_...  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Inspecting transcripts timestamps df for first 10 eps ---\n",
    "\n",
    "first_ten_metadata_timestamps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ce13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slug</th>\n",
       "      <th>guest_name</th>\n",
       "      <th>restaurants_mentioned_x</th>\n",
       "      <th>clean_transcript_text</th>\n",
       "      <th>periodic_timestamps</th>\n",
       "      <th>restaurants_mentioned_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>john-early</td>\n",
       "      <td>John Early</td>\n",
       "      <td>[Princes Hot Chicken, Hattie Bs]</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "      <td>[{'slug': 'john-early', 'start_index': 0, 'tim...</td>\n",
       "      <td>[Princes Hot Chicken, Hattie Bs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nina-conti</td>\n",
       "      <td>Nina Conti</td>\n",
       "      <td>[Di Palos]</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "      <td>[{'slug': 'nina-conti', 'start_index': 0, 'tim...</td>\n",
       "      <td>[Di Palos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ep-299-katherine-parkinson-live-in-london</td>\n",
       "      <td>Katherine Parkinson</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 oh boy, oh boy, the...</td>\n",
       "      <td>[{'slug': 'ep-299-katherine-parkinson-live-in-...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ep-288-bridget-christie-tasting-menu</td>\n",
       "      <td>Bridget Christie</td>\n",
       "      <td>[Soho Hotel Refuel Bar]</td>\n",
       "      <td>starting point is 00:00:00 huge news from off-...</td>\n",
       "      <td>[{'slug': 'ep-288-bridget-christie-tasting-men...</td>\n",
       "      <td>[Soho Hotel Refuel Bar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ep-278-john-kearns-tasting-menu</td>\n",
       "      <td>John Kearns</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 welcome to the off-...</td>\n",
       "      <td>[{'slug': 'ep-278-john-kearns-tasting-menu', '...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ep-268-jessica-hynes</td>\n",
       "      <td>Jessica Hynes</td>\n",
       "      <td>[Everest Cash and Carry]</td>\n",
       "      <td>starting point is 00:00:00 we get it. life get...</td>\n",
       "      <td>[{'slug': 'ep-268-jessica-hynes', 'start_index...</td>\n",
       "      <td>[Everest Cash and Carry]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ep-258-phil-dunster</td>\n",
       "      <td>Phil Dunster</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 i am charlotte casa...</td>\n",
       "      <td>[{'slug': 'ep-258-phil-dunster', 'start_index'...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ep-248-huge-davies</td>\n",
       "      <td>Huge Davies</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 welcome to the off ...</td>\n",
       "      <td>[{'slug': 'ep-248-huge-davies', 'start_index':...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ep-238-katy-wix</td>\n",
       "      <td>Katy Wix</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "      <td>[{'slug': 'ep-238-katy-wix', 'start_index': 0,...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ep-228-ray-winstone</td>\n",
       "      <td>Ray Winstone</td>\n",
       "      <td>[Clock Tower Cafe, Scotts, Smiths]</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "      <td>[{'slug': 'ep-228-ray-winstone', 'start_index'...</td>\n",
       "      <td>[Clock Tower Cafe, Scotts, Smiths]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ep-218-jada-pinkett-smith</td>\n",
       "      <td>Jada Pinkett Smith</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 ed. yeah, man. i'm ...</td>\n",
       "      <td>[{'slug': 'ep-218-jada-pinkett-smith', 'start_...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         slug           guest_name  \\\n",
       "0                                  john-early           John Early   \n",
       "1                                  nina-conti           Nina Conti   \n",
       "2   ep-299-katherine-parkinson-live-in-london  Katherine Parkinson   \n",
       "3        ep-288-bridget-christie-tasting-menu     Bridget Christie   \n",
       "4             ep-278-john-kearns-tasting-menu          John Kearns   \n",
       "5                        ep-268-jessica-hynes        Jessica Hynes   \n",
       "6                         ep-258-phil-dunster         Phil Dunster   \n",
       "7                          ep-248-huge-davies          Huge Davies   \n",
       "8                             ep-238-katy-wix             Katy Wix   \n",
       "9                         ep-228-ray-winstone         Ray Winstone   \n",
       "10                  ep-218-jada-pinkett-smith   Jada Pinkett Smith   \n",
       "\n",
       "               restaurants_mentioned_x  \\\n",
       "0     [Princes Hot Chicken, Hattie Bs]   \n",
       "1                           [Di Palos]   \n",
       "2                                   []   \n",
       "3              [Soho Hotel Refuel Bar]   \n",
       "4                                   []   \n",
       "5             [Everest Cash and Carry]   \n",
       "6                                   []   \n",
       "7                                   []   \n",
       "8                                   []   \n",
       "9   [Clock Tower Cafe, Scotts, Smiths]   \n",
       "10                                  []   \n",
       "\n",
       "                                clean_transcript_text  \\\n",
       "0   starting point is 00:00:00 oh no, it's james a...   \n",
       "1   starting point is 00:00:00 hello, it's ed gamb...   \n",
       "2   starting point is 00:00:00 oh boy, oh boy, the...   \n",
       "3   starting point is 00:00:00 huge news from off-...   \n",
       "4   starting point is 00:00:00 welcome to the off-...   \n",
       "5   starting point is 00:00:00 we get it. life get...   \n",
       "6   starting point is 00:00:00 i am charlotte casa...   \n",
       "7   starting point is 00:00:00 welcome to the off ...   \n",
       "8   starting point is 00:00:00 hello, it's ed gamb...   \n",
       "9   starting point is 00:00:00 hello, it's ed gamb...   \n",
       "10  starting point is 00:00:00 ed. yeah, man. i'm ...   \n",
       "\n",
       "                                  periodic_timestamps  \\\n",
       "0   [{'slug': 'john-early', 'start_index': 0, 'tim...   \n",
       "1   [{'slug': 'nina-conti', 'start_index': 0, 'tim...   \n",
       "2   [{'slug': 'ep-299-katherine-parkinson-live-in-...   \n",
       "3   [{'slug': 'ep-288-bridget-christie-tasting-men...   \n",
       "4   [{'slug': 'ep-278-john-kearns-tasting-menu', '...   \n",
       "5   [{'slug': 'ep-268-jessica-hynes', 'start_index...   \n",
       "6   [{'slug': 'ep-258-phil-dunster', 'start_index'...   \n",
       "7   [{'slug': 'ep-248-huge-davies', 'start_index':...   \n",
       "8   [{'slug': 'ep-238-katy-wix', 'start_index': 0,...   \n",
       "9   [{'slug': 'ep-228-ray-winstone', 'start_index'...   \n",
       "10  [{'slug': 'ep-218-jada-pinkett-smith', 'start_...   \n",
       "\n",
       "               restaurants_mentioned_y  \n",
       "0     [Princes Hot Chicken, Hattie Bs]  \n",
       "1                           [Di Palos]  \n",
       "2                                   []  \n",
       "3              [Soho Hotel Refuel Bar]  \n",
       "4                                   []  \n",
       "5             [Everest Cash and Carry]  \n",
       "6                                   []  \n",
       "7                                   []  \n",
       "8                                   []  \n",
       "9   [Clock Tower Cafe, Scotts, Smiths]  \n",
       "10                                  []  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- REDUNDANT: combining the test timestamps and transcripts with the metadata ---\n",
    "# They were already combined by extract and save transcripts timestamps\n",
    "\n",
    "\n",
    "combined_timestamps_metadata_df = combine_timestamps_and_metadata(test_timestamps_out_path, ten_test_episodes_metadata_output_path)\n",
    "combined_timestamps_metadata_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dbaddd",
   "metadata": {},
   "source": [
    "## Fuzzywuzzy testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c00816b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Functions: fuzzymatching functions: Adapted to use slug IDs ---\n",
    "\n",
    "# Recommended parameters based on typical transcript length and restaurant name length\n",
    "WINDOW_SIZE = 150 # The size of each chunk (token)\n",
    "OVERLAP_SIZE = 50 # How much overlap between chunks\n",
    "\n",
    "def _create_list_tuple_clean_sen_og_sen_og_index_v2(\n",
    "    text: str,\n",
    "    window_size: int = WINDOW_SIZE,\n",
    "    overlap_size: int = OVERLAP_SIZE,\n",
    ") -> List[Tuple[str, str, int]]:\n",
    "    \"\"\"\n",
    "    Splits the transcript into overlapping, fixed-size chunks (tokens) for fuzzy matching.\n",
    "    \n",
    "    Returns:\n",
    "        List[Tuple[str, str, int]]: A list containing (cleaned_chunk, original_chunk, true_start_index).\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "    text_len = len(text)\n",
    "    \n",
    "    # Calculate the step size (how far to advance for the next chunk)\n",
    "    step_size = window_size - overlap_size\n",
    "    \n",
    "    # Iterate using a sliding window\n",
    "    for start_index in range(0, text_len, step_size):\n",
    "        end_index = min(start_index + window_size, text_len)\n",
    "        \n",
    "        # 1. Get the original chunk\n",
    "        original_chunk = text[start_index:end_index]\n",
    "        \n",
    "        # We must skip processing very short trailing chunks that are only overlap\n",
    "        # If the chunk is too small and we're not at the end, skip it (handled by the loop)\n",
    "        if len(original_chunk) < 10 and end_index < text_len: # minimum length check\n",
    "             continue\n",
    "\n",
    "        # 2. Prepare the cleaned chunk for fuzzy matching\n",
    "        # Strip trailing/leading spaces from the chunk\n",
    "        original_chunk_stripped = original_chunk.strip()\n",
    "\n",
    "        if original_chunk_stripped:\n",
    "            # Re-calculate the exact start index in the original text after stripping\n",
    "            # (assuming only leading spaces are removed, which is safe for this method)\n",
    "            leading_spaces_count = len(original_chunk) - len(original_chunk.lstrip())\n",
    "            true_start_index = start_index + leading_spaces_count\n",
    "            \n",
    "            # Apply original cleaning (remove punctuation, lowercase)\n",
    "            cleaned_chunk = re.sub(r\"[^\\w\\s]\", \"\", original_chunk_stripped).lower()\n",
    "\n",
    "            # Store cleaned, original, and true start index\n",
    "            results.append(\n",
    "                (cleaned_chunk, original_chunk_stripped, true_start_index)\n",
    "            )\n",
    "            \n",
    "        # Stop early if the window covered the end of the text\n",
    "        if end_index == text_len:\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "def _create_list_tuple_clean_sen_og_sen_og_index(\n",
    "    text: str,\n",
    ") -> List[Tuple[str, str, int]]:\n",
    "    \"\"\"\n",
    "    Takes in a clean transcript string, and creates a list of tuples containing cleaned sentences\n",
    "    for fuzzymatching, original sentences and starting index for locating quotes.\n",
    "\n",
    "    Splits text using delimiter \". \". Assumes no sentences start with puntuation (leading spaces are the only shift from the start of the original to the start\n",
    "    of the cleaned sentence).\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, str, int]]: a list containing a tuple, with cleaned sentence, original\n",
    "                                    stripped sentence, and true start index (the start index of the original sentence,\n",
    "                                    in the original text).\n",
    "\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    current_idx_in_original = 0  # This tracks our position in the original 'text'\n",
    "\n",
    "    # Split into 'segments' (what will become sentences) by full stop/space.\n",
    "    segments = text.split(\". \")\n",
    "\n",
    "    for i, segment in enumerate(\n",
    "        segments\n",
    "    ):  # Note enumerate is a way to loop and get index (rather than a manual counter)\n",
    "        original_full_sentence_segment = segment\n",
    "        # Calculate the actual start index of the content within the segment itself (after stripping leading/trailing spaces)\n",
    "        # It asssumes the start index (in processes sentence) will only move due to leading spaces\n",
    "        # So, it calculates the original (assuming none start with punctuation), and retains it\n",
    "        # Later, we will use this original index to compare against timestamps\n",
    "        leading_spaces_count = len(original_full_sentence_segment) - len(\n",
    "            original_full_sentence_segment.lstrip()\n",
    "        )\n",
    "        true_start_index = current_idx_in_original + leading_spaces_count\n",
    "\n",
    "        original_sentence_stripped = original_full_sentence_segment.strip()\n",
    "\n",
    "        # Only process if the sentence is not empty after stripping\n",
    "        if original_sentence_stripped:\n",
    "            # Apply original cleaning, explicitly converting to lowercase for fuzzy matching\n",
    "            cleaned_sentence = re.sub(\n",
    "                r\"[^\\w\\s]\", \"\", original_sentence_stripped\n",
    "            ).lower()\n",
    "\n",
    "            # Store cleaned, original, and start index\n",
    "            results.append(\n",
    "                (cleaned_sentence, original_sentence_stripped, true_start_index)\n",
    "            )\n",
    "\n",
    "        # Update current_idx_in_original for the next segment.\n",
    "        # Add the length of the current segment and the delimiter length (2 for \". \").\n",
    "        # This assumes all segments (except possibly the last) were followed by \". \".\n",
    "        current_idx_in_original += len(original_full_sentence_segment)\n",
    "        if (\n",
    "            i < len(segments) - 1\n",
    "        ):  # Only add delimiter length if it's not the last segment\n",
    "            current_idx_in_original += len(\". \")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def _find_timestamp(\n",
    "    original_sentence_start_index: int, transcript_timestamps: List[dict]\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds the nearest timestamp occurring before or at a given sentence index.\n",
    "\n",
    "    This function searches through a list of timestamp dictionaries (which should\n",
    "    be pre-sorted by `start_index`) to find the timestamp that immediately\n",
    "    precedes or is at the start of a matched sentence.\n",
    "\n",
    "    Args:\n",
    "        original_sentence_start_index (int): The starting index of the sentence\n",
    "            in the full transcript string.\n",
    "        transcript_timestamps (List[dict]): A list of dictionaries, where each dict\n",
    "            contains 'start_index' and 'timestamp' for a periodic timestamp.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: The timestamp string (e.g., '00:01:23') if a match is found,\n",
    "                       otherwise returns None.\n",
    "    \"\"\"\n",
    "    if original_sentence_start_index is None:\n",
    "        return None\n",
    "    # Could sort timestamps here for good practice, but should be sorted already\n",
    "    # Reverse-iterate over timestamps to find the \"nearest before or at\"\n",
    "    for timestamp_dict in reversed(transcript_timestamps):\n",
    "        if timestamp_dict[\"start_index\"] <= original_sentence_start_index:\n",
    "            return timestamp_dict[\"timestamp\"]\n",
    "\n",
    "    return None  # If no timestamp found before the quote's starting position (all eps start \"Starting point is 00:00:00\")\n",
    "\n",
    "\n",
    "def _matches_by_res_name_from_list_of_res_names(\n",
    "    restaurant_names: List[str], searchable_sentences: List[str], min_score: int\n",
    ") -> Dict[str, List[Tuple[str, int, int]]]:\n",
    "    \"\"\"\n",
    "    Finds fuzzy matches for a list of restaurant names within a list of cleaned sentences.\n",
    "\n",
    "    This function iterates through each restaurant name and uses fuzzy matching to find\n",
    "    sentences that are a close match. Matches are filtered based on a minimum score.\n",
    "\n",
    "    Args:\n",
    "        restaurant_names (List[str]): A list of restaurant names to search for.\n",
    "        searchable_sentences (List[str]): A list of pre-cleaned sentences to search within.\n",
    "        min_score (int): The minimum fuzzy match score (from 0-100) to consider\n",
    "                         a match valid.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[Tuple[str, int, int]]]: A dictionary where:\n",
    "            - Keys are the restaurant names from `restaurant_names`.\n",
    "            - Values are a list of filtered matches for that restaurant.\n",
    "            - Each match is a tuple containing:\n",
    "                - str: The matched sentence text.\n",
    "                - int: The fuzzy matching score.\n",
    "                - int: The index of the matched sentence in the `searchable_sentences` list.\n",
    "    \"\"\"\n",
    "    filtered_matches_by_string = {}\n",
    "    for res_name in restaurant_names:\n",
    "        matches = process.extract(\n",
    "            res_name, searchable_sentences, scorer=fuzz.partial_ratio, limit=20\n",
    "        )\n",
    "\n",
    "        filtered_matches = []\n",
    "        # --- FIX: Unpack the tuple of 2 items correctly ---\n",
    "        for match_text, score in matches:\n",
    "            if score >= min_score:\n",
    "                # Find the index of the matched sentence in the original list\n",
    "                # We use a try-except block for robustness in case of unexpected data.\n",
    "                try:\n",
    "                    original_sentence_index = searchable_sentences.index(match_text)\n",
    "                    # Append all three pieces of information\n",
    "                    filtered_matches.append(\n",
    "                        (match_text, score, original_sentence_index)\n",
    "                    )\n",
    "                except ValueError:\n",
    "                    # This will happen if the match text isn't found in the list,\n",
    "                    # e.g., due to slight string differences not captured by .index()\n",
    "                    continue\n",
    "\n",
    "        filtered_matches_by_string[res_name] = filtered_matches\n",
    "\n",
    "    return filtered_matches_by_string\n",
    "\n",
    "def find_top_match_and_timestamps(\n",
    "    combined_df: pd.DataFrame, min_match_score: int = 90\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Finds fuzzy matches for restaurant mentions in episode transcripts and associates them with timestamps.\n",
    "\n",
    "    This function iterates through each episode's metadata and transcript data. For each mentioned\n",
    "    restaurant, it performs a fuzzy search within the transcript. It then returns a DataFrame\n",
    "    of the top matches and their corresponding timestamps, or notes if no match was found.\n",
    "\n",
    "    Args:\n",
    "        combined_df (pd.DataFrame): A DataFrame containing episode metadata, cleaned transcripts,\n",
    "                                    and periodic timestamps.\n",
    "        min_match_score (int): The minimum fuzzy match score (0-100) required to consider\n",
    "                               a match valid.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame where each row represents a restaurant mention. It contains\n",
    "                      the following columns:\n",
    "                          - 'slug': The episode slug e.g. ep-217-ross-noble or elle-fanning\n",
    "                          - 'Restaurant': The name of the restaurant mentioned.\n",
    "                          - 'Mention text': The original sentence where the mention was found.\n",
    "                          - 'Match Score': The fuzzy match score.\n",
    "                          - 'Match Type': The type of match (e.g., 'full, over 90' or 'No match found').\n",
    "                          - 'Timestamp': The nearest preceding timestamp for the mention.\n",
    "                          - 'Transcript sample': A short sample of the transcript text.\n",
    "    \"\"\"\n",
    "    all_mentions_collected = []\n",
    "\n",
    "    for index, combined_row in combined_df.iterrows():\n",
    "        slug = combined_row.get(\"slug\")\n",
    "        guest_name = combined_row.get(\"guest_name\")\n",
    "        clean_transcript_text = combined_row.get(\"clean_transcript_text\")\n",
    "        periodic_timestamps = combined_row.get(\"periodic_timestamps\")\n",
    "\n",
    "        restaurants_data = combined_row.get(\"restaurants_mentioned\", [])\n",
    "        transcript_sample = (\n",
    "            clean_transcript_text[:200]\n",
    "            if isinstance(clean_transcript_text, str)\n",
    "            else \"No Transcript Found\"\n",
    "        )\n",
    "\n",
    "        # Unsure what data type the res mentions are, hence need for this\n",
    "        restaurants_list = []\n",
    "        print(f\"The data type of the restaurant matches is{type(restaurants_data)}\")\n",
    "        if isinstance(restaurants_data, list):\n",
    "            restaurants_list = restaurants_data\n",
    "        elif isinstance(restaurants_data, np.ndarray) and restaurants_data.size > 0:\n",
    "            # Flatten the array and convert it to a standard Python list of strings\n",
    "            restaurants_raw_list = restaurants_data.flatten().tolist()\n",
    "            restaurants_list = [\n",
    "                name.strip().lower() for name in restaurants_raw_list if name.strip()\n",
    "            ]\n",
    "        elif isinstance(restaurants_data, str):\n",
    "            restaurants_list = [\n",
    "                name.strip() for name in restaurants_data.split(\",\") if name.strip()\n",
    "            ]\n",
    "\n",
    "        if restaurants_list:\n",
    "            episode_sentences_data = _create_list_tuple_clean_sen_og_sen_og_index(\n",
    "                clean_transcript_text\n",
    "            )\n",
    "            searchable_sentences = [\n",
    "                item[0] for item in episode_sentences_data\n",
    "            ]  # This is to select the cleaned sentence from the list of tuple\n",
    "            # of cleaned sentence, original, and true start index that create_sentence_list creates\n",
    "\n",
    "            all_matches_for_episode = _matches_by_res_name_from_list_of_res_names(\n",
    "                restaurants_list, searchable_sentences, 90\n",
    "            )\n",
    "            # --- all_matches_for_episode is a dict with key res_name and value lists of matches (matches are tuples of quote, score)\n",
    "            for (\n",
    "                restaurant_name_query,\n",
    "                match_list_for_query,\n",
    "            ) in all_matches_for_episode.items():\n",
    "                if match_list_for_query:\n",
    "                    top_match = match_list_for_query[0]\n",
    "                    # Unpack the top match's data\n",
    "                    matched_cleaned_text, score, matched_sentence_index = top_match\n",
    "                    original_sentence_data = episode_sentences_data[\n",
    "                        matched_sentence_index\n",
    "                    ]  # This takes you back to episode sentences data for the sentence index\n",
    "                    # Which is a tuple of clean sentence, original, and index of sentence within sen list\n",
    "                    original_sentence_text = original_sentence_data[\n",
    "                        1\n",
    "                    ]  # The og sentence is at index 1 in this tuple\n",
    "                    original_start_index = original_sentence_data[\n",
    "                        2\n",
    "                    ]  # The og start index is at index 2 in this tuple\n",
    "\n",
    "                    timestamp = _find_timestamp(\n",
    "                        original_start_index, periodic_timestamps\n",
    "                    )\n",
    "\n",
    "                    mention = {\n",
    "                        \"Episode ID\": slug,\n",
    "                        \"Restaurant\": restaurant_name_query,\n",
    "                        \"Mention text\": original_sentence_text,\n",
    "                        \"Match Score\": score,\n",
    "                        \"Match Type\": f\"full, over {min_match_score}\",\n",
    "                        \"Timestamp\": timestamp,\n",
    "                        \"transcript_sample\": transcript_sample,\n",
    "                    }\n",
    "                    all_mentions_collected.append(mention)\n",
    "                else:\n",
    "                    null_mention = {\n",
    "                        \"Episode ID\": slug,\n",
    "                        \"Restaurant\": restaurant_name_query,\n",
    "                        \"Mention text\": None,\n",
    "                        \"Match Score\": 0,\n",
    "                        \"Match Type\": \"No match found\",\n",
    "                        \"Timestamp\": None,\n",
    "                        \"transcript_sample\": transcript_sample,\n",
    "                    }\n",
    "                    all_mentions_collected.append(null_mention)\n",
    "        else:\n",
    "            print(\n",
    "                f\"  No raw mentions found in 'restaurants_mentioned' list for Episode {slug}. Skipping\"\n",
    "            )\n",
    "    combined_df = pd.DataFrame(all_mentions_collected)\n",
    "    return combined_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d34395ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_match_and_timestamps_v2(\n",
    "    combined_df: pd.DataFrame, min_match_score: int = 90\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Finds fuzzy matches for restaurant mentions in episode transcripts and associates them with timestamps.\n",
    "\n",
    "    This function iterates through each episode's metadata and transcript data. For each mentioned\n",
    "    restaurant, it performs a fuzzy search within the transcript. It then returns a DataFrame\n",
    "    of the top matches and their corresponding timestamps, or notes if no match was found.\n",
    "\n",
    "    Args:\n",
    "        combined_df (pd.DataFrame): A DataFrame containing episode metadata, cleaned transcripts,\n",
    "                                    and periodic timestamps.\n",
    "        min_match_score (int): The minimum fuzzy match score (0-100) required to consider\n",
    "                               a match valid.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame where each row represents a restaurant mention. It contains\n",
    "                      the following columns:\n",
    "                          - 'slug': The episode slug e.g. ep-217-ross-noble or elle-fanning\n",
    "                          - 'Restaurant': The name of the restaurant mentioned.\n",
    "                          - 'Mention text': The original sentence where the mention was found.\n",
    "                          - 'Match Score': The fuzzy match score.\n",
    "                          - 'Match Type': The type of match (e.g., 'full, over 90' or 'No match found').\n",
    "                          - 'Timestamp': The nearest preceding timestamp for the mention.\n",
    "                          - 'Transcript sample': A short sample of the transcript text.\n",
    "    \"\"\"\n",
    "    all_mentions_collected = []\n",
    "\n",
    "    for index, combined_row in combined_df.iterrows():\n",
    "        slug = combined_row.get(\"slug\")\n",
    "        guest_name = combined_row.get(\"guest_name\")\n",
    "        clean_transcript_text = combined_row.get(\"clean_transcript_text\")\n",
    "        periodic_timestamps = combined_row.get(\"periodic_timestamps\")\n",
    "\n",
    "        restaurants_data = combined_row.get(\"restaurants_mentioned\", [])\n",
    "        transcript_sample = (\n",
    "            clean_transcript_text[:200]\n",
    "            if isinstance(clean_transcript_text, str)\n",
    "            else \"No Transcript Found\"\n",
    "        )\n",
    "\n",
    "        # Unsure what data type the res mentions are, hence need for this\n",
    "        restaurants_list = []\n",
    "        # print(f\"The data type of the restaurant matches is{type(restaurants_data)}\")\n",
    "        # print(f\"restaurants data (raw) : {restaurants_data}\")\n",
    "        if isinstance(restaurants_data, list):\n",
    "            restaurants_list = restaurants_data\n",
    "        elif isinstance(restaurants_data, np.ndarray) and restaurants_data.size > 0:\n",
    "            # Flatten the array and convert it to a standard Python list of strings\n",
    "            restaurants_raw_list = restaurants_data.flatten().tolist()\n",
    "            restaurants_list = [\n",
    "                name.strip().lower() for name in restaurants_raw_list if name.strip()\n",
    "            ]\n",
    "            # print(f\"\\n restauratns list (processed): {restaurants_list}\")\n",
    "        elif isinstance(restaurants_data, str):\n",
    "            restaurants_list = [\n",
    "                name.strip() for name in restaurants_data.split(\",\") if name.strip()\n",
    "            ]\n",
    "        # Searching starts here\n",
    "        if restaurants_list:\n",
    "            episode_sentences_data = _create_list_tuple_clean_sen_og_sen_og_index(\n",
    "                clean_transcript_text\n",
    "            )\n",
    "            searchable_sentences = [\n",
    "                item[0] for item in episode_sentences_data\n",
    "            ]  # This is to select the cleaned sentence from the list of tuple\n",
    "            # of cleaned sentence, original, and true start index that create_sentence_list creates\n",
    "\n",
    "            all_matches_for_episode = _matches_by_res_name_from_list_of_res_names(\n",
    "                restaurants_list, searchable_sentences, 90\n",
    "            )\n",
    "            # --- all_matches_for_episode is a dict with key res_name and value lists of matches (matches r tuples of quote, score)\n",
    "            for (\n",
    "                restaurant_name_query,\n",
    "                match_list_for_query,\n",
    "            ) in all_matches_for_episode.items():\n",
    "                if match_list_for_query:\n",
    "                    top_match = match_list_for_query[0]\n",
    "                    # Unpack the top match's data\n",
    "                    matched_cleaned_text, score, matched_sentence_index = top_match\n",
    "                    original_sentence_data = episode_sentences_data[\n",
    "                        matched_sentence_index\n",
    "                    ]  # This takes you back to episode sentences data for the sentence index\n",
    "                    # Which is a tuple of clean sentence, original, and index of sentence within sen list\n",
    "                    original_sentence_text = original_sentence_data[\n",
    "                        1\n",
    "                    ]  # The og sentence is at index 1 in this tuple\n",
    "                    original_start_index = original_sentence_data[\n",
    "                        2\n",
    "                    ]  # The og start index is at index 2 in this tuple\n",
    "\n",
    "                    timestamp = _find_timestamp(\n",
    "                        original_start_index, periodic_timestamps\n",
    "                    )\n",
    "\n",
    "                    mention = {\n",
    "                        \"Episode ID\": slug,\n",
    "                        \"Restaurant\": restaurant_name_query,\n",
    "                        \"Mention text\": original_sentence_text,\n",
    "                        \"Match Score\": score,\n",
    "                        \"Match Type\": f\"full, over {min_match_score}\",\n",
    "                        \"Timestamp\": timestamp,\n",
    "                        \"transcript_sample\": transcript_sample,\n",
    "                    }\n",
    "                    all_mentions_collected.append(mention)\n",
    "                else:\n",
    "                    null_mention = {\n",
    "                        \"Episode ID\": slug,\n",
    "                        \"Restaurant\": restaurant_name_query,\n",
    "                        \"Mention text\": None,\n",
    "                        \"Match Score\": 0,\n",
    "                        \"Match Type\": \"No match found\",\n",
    "                        \"Timestamp\": None,\n",
    "                        \"transcript_sample\": transcript_sample,\n",
    "                    }\n",
    "                    all_mentions_collected.append(null_mention)\n",
    "        else:\n",
    "            print(\n",
    "                f\"  No raw mentions found in 'restaurants_mentioned' list for Episode {slug}. Skipping\"\n",
    "            )\n",
    "    combined_df = pd.DataFrame(all_mentions_collected)\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "288d840e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data type of the restaurant matches is<class 'numpy.ndarray'>\n",
      "restaurants data (raw) : ['Princes Hot Chicken' 'Hattie Bs']\n",
      "\n",
      " restaruatns list (processed): ['princes hot chicken', 'hattie bs']\n",
      "The data type of the restaurant matches is<class 'numpy.ndarray'>\n",
      "restaurants data (raw) : ['Di Palos']\n",
      "\n",
      " restaruatns list (processed): ['di palos']\n",
      "The data type of the restaurant matches is<class 'numpy.ndarray'>\n",
      "restaurants data (raw) : []\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-299-katherine-parkinson-live-in-london. Skipping\n",
      "The data type of the restaurant matches is<class 'numpy.ndarray'>\n",
      "restaurants data (raw) : ['Soho Hotel Refuel Bar']\n",
      "\n",
      " restaruatns list (processed): ['soho hotel refuel bar']\n",
      "The data type of the restaurant matches is<class 'numpy.ndarray'>\n",
      "restaurants data (raw) : []\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-278-john-kearns-tasting-menu. Skipping\n",
      "The data type of the restaurant matches is<class 'numpy.ndarray'>\n",
      "restaurants data (raw) : ['Everest Cash and Carry']\n",
      "\n",
      " restaruatns list (processed): ['everest cash and carry']\n",
      "The data type of the restaurant matches is<class 'numpy.ndarray'>\n",
      "restaurants data (raw) : []\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-258-phil-dunster. Skipping\n",
      "The data type of the restaurant matches is<class 'numpy.ndarray'>\n",
      "restaurants data (raw) : []\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-248-huge-davies. Skipping\n",
      "The data type of the restaurant matches is<class 'numpy.ndarray'>\n",
      "restaurants data (raw) : []\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-238-katy-wix. Skipping\n",
      "The data type of the restaurant matches is<class 'numpy.ndarray'>\n",
      "restaurants data (raw) : ['Clock Tower Cafe' 'Scotts' 'Smiths']\n",
      "\n",
      " restaruatns list (processed): ['clock tower cafe', 'scotts', 'smiths']\n",
      "The data type of the restaurant matches is<class 'numpy.ndarray'>\n",
      "restaurants data (raw) : []\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-218-jada-pinkett-smith. Skipping\n",
      "\n",
      "--- TOP COLLECTED ---\n",
      "Top Mentions DataFrame created with 8 rows.\n"
     ]
    }
   ],
   "source": [
    "# --- Test: fuzzymatching on first 10 eps metadata and mentions (first_ten_metadata_timestamps_df) ---\n",
    "# --- Metadata includes slug, guest_name, restaurants_mentioned\n",
    "\n",
    "# --- Run top matches on the test data ---\n",
    "top_mentions_df = find_top_match_and_timestamps_v2(first_ten_metadata_timestamps_df , 90)\n",
    "\n",
    "# --- Convert list into dataframe, print output ---\n",
    "\n",
    "print(f\"\\n--- TOP COLLECTED ---\")\n",
    "print(f\"Top Mentions DataFrame created with {len(top_mentions_df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4dbe56b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Episode ID</th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Mention text</th>\n",
       "      <th>Match Score</th>\n",
       "      <th>Match Type</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>transcript_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>john-early</td>\n",
       "      <td>princes hot chicken</td>\n",
       "      <td>you can go to prince's hot chicken? it's not n...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:58:14</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>john-early</td>\n",
       "      <td>hattie bs</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>No match found</td>\n",
       "      <td>None</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nina-conti</td>\n",
       "      <td>di palos</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>No match found</td>\n",
       "      <td>None</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ep-288-bridget-christie-tasting-menu</td>\n",
       "      <td>soho hotel refuel bar</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>No match found</td>\n",
       "      <td>None</td>\n",
       "      <td>starting point is 00:00:00 huge news from off-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ep-268-jessica-hynes</td>\n",
       "      <td>everest cash and carry</td>\n",
       "      <td>starting point is 00:19:42 there's a great, th...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:19:42</td>\n",
       "      <td>starting point is 00:00:00 we get it. life get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ep-228-ray-winstone</td>\n",
       "      <td>clock tower cafe</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>No match found</td>\n",
       "      <td>None</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ep-228-ray-winstone</td>\n",
       "      <td>scotts</td>\n",
       "      <td>but my favourite, favourite fish restaurant th...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:57:41</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ep-228-ray-winstone</td>\n",
       "      <td>smiths</td>\n",
       "      <td>in fact, once going back about ten years ago i...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:26:40</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Episode ID              Restaurant  \\\n",
       "0                            john-early     princes hot chicken   \n",
       "1                            john-early               hattie bs   \n",
       "2                            nina-conti                di palos   \n",
       "3  ep-288-bridget-christie-tasting-menu   soho hotel refuel bar   \n",
       "4                  ep-268-jessica-hynes  everest cash and carry   \n",
       "5                   ep-228-ray-winstone        clock tower cafe   \n",
       "6                   ep-228-ray-winstone                  scotts   \n",
       "7                   ep-228-ray-winstone                  smiths   \n",
       "\n",
       "                                        Mention text  Match Score  \\\n",
       "0  you can go to prince's hot chicken? it's not n...          100   \n",
       "1                                               None            0   \n",
       "2                                               None            0   \n",
       "3                                               None            0   \n",
       "4  starting point is 00:19:42 there's a great, th...          100   \n",
       "5                                               None            0   \n",
       "6  but my favourite, favourite fish restaurant th...          100   \n",
       "7  in fact, once going back about ten years ago i...          100   \n",
       "\n",
       "       Match Type Timestamp                                  transcript_sample  \n",
       "0   full, over 90  00:58:14  starting point is 00:00:00 oh no, it's james a...  \n",
       "1  No match found      None  starting point is 00:00:00 oh no, it's james a...  \n",
       "2  No match found      None  starting point is 00:00:00 hello, it's ed gamb...  \n",
       "3  No match found      None  starting point is 00:00:00 huge news from off-...  \n",
       "4   full, over 90  00:19:42  starting point is 00:00:00 we get it. life get...  \n",
       "5  No match found      None  starting point is 00:00:00 hello, it's ed gamb...  \n",
       "6   full, over 90  00:57:41  starting point is 00:00:00 hello, it's ed gamb...  \n",
       "7   full, over 90  00:26:40  starting point is 00:00:00 hello, it's ed gamb...  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_mentions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0773fa",
   "metadata": {},
   "source": [
    "## Helpler to match old transcripts and avoid redownloading - tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ad34213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "\n",
    "\n",
    "def assimilate_existing_transcripts(\n",
    "    out_dir: Path,\n",
    "    url_map: Dict[str, str],\n",
    "    status: Dict[str, Dict],\n",
    "    legacy_dir: Optional[Path] = None,\n",
    "    rename_to_slug: bool = True,\n",
    "    overwrite: bool = False,\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Find legacy files named like 'ep_1.html' or 'ep-1.html' in out_dir (and optionally legacy_dir),\n",
    "    map them to url_map keys (slugs that contain 'ep-<num>' or '<num>'), update the status dict and\n",
    "    optionally rename/move them to the slug-based filename.\n",
    "\n",
    "    Args:\n",
    "        out_dir: Path where new slug files should live.\n",
    "        url_map: mapping slug -> url (used to find matching slug for an ep number).\n",
    "        status: status dict to update in-place (returned for convenience).\n",
    "        legacy_dir: optional extra directory to check for files (if your old files live elsewhere).\n",
    "        rename_to_slug: if True, move/rename legacy file to new slug filename (safe move).\n",
    "        overwrite: if True, allow overwriting existing slug files (be careful).\n",
    "\n",
    "    Returns:\n",
    "        Updated status dict (mutated in-place).\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    candidates = []\n",
    "\n",
    "    _LEGACY_EP_RE = re.compile(r\"ep[_\\-]?(\\d+)\\.html$\", flags=re.IGNORECASE)\n",
    "\n",
    "    # collect files to examine from out_dir\n",
    "    for p in out_dir.glob(\"*.html\"):\n",
    "        candidates.append(p)\n",
    "\n",
    "    # also check legacy_dir if provided\n",
    "    if legacy_dir:\n",
    "        legacy_dir = Path(legacy_dir)\n",
    "        if legacy_dir.exists():\n",
    "            for p in legacy_dir.glob(\"*.html\"):\n",
    "                # avoid double-adding files that are already in out_dir (same path)\n",
    "                if p.resolve() not in [c.resolve() for c in candidates]:\n",
    "                    candidates.append(p)\n",
    "\n",
    "    # build reverse map: number_str -> list of slugs that contain that number token\n",
    "    # e.g. '1' -> ['ep-1-john-doe', 'ep-1-other']\n",
    "    num_to_slugs = {}\n",
    "    for slug in url_map.keys():\n",
    "        # find first number token like ep-<num> or ep<num>\n",
    "        m = re.search(r\"ep[-_]?(?P<num>\\d+)\", slug, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            num = m.group(\"num\")\n",
    "            num_to_slugs.setdefault(num, []).append(slug)\n",
    "        else:\n",
    "            # also consider bare numbers anywhere (e.g. 'episode-23-guest')\n",
    "            m2 = re.search(r\"(?<!\\d)(\\d+)(?!\\d)\", slug)\n",
    "            if m2:\n",
    "                num = m2.group(1)\n",
    "                num_to_slugs.setdefault(num, []).append(slug)\n",
    "\n",
    "    summary = {\"found\": 0, \"mapped\": 0, \"renamed\": 0, \"skipped\": 0}\n",
    "\n",
    "    for p in candidates:\n",
    "        name = p.name\n",
    "        m = _LEGACY_EP_RE.match(name)\n",
    "        if not m:\n",
    "            # not a legacy ep_N file; ignore\n",
    "            continue\n",
    "        summary[\"found\"] += 1\n",
    "        epnum = m.group(1)\n",
    "\n",
    "        # find candidate slugs for this ep number\n",
    "        candidates_for_num = num_to_slugs.get(epnum, [])\n",
    "\n",
    "        if not candidates_for_num:\n",
    "            # no matching slug for the number — skip for now\n",
    "            logger.debug(\"Found legacy file %s but no slug contains ep-%s; skipping\", name, epnum)\n",
    "            summary[\"skipped\"] += 1\n",
    "            continue\n",
    "\n",
    "        # If multiple slugs match one number, prefer exact 'ep-<num>' prefix match\n",
    "        chosen_slug = None\n",
    "        for s in candidates_for_num:\n",
    "            if re.match(fr\"^ep[-_]?{epnum}(\\b|-|$)\", s, flags=re.IGNORECASE):\n",
    "                chosen_slug = s\n",
    "                break\n",
    "        if chosen_slug is None:\n",
    "            chosen_slug = candidates_for_num[0]\n",
    "\n",
    "        # Build destination path for slug file\n",
    "        safe_slug = _sanitize_key(chosen_slug)\n",
    "        dest = out_dir / f\"{safe_slug}.html\"\n",
    "\n",
    "        # If dest already exists and is same file, just update status\n",
    "        try:\n",
    "            if dest.exists() and dest.resolve() == p.resolve():\n",
    "                logger.debug(\"Legacy file %s already at desired location %s\", p, dest)\n",
    "            elif dest.exists() and not overwrite:\n",
    "                # dest already exists (someone downloaded or moved it earlier) -> skip rename but update status to point to dest\n",
    "                logger.info(\"Destination %s exists; skipping move of %s (overwrite=False)\", dest, p)\n",
    "                summary[\"skipped\"] += 1\n",
    "            else:\n",
    "                if rename_to_slug:\n",
    "                    # move (or copy+unlink) legacy file to dest in a safe manner\n",
    "                    logger.info(\"Renaming/moving legacy file %s -> %s\", p, dest)\n",
    "                    # ensure parent exists\n",
    "                    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    # use shutil.move to preserve contents; if same filesystem this is cheap\n",
    "                    shutil.move(str(p), str(dest))\n",
    "                    summary[\"renamed\"] += 1\n",
    "                else:\n",
    "                    # don't move but use p as saved_path\n",
    "                    dest = p\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Failed to move/inspect legacy file %s: %s\", p, e)\n",
    "            summary[\"skipped\"] += 1\n",
    "            continue\n",
    "\n",
    "        # Update status entry for chosen slug\n",
    "        meta = status.setdefault(chosen_slug, {\"url\": url_map.get(chosen_slug), \"attempts\": 0, \"status\": \"pending\", \"saved_path\": None, \"last_error\": None})\n",
    "        meta.update({\n",
    "            \"attempts\": max(meta.get(\"attempts\", 0), 1),\n",
    "            \"status\": \"success\",\n",
    "            \"saved_path\": str(dest),\n",
    "            \"last_error\": None,\n",
    "        })\n",
    "        logger.info(\"Associated legacy file %s -> slug=%s saved_path=%s\", name, chosen_slug, dest)\n",
    "        summary[\"mapped\"] += 1\n",
    "\n",
    "    logger.info(\"Assimilation summary: %s\", summary)\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "76ac5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_transcripts_legacy(\n",
    "    url_map: Dict[str, str],\n",
    "    out_dir: str,\n",
    "    status_path: str,\n",
    "    max_attempts_per_url: int = 5,\n",
    "    backoff_base: float = 1.0,\n",
    "    max_workers: int = 3,\n",
    "    session: Optional[requests.Session] = None,\n",
    "    timeout: float = 12.0,\n",
    "    legacy_dir = None\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Download URLs to out_dir using url_map (keys are slugs used as filenames).\n",
    "    Added logging provides visibility into what the function does on each run.\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    status_path = Path(status_path)\n",
    "\n",
    "    logger.info(\"Starting download_transcripts: %d urls, out_dir=%s, status_path=%s\",\n",
    "                len(url_map), out_dir, status_path)\n",
    "\n",
    "    # Load existing status if present (allows resume)\n",
    "    if status_path.exists():\n",
    "        try:\n",
    "            with open(status_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                status = json.load(f)\n",
    "            logger.debug(\"Loaded existing status.json with %d entries\", len(status))\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Failed to load status.json (%s). Starting with empty status.\", e)\n",
    "            status = {}\n",
    "    else:\n",
    "        logger.debug(\"No status.json file found at %s. Starting fresh.\", status_path)\n",
    "        status = {}\n",
    "\n",
    "    # Initialize status entries for any missing keys (log each new init)\n",
    "    for key, url in url_map.items():\n",
    "        if key not in status:\n",
    "            status[key] = {\n",
    "                \"url\": url,\n",
    "                \"attempts\": 0,\n",
    "                \"status\": \"pending\",  # pending | success | failed\n",
    "                \"saved_path\": None,\n",
    "                \"last_error\": None,\n",
    "            }\n",
    "            logger.debug(\"Initialized status for key='%s' -> %s\", key, url)\n",
    "\n",
    "    # try assimilating legacy files in out_dir and a legacy folder (if you have one)\n",
    "    if legacy_dir:\n",
    "        status = assimilate_existing_transcripts(out_dir=out_dir, url_map=url_map, status=status, legacy_dir=legacy_dir, rename_to_slug=True, overwrite=False)\n",
    "        # persist immediately so the status file reflects these existing files\n",
    "        with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(status, f, indent=2)\n",
    "        logger.debug(\"Persisted status.json after assimilating legacy transcripts.\")\n",
    "\n",
    "    # Use a single session for pooling\n",
    "    session = session or requests.Session()\n",
    "\n",
    "    def _attempt_download(key: str, meta: Dict) -> Dict:\n",
    "        url = meta[\"url\"]\n",
    "        attempts = meta[\"attempts\"]\n",
    "        result = dict(meta)\n",
    "\n",
    "        # If already succeeded, skip and log reason\n",
    "        if meta.get(\"status\") == \"success\":\n",
    "            logger.debug(\"Skipping key='%s' (already success, saved_path=%s)\", key, meta.get(\"saved_path\"))\n",
    "            return result\n",
    "\n",
    "        # If max attempts reached, log and skip\n",
    "        if attempts >= max_attempts_per_url:\n",
    "            result[\"status\"] = \"failed\"\n",
    "            result[\"last_error\"] = \"max_attempts_reached\"\n",
    "            logger.info(\"Key='%s' reached max attempts (%d). Marking failed.\", key, attempts)\n",
    "            return result\n",
    "\n",
    "        # Log the attempt about to be made\n",
    "        logger.debug(\"Attempting key='%s' (attempt %d) -> %s\", key, attempts + 1, url)\n",
    "        try:\n",
    "            headers = _choose_headers()\n",
    "            resp = session.get(url, headers=headers, timeout=timeout)\n",
    "\n",
    "            # If success (200)\n",
    "            if resp.status_code == 200:\n",
    "                safe_key = _sanitize_key(key)\n",
    "                filename = f\"{safe_key}.html\"\n",
    "                saved_path = str(out_dir / filename)\n",
    "\n",
    "                # If file already exists, log that we're overwriting (helps debug)\n",
    "                if Path(saved_path).exists():\n",
    "                    logger.debug(\"File %s already exists and will be overwritten by key='%s'\", saved_path, key)\n",
    "\n",
    "                with open(saved_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "                    fh.write(resp.text)\n",
    "\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"success\",\n",
    "                    \"saved_path\": saved_path,\n",
    "                    \"last_error\": None,\n",
    "                })\n",
    "                logger.info(\"Saved %s -> %s (key=%s)\", url, saved_path, key)\n",
    "                return result\n",
    "\n",
    "            # Retryable status codes\n",
    "            if resp.status_code in (429, 500, 502, 503, 504):\n",
    "                # Parse Retry-After header if present and include in result\n",
    "                retry_after_raw = resp.headers.get(\"Retry-After\")\n",
    "                retry_after_seconds = _parse_retry_after(retry_after_raw)\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"pending\",\n",
    "                    \"last_error\": f\"status_{resp.status_code}\",\n",
    "                    \"retry_after_seconds\": retry_after_seconds,\n",
    "                })\n",
    "                logger.warning(\"Retryable HTTP %s for key='%s' url=%s (attempt %s)\",\n",
    "                               resp.status_code, key, url, attempts + 1)\n",
    "                # Log headers optionally for 429 to see Retry-After\n",
    "                if resp.status_code == 429:\n",
    "                    logger.debug(\"429 response headers for key='%s': Retry-After=%s\", key, retry_after_raw)\n",
    "                    logger.debug(\"Parsed Retry-After seconds for key='%s': %s\", key, retry_after_seconds)\n",
    "                return result\n",
    "\n",
    "            # Non-retryable\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"failed\",\n",
    "                \"last_error\": f\"status_{resp.status_code}\"\n",
    "            })\n",
    "            logger.error(\"Non-retryable HTTP %s for key='%s' url=%s\", resp.status_code, key, url)\n",
    "            return result\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            # Network error: retryable\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"pending\",\n",
    "                \"last_error\": repr(e)\n",
    "            })\n",
    "            logger.warning(\"RequestException for key='%s' url=%s (attempt %s): %s\", key, url, attempts + 1, e)\n",
    "            return result\n",
    "\n",
    "    # Worker wrapper with backoff\n",
    "    def _worker_task(key):\n",
    "        meta = status[key]\n",
    "        if meta.get(\"status\") == \"success\" or meta.get(\"attempts\", 0) >= max_attempts_per_url:\n",
    "            return key, meta\n",
    "\n",
    "        new_meta = _attempt_download(key, meta)\n",
    "\n",
    "        if new_meta[\"status\"] == \"pending\":\n",
    "            # computed exponential backoff (what we would do)\n",
    "            comp_sleep = backoff_base * (2 ** (new_meta[\"attempts\"] - 1))\n",
    "            jitter = random.uniform(0, 1.0)\n",
    "            computed_sleep = comp_sleep + jitter\n",
    "\n",
    "            # server-provided advice (if any)\n",
    "            retry_after = new_meta.get(\"retry_after_seconds\")\n",
    "            if retry_after is not None:\n",
    "                # use the server's suggestion if it's longer than our computed wait\n",
    "                sleep_time = max(computed_sleep, float(retry_after))\n",
    "            else:\n",
    "                sleep_time = computed_sleep\n",
    "\n",
    "            # cap to avoid runaway sleeps (adjust cap as desired)\n",
    "            sleep_time = min(sleep_time, 600.0)\n",
    "\n",
    "            logger.info(\"Backing off %0.2fs for key='%s' (attempt %s) [computed=%0.2fs, server=%s]\",\n",
    "                        sleep_time, key, new_meta[\"attempts\"], computed_sleep, retry_after)\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        return key, new_meta\n",
    "\n",
    "    # Main loop\n",
    "    pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "    round_idx = 0\n",
    "    while pending_keys:\n",
    "        round_idx += 1\n",
    "        logger.info(\"Download round %d: %d pending\", round_idx, len(pending_keys))\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futures = {ex.submit(_worker_task, key): key for key in pending_keys}\n",
    "            for fut in as_completed(futures):\n",
    "                key = futures[fut]\n",
    "                try:\n",
    "                    k, new_meta = fut.result()\n",
    "                    status[k].update(new_meta)\n",
    "                except Exception as e:\n",
    "                    logger.exception(\"Unhandled exception for key %s: %s\", key, e)\n",
    "                    status[key][\"attempts\"] = status[key].get(\"attempts\", 0) + 1\n",
    "                    status[key][\"last_error\"] = repr(e)\n",
    "\n",
    "        # persist status to disk after every round\n",
    "        try:\n",
    "            with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(status, f, indent=2)\n",
    "            logger.debug(\"Persisted status.json (round %d).\", round_idx)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Failed to write status file: %s\", e)\n",
    "\n",
    "        # Prepare next round\n",
    "        pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "\n",
    "        if pending_keys:\n",
    "            logger.info(\"Sleeping 2s between rounds to be polite...\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    # final persist and summary\n",
    "    with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(status, f, indent=2)\n",
    "\n",
    "    # Final summary counts\n",
    "    succ = sum(1 for v in status.values() if v.get(\"status\") == \"success\")\n",
    "    failed = sum(1 for v in status.values() if v.get(\"status\") == \"failed\")\n",
    "    pending = sum(1 for v in status.values() if v.get(\"status\") == \"pending\")\n",
    "    logger.info(\"Download finished. success=%d failed=%d pending=%d\", succ, failed, pending)\n",
    "\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7972c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrate_scraper_legacy(\n",
    "    df,                     # DataFrame with 'slug' and optionally 'url'\n",
    "    base_url,               # base URL for constructing URLs if df has no 'url' column\n",
    "    out_dir,                # folder to save HTML transcripts\n",
    "    max_attempts_per_url=5,\n",
    "    backoff_base=1.0,\n",
    "    max_workers=3,\n",
    "    timeout=12.0,\n",
    "    legacy_dir = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Orchestrates the scraping process:\n",
    "      1. Prepares a slug → URL map\n",
    "      2. Ensures output folder exists\n",
    "      3. Calls download_transcripts() with sensible defaults\n",
    "      4. Returns the status dict for all downloads\n",
    "\n",
    "    Args:\n",
    "        df: dataframe (not filepath) with slugs and urls in (also raw titles, guest names)\n",
    "        base_url: The base url for the transcripts from podscripts.com\n",
    "        out_dir: The folder to save the transcripts to\n",
    "        max_attempts_per_url\n",
    "        backoff_base\n",
    "        max_workers\n",
    "        timeout\n",
    "    \"\"\"\n",
    "    # ---------------------\n",
    "    # Setup logger for this run\n",
    "    # ---------------------\n",
    "    logger = configure_logger()\n",
    "    logger.info(\"Starting scraper orchestration for %d episodes\", len(df))\n",
    "\n",
    "    # ---------------------\n",
    "    # Prepare URL map\n",
    "    # ---------------------\n",
    "    if \"url\" in df.columns:\n",
    "        url_map = {row[\"slug\"]: row[\"url\"] for _, row in df.iterrows()}\n",
    "        logger.info(\"Using existing URLs from DataFrame\")\n",
    "    else:\n",
    "        url_map = {row[\"slug\"]: base_url.rstrip(\"/\") + \"/\" + row[\"slug\"].lstrip(\"/\") for _, row in df.iterrows()}\n",
    "        logger.info(\"Constructed URLs from base_url and slugs\")\n",
    "\n",
    "    # ---------------------\n",
    "    # Ensure output folder exists\n",
    "    # ---------------------\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    status_path = out_dir / \"status.json\"\n",
    "\n",
    "    # ---------------------\n",
    "    # Call the scraper\n",
    "    # ---------------------\n",
    "    logger.info(\"Running download_transcripts with %d URLs\", len(url_map))\n",
    "    status = download_transcripts_legacy(\n",
    "        url_map=url_map,\n",
    "        out_dir=out_dir,\n",
    "        status_path=status_path,\n",
    "        max_attempts_per_url=max_attempts_per_url,\n",
    "        backoff_base=backoff_base,\n",
    "        max_workers=max_workers,\n",
    "        timeout=timeout,\n",
    "        legacy_dir = legacy_dir\n",
    "    )\n",
    "\n",
    "    logger.info(\"Scraper orchestration finished\")\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "18935a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-2\n",
      "-3\n",
      "-4\n",
      "-5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guest_name</th>\n",
       "      <th>url</th>\n",
       "      <th>slug</th>\n",
       "      <th>restaurants_mentioned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>Scroobius Pip</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-1-scroobius-pip</td>\n",
       "      <td>[Oli Babas Kerb Camden]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Grace Dent</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-2-grace-dent</td>\n",
       "      <td>[Little Owl, Trullo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>Richard Osman</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-3-richard-osman</td>\n",
       "      <td>[Five Guys, Cora Pearl, Berners Tavern]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Nish Kumar</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-4-nish-kumar</td>\n",
       "      <td>[Bademiya, The Owl and The Pussycat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Aisling Bea</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-5-aisling-bea</td>\n",
       "      <td>[Cafe Gratitude, Burger and Lobster]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        guest_name                                                url  \\\n",
       "321  Scroobius Pip  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "320     Grace Dent  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "319  Richard Osman  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "318     Nish Kumar  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "317    Aisling Bea  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "\n",
       "                   slug                    restaurants_mentioned  \n",
       "321  ep-1-scroobius-pip                  [Oli Babas Kerb Camden]  \n",
       "320     ep-2-grace-dent                     [Little Owl, Trullo]  \n",
       "319  ep-3-richard-osman  [Five Guys, Cora Pearl, Berners Tavern]  \n",
       "318     ep-4-nish-kumar     [Bademiya, The Owl and The Pussycat]  \n",
       "317    ep-5-aisling-bea     [Cafe Gratitude, Burger and Lobster]  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the test data (episodes 1-5)\n",
    "\n",
    "indices_to_slice_3 = range(-1, -6, -1)\n",
    "\n",
    "for num in indices_to_slice_3:\n",
    "    print(num)\n",
    "\n",
    "first_five_eps_test_metadata_for_legacy_matcher = full_episodes_metadata_test_df.iloc[indices_to_slice_3]\n",
    "first_five_eps_test_metadata_for_legacy_matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "72d4ff28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 10:54:55,134 INFO: Starting scraper orchestration for 5 episodes\n",
      "2025-12-01 10:54:55,134 INFO: Using existing URLs from DataFrame\n",
      "2025-12-01 10:54:55,137 INFO: Running download_transcripts with 5 URLs\n",
      "2025-12-01 10:54:55,137 INFO: Starting download_transcripts: 5 urls, out_dir=c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp, status_path=c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\status.json\n",
      "2025-12-01 10:54:55,158 DEBUG: Loaded existing status.json with 16 entries\n",
      "2025-12-01 10:54:55,212 INFO: Assimilation summary: {'found': 0, 'mapped': 0, 'renamed': 0, 'skipped': 0}\n",
      "2025-12-01 10:54:55,212 DEBUG: Persisted status.json after assimilating legacy transcripts.\n",
      "2025-12-01 10:54:55,221 INFO: Download finished. success=16 failed=0 pending=0\n",
      "2025-12-01 10:54:55,221 INFO: Scraper orchestration finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ep-217-ross-noble-christmas-special': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-217-ross-noble-christmas-special',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'C:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-217-ross-noble-christmas-special.html',\n",
       "  'last_error': None},\n",
       " 'ep-207-nick-frost': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-207-nick-frost',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'C:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-207-nick-frost.html',\n",
       "  'last_error': None},\n",
       " 'ep-197-jenny-eclair': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-197-jenny-eclair',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'C:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-197-jenny-eclair.html',\n",
       "  'last_error': None},\n",
       " 'ep-187-lily-allen': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-187-lily-allen',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'C:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-187-lily-allen.html',\n",
       "  'last_error': None},\n",
       " 'ep-178-fern-brady': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-178-fern-brady',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'C:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-178-fern-brady.html',\n",
       "  'last_error': None},\n",
       " 'ep-169-ania-magliano': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-169-ania-magliano',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'C:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-169-ania-magliano.html',\n",
       "  'last_error': None},\n",
       " 'ep-159-felicity-ward': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-159-felicity-ward',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'C:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-159-felicity-ward.html',\n",
       "  'last_error': None},\n",
       " 'ep-149-adam-buxton': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-149-adam-buxton',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'C:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-149-adam-buxton.html',\n",
       "  'last_error': None},\n",
       " 'ep-139-nadiya-hussain': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-139-nadiya-hussain',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'C:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-139-nadiya-hussain.html',\n",
       "  'last_error': None},\n",
       " 'ep-129-jason-reitman': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-129-jason-reitman',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'C:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-129-jason-reitman.html',\n",
       "  'last_error': None},\n",
       " 'ep-119-jamie-oliver': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-119-jamie-oliver',\n",
       "  'attempts': 2,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'C:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-119-jamie-oliver.html',\n",
       "  'last_error': None,\n",
       "  'retry_after_seconds': 56.0},\n",
       " 'ep-1-scroobius-pip': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-1-scroobius-pip',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'c:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-1-scroobius-pip.html',\n",
       "  'last_error': None},\n",
       " 'ep-2-grace-dent': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-2-grace-dent',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'c:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-2-grace-dent.html',\n",
       "  'last_error': None},\n",
       " 'ep-3-richard-osman': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-3-richard-osman',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'c:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-3-richard-osman.html',\n",
       "  'last_error': None},\n",
       " 'ep-4-nish-kumar': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-4-nish-kumar',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'c:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-4-nish-kumar.html',\n",
       "  'last_error': None},\n",
       " 'ep-5-aisling-bea': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-5-aisling-bea',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'c:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-5-aisling-bea.html',\n",
       "  'last_error': None}}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test legacy matching scraper\n",
    "\n",
    "\n",
    "orchestrate_scraper_legacy(\n",
    "    first_five_eps_test_metadata_for_legacy_matcher,\n",
    "    transcript_base_url,\n",
    "    Test_data_dir,\n",
    "    5,\n",
    "    1,\n",
    "    2,\n",
    "    12,\n",
    "    Test_data_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df962720",
   "metadata": {},
   "source": [
    "## Pre legacy integration scraper version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "85dfeb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to set up logger for new web scraper\n",
    "def configure_logger(log_file: Optional[str] = None, level: int = logging.DEBUG):\n",
    "    \"\"\"\n",
    "    Configure a compact logger for the scraper.\n",
    "    - Console handler always enabled.\n",
    "    - Optional file handler if log_file provided.\n",
    "    - Default level: DEBUG for maximum visibility while testing.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"scraper\")\n",
    "    logger.setLevel(level)\n",
    "\n",
    "    # Avoid adding handlers multiple times when running multiple times in a notebook\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    # Console handler (clear, one-line format)\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(level)\n",
    "    ch.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s: %(message)s\"))\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    # Optional file handler (rotating not necessary here — keep simple)\n",
    "    if log_file:\n",
    "        fh = logging.FileHandler(log_file, encoding=\"utf-8\")\n",
    "        fh.setLevel(level)\n",
    "        fh.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s: %(message)s\"))\n",
    "        logger.addHandler(fh)\n",
    "\n",
    "    return logger\n",
    "\n",
    "# small sanitize helper (same as before)\n",
    "def _sanitize_key(key: str) -> str:\n",
    "    if not isinstance(key, str):\n",
    "        key = str(key)\n",
    "    s = key.strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\-_]+\", \"-\", s)\n",
    "    s = re.sub(r\"-{2,}\", \"-\", s)\n",
    "    return s.strip(\"-_\")\n",
    "\n",
    "# ---- Helper: random-ish UA list (small) ----\n",
    "_SIMPLE_USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "]\n",
    "\n",
    "def _choose_headers():\n",
    "    return {\"User-Agent\": random.choice(_SIMPLE_USER_AGENTS)}\n",
    "\n",
    "# ----- Helper to access retry limits from the server (for use in scraper)\n",
    "def _parse_retry_after(header_value: Optional[str]) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Parse Retry-After header. It can be:\n",
    "      - an integer number of seconds, e.g. \"120\"\n",
    "      - a HTTP-date string, e.g. \"Wed, 21 Oct 2015 07:28:00 GMT\"\n",
    "    Return number of seconds to wait (float), or None if not parseable.\n",
    "    \"\"\"\n",
    "    if not header_value:\n",
    "        return None\n",
    "    header_value = header_value.strip()\n",
    "    # try integer seconds\n",
    "    if header_value.isdigit():\n",
    "        try:\n",
    "            return float(header_value)\n",
    "        except Exception:\n",
    "            return None\n",
    "    # try HTTP-date\n",
    "    try:\n",
    "        dt = parsedate_to_datetime(header_value)\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        now = datetime.now(timezone.utc)\n",
    "        delta = (dt - now).total_seconds()\n",
    "        return max(0.0, float(delta))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "315dd2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_transcripts(\n",
    "    url_map: Dict[str, str],\n",
    "    out_dir: str,\n",
    "    status_path: str,\n",
    "    max_attempts_per_url: int = 5,\n",
    "    backoff_base: float = 1.0,\n",
    "    max_workers: int = 3,\n",
    "    session: Optional[requests.Session] = None,\n",
    "    timeout: float = 12.0,\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Download URLs to out_dir using url_map (keys are slugs used as filenames).\n",
    "    Added logging provides visibility into what the function does on each run.\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    status_path = Path(status_path)\n",
    "\n",
    "    logger.info(\"Starting download_transcripts: %d urls, out_dir=%s, status_path=%s\",\n",
    "                len(url_map), out_dir, status_path)\n",
    "\n",
    "    # Load existing status if present (allows resume)\n",
    "    if status_path.exists():\n",
    "        try:\n",
    "            with open(status_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                status = json.load(f)\n",
    "            logger.debug(\"Loaded existing status.json with %d entries\", len(status))\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Failed to load status.json (%s). Starting with empty status.\", e)\n",
    "            status = {}\n",
    "    else:\n",
    "        logger.debug(\"No status.json file found at %s. Starting fresh.\", status_path)\n",
    "        status = {}\n",
    "\n",
    "    # Initialize status entries for any missing keys (log each new init)\n",
    "    for key, url in url_map.items():\n",
    "        if key not in status:\n",
    "            status[key] = {\n",
    "                \"url\": url,\n",
    "                \"attempts\": 0,\n",
    "                \"status\": \"pending\",  # pending | success | failed\n",
    "                \"saved_path\": None,\n",
    "                \"last_error\": None,\n",
    "            }\n",
    "            logger.debug(\"Initialized status for key='%s' -> %s\", key, url)\n",
    "\n",
    "    # Use a single session for pooling\n",
    "    session = session or requests.Session()\n",
    "\n",
    "    def _attempt_download(key: str, meta: Dict) -> Dict:\n",
    "        url = meta[\"url\"]\n",
    "        attempts = meta[\"attempts\"]\n",
    "        result = dict(meta)\n",
    "\n",
    "        # If already succeeded, skip and log reason\n",
    "        if meta.get(\"status\") == \"success\":\n",
    "            logger.debug(\"Skipping key='%s' (already success, saved_path=%s)\", key, meta.get(\"saved_path\"))\n",
    "            return result\n",
    "\n",
    "        # If max attempts reached, log and skip\n",
    "        if attempts >= max_attempts_per_url:\n",
    "            result[\"status\"] = \"failed\"\n",
    "            result[\"last_error\"] = \"max_attempts_reached\"\n",
    "            logger.info(\"Key='%s' reached max attempts (%d). Marking failed.\", key, attempts)\n",
    "            return result\n",
    "\n",
    "        # Log the attempt about to be made\n",
    "        logger.debug(\"Attempting key='%s' (attempt %d) -> %s\", key, attempts + 1, url)\n",
    "        try:\n",
    "            headers = _choose_headers()\n",
    "            resp = session.get(url, headers=headers, timeout=timeout)\n",
    "\n",
    "            # If success (200)\n",
    "            if resp.status_code == 200:\n",
    "                safe_key = _sanitize_key(key)\n",
    "                filename = f\"{safe_key}.html\"\n",
    "                saved_path = str(out_dir / filename)\n",
    "\n",
    "                # If file already exists, log that we're overwriting (helps debug)\n",
    "                if Path(saved_path).exists():\n",
    "                    logger.debug(\"File %s already exists and will be overwritten by key='%s'\", saved_path, key)\n",
    "\n",
    "                with open(saved_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "                    fh.write(resp.text)\n",
    "\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"success\",\n",
    "                    \"saved_path\": saved_path,\n",
    "                    \"last_error\": None,\n",
    "                })\n",
    "                logger.info(\"Saved %s -> %s (key=%s)\", url, saved_path, key)\n",
    "                return result\n",
    "\n",
    "            # Retryable status codes\n",
    "            if resp.status_code in (429, 500, 502, 503, 504):\n",
    "                # Parse Retry-After header if present and include in result\n",
    "                retry_after_raw = resp.headers.get(\"Retry-After\")\n",
    "                retry_after_seconds = _parse_retry_after(retry_after_raw)\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"pending\",\n",
    "                    \"last_error\": f\"status_{resp.status_code}\",\n",
    "                    \"retry_after_seconds\": retry_after_seconds,\n",
    "                })\n",
    "                logger.warning(\"Retryable HTTP %s for key='%s' url=%s (attempt %s)\",\n",
    "                               resp.status_code, key, url, attempts + 1)\n",
    "                # Log headers optionally for 429 to see Retry-After\n",
    "                if resp.status_code == 429:\n",
    "                    logger.debug(\"429 response headers for key='%s': Retry-After=%s\", key, retry_after_raw)\n",
    "                    logger.debug(\"Parsed Retry-After seconds for key='%s': %s\", key, retry_after_seconds)\n",
    "                return result\n",
    "\n",
    "            # Non-retryable\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"failed\",\n",
    "                \"last_error\": f\"status_{resp.status_code}\"\n",
    "            })\n",
    "            logger.error(\"Non-retryable HTTP %s for key='%s' url=%s\", resp.status_code, key, url)\n",
    "            return result\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            # Network error: retryable\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"pending\",\n",
    "                \"last_error\": repr(e)\n",
    "            })\n",
    "            logger.warning(\"RequestException for key='%s' url=%s (attempt %s): %s\", key, url, attempts + 1, e)\n",
    "            return result\n",
    "\n",
    "    # Worker wrapper with backoff\n",
    "    def _worker_task(key):\n",
    "        meta = status[key]\n",
    "        if meta.get(\"status\") == \"success\" or meta.get(\"attempts\", 0) >= max_attempts_per_url:\n",
    "            return key, meta\n",
    "\n",
    "        new_meta = _attempt_download(key, meta)\n",
    "\n",
    "        if new_meta[\"status\"] == \"pending\":\n",
    "            # computed exponential backoff (what we would do)\n",
    "            comp_sleep = backoff_base * (2 ** (new_meta[\"attempts\"] - 1))\n",
    "            jitter = random.uniform(0, 1.0)\n",
    "            computed_sleep = comp_sleep + jitter\n",
    "\n",
    "            # server-provided advice (if any)\n",
    "            retry_after = new_meta.get(\"retry_after_seconds\")\n",
    "            if retry_after is not None:\n",
    "                # use the server's suggestion if it's longer than our computed wait\n",
    "                sleep_time = max(computed_sleep, float(retry_after))\n",
    "            else:\n",
    "                sleep_time = computed_sleep\n",
    "\n",
    "            # cap to avoid runaway sleeps (adjust cap as desired)\n",
    "            sleep_time = min(sleep_time, 600.0)\n",
    "\n",
    "            logger.info(\"Backing off %0.2fs for key='%s' (attempt %s) [computed=%0.2fs, server=%s]\",\n",
    "                        sleep_time, key, new_meta[\"attempts\"], computed_sleep, retry_after)\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        return key, new_meta\n",
    "\n",
    "    # Main loop\n",
    "    pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "    round_idx = 0\n",
    "    while pending_keys:\n",
    "        round_idx += 1\n",
    "        logger.info(\"Download round %d: %d pending\", round_idx, len(pending_keys))\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futures = {ex.submit(_worker_task, key): key for key in pending_keys}\n",
    "            for fut in as_completed(futures):\n",
    "                key = futures[fut]\n",
    "                try:\n",
    "                    k, new_meta = fut.result()\n",
    "                    status[k].update(new_meta)\n",
    "                except Exception as e:\n",
    "                    logger.exception(\"Unhandled exception for key %s: %s\", key, e)\n",
    "                    status[key][\"attempts\"] = status[key].get(\"attempts\", 0) + 1\n",
    "                    status[key][\"last_error\"] = repr(e)\n",
    "\n",
    "        # persist status to disk after every round\n",
    "        try:\n",
    "            with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(status, f, indent=2)\n",
    "            logger.debug(\"Persisted status.json (round %d).\", round_idx)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Failed to write status file: %s\", e)\n",
    "\n",
    "        # Prepare next round\n",
    "        pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "\n",
    "        if pending_keys:\n",
    "            logger.info(\"Sleeping 2s between rounds to be polite...\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    # final persist and summary\n",
    "    with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(status, f, indent=2)\n",
    "\n",
    "    # Final summary counts\n",
    "    succ = sum(1 for v in status.values() if v.get(\"status\") == \"success\")\n",
    "    failed = sum(1 for v in status.values() if v.get(\"status\") == \"failed\")\n",
    "    pending = sum(1 for v in status.values() if v.get(\"status\") == \"pending\")\n",
    "    logger.info(\"Download finished. success=%d failed=%d pending=%d\", succ, failed, pending)\n",
    "\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "82b0f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orchestration function for the scraper, to scrape transcripts\n",
    "# Replaces extract_and_save_transcripts_html\n",
    "def orchestrate_scraper(\n",
    "    df,                     # DataFrame with 'slug' and optionally 'url'\n",
    "    base_url,               # base URL for constructing URLs if df has no 'url' column\n",
    "    out_dir,                # folder to save HTML transcripts\n",
    "    max_attempts_per_url=5,\n",
    "    backoff_base=1.0,\n",
    "    max_workers=3,\n",
    "    timeout=12.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Orchestrates the scraping process:\n",
    "      1. Prepares a slug → URL map\n",
    "      2. Ensures output folder exists\n",
    "      3. Calls download_transcripts() with sensible defaults\n",
    "      4. Returns the status dict for all downloads\n",
    "\n",
    "    Args:\n",
    "        df: dataframe (not filepath) with slugs and urls in (also raw titles, guest names)\n",
    "        base_url: The base url for the transcripts from podscripts.com\n",
    "        out_dir: The folder to save the transcripts to\n",
    "        max_attempts_per_url\n",
    "        backoff_base\n",
    "        max_workers\n",
    "        timeout\n",
    "    \"\"\"\n",
    "    # ---------------------\n",
    "    # Setup logger for this run\n",
    "    # ---------------------\n",
    "    logger = configure_logger()\n",
    "    logger.info(\"Starting scraper orchestration for %d episodes\", len(df))\n",
    "\n",
    "    # ---------------------\n",
    "    # Prepare URL map\n",
    "    # ---------------------\n",
    "    if \"url\" in df.columns:\n",
    "        url_map = {row[\"slug\"]: row[\"url\"] for _, row in df.iterrows()}\n",
    "        logger.info(\"Using existing URLs from DataFrame\")\n",
    "    else:\n",
    "        url_map = {row[\"slug\"]: base_url.rstrip(\"/\") + \"/\" + row[\"slug\"].lstrip(\"/\") for _, row in df.iterrows()}\n",
    "        logger.info(\"Constructed URLs from base_url and slugs\")\n",
    "\n",
    "    # ---------------------\n",
    "    # Ensure output folder exists\n",
    "    # ---------------------\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    status_path = out_dir / \"status.json\"\n",
    "\n",
    "    # ---------------------\n",
    "    # Call the scraper\n",
    "    # ---------------------\n",
    "    logger.info(\"Running download_transcripts with %d URLs\", len(url_map))\n",
    "    status = download_transcripts(\n",
    "        url_map=url_map,\n",
    "        out_dir=out_dir,\n",
    "        status_path=status_path,\n",
    "        max_attempts_per_url=max_attempts_per_url,\n",
    "        backoff_base=backoff_base,\n",
    "        max_workers=max_workers,\n",
    "        timeout=timeout\n",
    "    )\n",
    "\n",
    "    logger.info(\"Scraper orchestration finished\")\n",
    "    return status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "eab33a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Episode ID</th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Mention text</th>\n",
       "      <th>Match Score</th>\n",
       "      <th>Match Type</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>transcript_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>john-early</td>\n",
       "      <td>princes hot chicken</td>\n",
       "      <td>you can go to prince's hot chicken? it's not n...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:58:14</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>john-early</td>\n",
       "      <td>hattie bs</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>No match found</td>\n",
       "      <td>None</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kunal-nayyar</td>\n",
       "      <td>moti mahal</td>\n",
       "      <td>ah</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>01:00:27</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kunal-nayyar</td>\n",
       "      <td>the tamil prince</td>\n",
       "      <td>there's a pub, an indian pub called the tamil ...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:32:33</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kunal-nayyar</td>\n",
       "      <td>the dover</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>No match found</td>\n",
       "      <td>None</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>ep-3-richard-osman</td>\n",
       "      <td>cora pearl</td>\n",
       "      <td>this was a difficult question for me until lit...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:13:00</td>\n",
       "      <td>starting point is 00:00:00 hello, listeners of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>ep-3-richard-osman</td>\n",
       "      <td>berners tavern</td>\n",
       "      <td>there's a restaurant near here called the burn...</td>\n",
       "      <td>93</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:38:58</td>\n",
       "      <td>starting point is 00:00:00 hello, listeners of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>ep-2-grace-dent</td>\n",
       "      <td>little owl</td>\n",
       "      <td>it would be, the side dish would be from littl...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:34:52</td>\n",
       "      <td>starting point is 00:00:00 hello, listeners of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>ep-2-grace-dent</td>\n",
       "      <td>trullo</td>\n",
       "      <td>and it's the beef shin ragu with probably it's...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:20:26</td>\n",
       "      <td>starting point is 00:00:00 hello, listeners of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>ep-1-scroobius-pip</td>\n",
       "      <td>oli babas kerb camden</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>No match found</td>\n",
       "      <td>None</td>\n",
       "      <td>starting point is 00:00:00 hello, listeners of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>519 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Episode ID             Restaurant  \\\n",
       "0            john-early    princes hot chicken   \n",
       "1            john-early              hattie bs   \n",
       "2          kunal-nayyar             moti mahal   \n",
       "3          kunal-nayyar       the tamil prince   \n",
       "4          kunal-nayyar              the dover   \n",
       "..                  ...                    ...   \n",
       "514  ep-3-richard-osman             cora pearl   \n",
       "515  ep-3-richard-osman         berners tavern   \n",
       "516     ep-2-grace-dent             little owl   \n",
       "517     ep-2-grace-dent                 trullo   \n",
       "518  ep-1-scroobius-pip  oli babas kerb camden   \n",
       "\n",
       "                                          Mention text  Match Score  \\\n",
       "0    you can go to prince's hot chicken? it's not n...          100   \n",
       "1                                                 None            0   \n",
       "2                                                   ah          100   \n",
       "3    there's a pub, an indian pub called the tamil ...          100   \n",
       "4                                                 None            0   \n",
       "..                                                 ...          ...   \n",
       "514  this was a difficult question for me until lit...          100   \n",
       "515  there's a restaurant near here called the burn...           93   \n",
       "516  it would be, the side dish would be from littl...          100   \n",
       "517  and it's the beef shin ragu with probably it's...          100   \n",
       "518                                               None            0   \n",
       "\n",
       "         Match Type Timestamp  \\\n",
       "0     full, over 90  00:58:14   \n",
       "1    No match found      None   \n",
       "2     full, over 90  01:00:27   \n",
       "3     full, over 90  00:32:33   \n",
       "4    No match found      None   \n",
       "..              ...       ...   \n",
       "514   full, over 90  00:13:00   \n",
       "515   full, over 90  00:38:58   \n",
       "516   full, over 90  00:34:52   \n",
       "517   full, over 90  00:20:26   \n",
       "518  No match found      None   \n",
       "\n",
       "                                     transcript_sample  \n",
       "0    starting point is 00:00:00 oh no, it's james a...  \n",
       "1    starting point is 00:00:00 oh no, it's james a...  \n",
       "2    starting point is 00:00:00 oh no, it's james a...  \n",
       "3    starting point is 00:00:00 oh no, it's james a...  \n",
       "4    starting point is 00:00:00 oh no, it's james a...  \n",
       "..                                                 ...  \n",
       "514  starting point is 00:00:00 hello, listeners of...  \n",
       "515  starting point is 00:00:00 hello, listeners of...  \n",
       "516  starting point is 00:00:00 hello, listeners of...  \n",
       "517  starting point is 00:00:00 hello, listeners of...  \n",
       "518  starting point is 00:00:00 hello, listeners of...  \n",
       "\n",
       "[519 rows x 7 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easy_wins_inspection_path = os.path.join(PROCESSED_DATA_DIR, \"easy_win_mention_search_df.parquet\")\n",
    "\n",
    "easy_wins_df = try_read_parquet(easy_wins_inspection_path)\n",
    "\n",
    "easy_wins_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa553d1",
   "metadata": {},
   "source": [
    "## Pulling restaurants with their associated regions and titles, this includes:\n",
    " - A region subtitle which is a p element preceding the li elements\n",
    " - A title which is an h3 element in the same div as all of it's child li elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a1c5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- V1 Function to make a dataframe of restaurants with associated regions and titles ---\n",
    "# parse_offmenu_restaurants(html str or bs4)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from difflib import get_close_matches\n",
    "import unicodedata\n",
    "\n",
    "def _clean_text(s):\n",
    "    \"\"\"Normalize whitespace, remove weird characters, lowercase.\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s))\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def _normalize_key(s):\n",
    "    \"\"\"Lowercase, remove punctuation that's not helpful for matching.\"\"\"\n",
    "    s = _clean_text(s).lower()\n",
    "    # remove punctuation except ampersand (if you prefer keep)\n",
    "    s = re.sub(r\"[^\\w\\s&]\", \"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def parse_offmenu_restaurants(html):\n",
    "    \"\"\"\n",
    "    Parse Off Menu restaurant list structure into a DataFrame with:\n",
    "    region_header, subtitle, restaurant, guest, raw_html_segment\n",
    "\n",
    "    Args:\n",
    "        html (str or BeautifulSoup): HTML page content (string) or BeautifulSoup object\n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    if isinstance(html, BeautifulSoup):\n",
    "        soup = html\n",
    "    else:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # Find all h3 elements (each h3 belongs to a region container div per your description)\n",
    "    for h3 in soup.find_all(\"h3\"):\n",
    "        region = _clean_text(h3.get_text())\n",
    "        # choose the nearest containing div for this h3\n",
    "        container = h3.find_parent(\"div\")\n",
    "        if container is None:\n",
    "            # fallback: use h3's parent\n",
    "            container = h3.parent\n",
    "\n",
    "        current_subtitle = None\n",
    "\n",
    "        # iterate in document order over p and li tags inside this container\n",
    "        # container.find_all with list preserves document order of matches\n",
    "        for tag in container.find_all([\"p\", \"li\"], recursive=True):\n",
    "            # If this p is a subtitle (contains <strong> and is not inside an li)\n",
    "            if tag.name == \"p\":\n",
    "                # check whether this p is inside an li (we don't want subtitles that sit inside lis)\n",
    "                if tag.find_parent(\"li\") is None:\n",
    "                    strong = tag.find(\"strong\")\n",
    "                    if strong:\n",
    "                        # subtitle candidate\n",
    "                        subtitle_text = _clean_text(strong.get_text())\n",
    "                        if subtitle_text:\n",
    "                            current_subtitle = subtitle_text\n",
    "                            # continue to next element; subtitles don't produce rows themselves\n",
    "                            continue\n",
    "                # else: might be paragraph inside an li -> handled below when li found\n",
    "\n",
    "            # If the tag is an li -> extract restaurant + guest\n",
    "            if tag.name == \"li\":\n",
    "                li = tag\n",
    "                # Extract restaurant name:\n",
    "                # Prefer the <a> text (anchor inside the li). Fallback to any leading text.\n",
    "                rest_name = \"\"\n",
    "                a = li.find(\"a\")\n",
    "                if a and _clean_text(a.get_text()):\n",
    "                    rest_name = _clean_text(a.get_text())\n",
    "                else:\n",
    "                    # Try to take text before any <strong> or bracket\n",
    "                    li_text = _clean_text(li.get_text(separator=\" \", strip=True))\n",
    "                    # remove guest strong text if present\n",
    "                    guest_candidate = li.find(\"strong\")\n",
    "                    guest_text = _clean_text(guest_candidate.get_text()) if guest_candidate else \"\"\n",
    "                    if guest_text:\n",
    "                        # remove guest_text from li_text (first occurrence)\n",
    "                        li_text = re.sub(re.escape(guest_text), \"\", li_text, count=1).strip()\n",
    "                    # often li begins with \"Restaurant — \" or similar; we take up to dash or bracket\n",
    "                    rest_name = li_text.split(\"—\")[0].split(\"(\")[0].strip()\n",
    "\n",
    "                # Extract guest: look for <strong> inside the li that likely holds guest name\n",
    "                guest = \"\"\n",
    "                strongs_in_li = li.find_all(\"strong\")\n",
    "                if strongs_in_li:\n",
    "                    # often the guest name is the last <strong> inside the li\n",
    "                    guest = _clean_text(strongs_in_li[-1].get_text())\n",
    "                    # if the guest looks like the subtitle (some pages might be inconsistent), heuristics:\n",
    "                    # ignore if guest equals restaurant name or is empty\n",
    "                    if guest.lower() in rest_name.lower():\n",
    "                        guest = \"\"\n",
    "\n",
    "                # final cleaning\n",
    "                rest_name = _clean_text(rest_name)\n",
    "                guest = _clean_text(guest)\n",
    "\n",
    "                rows.append({\n",
    "                    \"region_header\": region,\n",
    "                    \"subtitle\": current_subtitle if current_subtitle else \"\",\n",
    "                    \"restaurant\": rest_name,\n",
    "                    \"guest\": guest,\n",
    "                    \"raw_html_segment\": str(li)\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    # some normalization columns for merging later\n",
    "    df[\"restaurant_key\"] = df[\"restaurant\"].apply(_normalize_key)\n",
    "    df[\"guest_key\"] = df[\"guest\"].apply(_normalize_key)\n",
    "    df[\"region_header\"] = df[\"region_header\"].apply(_clean_text)\n",
    "    df[\"subtitle\"] = df[\"subtitle\"].apply(_clean_text)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "652922fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- V2 Function to make a dataframe of restaurants with associated regions and titles ---\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ---------- assume your original _clean_res is present in the same module ----------\n",
    "# def _clean_res(res_element: BeautifulSoup) -> Tuple[str, List[str]]:\n",
    "#     ... (user-provided function, unchanged) ...\n",
    "\n",
    "def _restaurant_key_from_clean_name(clean_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a stable normalized key from the restaurant name that came from your _clean_res.\n",
    "    This intentionally keeps normalization minimal and consistent with your cleaning:\n",
    "      - lower-case\n",
    "      - collapse whitespace\n",
    "      - strip\n",
    "    (We DO NOT re-run aggressive punctuation/parentheses stripping because your _clean_res already did that.)\n",
    "    \"\"\"\n",
    "    if clean_name is None:\n",
    "        return \"\"\n",
    "    k = str(clean_name).lower()\n",
    "    k = re.sub(r\"\\s+\", \" \", k).strip()\n",
    "    return k\n",
    "\n",
    "def parse_restaurants_using_user_cleaners(html_string: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse the Off Menu restaurants page, *using the user's _clean_res function* to extract the\n",
    "    restaurant display name and mentions. Associates each li with its containing h3 region and the\n",
    "    nearest preceding subtitle (<p><strong>) inside the same container.\n",
    "\n",
    "    Returns a DataFrame with columns:\n",
    "      - restaurant (cleaned display name as returned by your _clean_res)\n",
    "      - guests (list of strings from your _clean_res)\n",
    "      - restaurant_key (lowercased collapsed whitespace key for exact matching)\n",
    "      - region_header (text of the h3)\n",
    "      - subtitle (closest subtitle found before the li inside the container)\n",
    "      - raw_html_segment (stringified li HTML)\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_string, \"html.parser\")\n",
    "\n",
    "    rows: List[Dict] = []\n",
    "\n",
    "    # iterate through each h3 region header (your page has two: e.g. \"UNITED KINGDOM AND IRELAND\", \"WORLD\")\n",
    "    for h3 in soup.find_all(\"h3\"):\n",
    "        region_header = h3.get_text(\" \", strip=True)\n",
    "        # pick the nearest parent div container for this h3 (as you described)\n",
    "        container = h3.find_parent(\"div\") or h3.parent\n",
    "\n",
    "        current_subtitle = \"\"  # updated whenever we see a <p> with <strong> not inside an <li>\n",
    "\n",
    "        # Iterate over elements inside this container in document order.\n",
    "        # We look for <p> (subtitle candidates) and <li> (restaurant entries).\n",
    "        for tag in container.find_all(recursive=True):\n",
    "            # Subtitle detection: <p> with <strong>, but not inside an li\n",
    "            if tag.name == \"p\" and tag.find_parent(\"li\") is None:\n",
    "                strong = tag.find(\"strong\")\n",
    "                if strong and strong.get_text(strip=True):\n",
    "                    current_subtitle = strong.get_text(\" \", strip=True)\n",
    "                    # continue scanning; subtitles don't create rows by themselves\n",
    "                    continue\n",
    "\n",
    "            # Restaurant extraction: when encountering <li>\n",
    "            if tag.name == \"li\":\n",
    "                li = tag\n",
    "                li_text = li.get_text(\" \", strip=True)\n",
    "                # keep your original guard: only parse lis that have parentheses (your _clean_res expects that)\n",
    "                # but we'll still try to call _clean_res even if parentheses are absent, to be resilient\n",
    "                try:\n",
    "                    cleaned_name, mentions = _clean_res(li)\n",
    "                except Exception:\n",
    "                    # If your original function errors for some li, fall back to a safe simple extraction\n",
    "                    # (this keeps the scraper robust while preserving your cleaning where possible)\n",
    "                    # fallback: prefer <a> text or text before '('\n",
    "                    a = li.find(\"a\")\n",
    "                    if a and a.get_text(strip=True):\n",
    "                        cleaned_name = a.get_text(\" \", strip=True)\n",
    "                    else:\n",
    "                        if \"(\" in li_text:\n",
    "                            cleaned_name = li_text.split(\"(\", 1)[0].strip()\n",
    "                        else:\n",
    "                            cleaned_name = li_text\n",
    "                    mentions = []\n",
    "                # produce normalized key consistent with your cleaning logic\n",
    "                rest_key = _restaurant_key_from_clean_name(cleaned_name)\n",
    "\n",
    "                rows.append({\n",
    "                    \"region_header\": region_header,\n",
    "                    \"subtitle\": current_subtitle,\n",
    "                    \"restaurant\": cleaned_name,\n",
    "                    \"guests\": mentions,\n",
    "                    \"restaurant_key\": rest_key,\n",
    "                    \"raw_html_segment\": str(li)\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    # defensive: drop exact-duplicate rows (same key + region)\n",
    "    if not df.empty:\n",
    "        df = df.drop_duplicates(subset=[\"restaurant_key\", \"region_header\"])\n",
    "\n",
    "    # keep column ordering friendly\n",
    "    df = df[[\"region_header\", \"subtitle\", \"restaurant\", \"guests\", \"restaurant_key\", \"raw_html_segment\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e989e787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- V3 Function to make a dataframe of restaurants with associated regions and titles ---\n",
    "# Uses my _clean_res as we know this works\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "def _clean_res(res_element: BeautifulSoup) -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Extracts and cleans a restaurant name and its mentions from a BeautifulSoup `li` element.\n",
    "\n",
    "    Args:\n",
    "        res_element (BeautifulSoup): The BeautifulSoup Tag element for a restaurant.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, List[str]]: The cleaned restaurant name and a list of guests who mentioned it.\n",
    "    \"\"\"\n",
    "    text = res_element.text\n",
    "    csplit = text.split(\"(\")\n",
    "    res_name = (\n",
    "        re.sub(r\"[^\\w\\s]\", \"\", csplit[0].strip().replace(\"&\", \"and\"))\n",
    "        .replace(\"é\", \"e\")\n",
    "        .replace(\"ô\", \"o\")\n",
    "    )\n",
    "    res_mentions = csplit[1].strip().strip(\")\").split(\",\")\n",
    "    return (res_name, res_mentions)\n",
    "\n",
    "# ---------- assume your original _clean_res is present in the same module ----------\n",
    "# def _clean_res(res_element: BeautifulSoup) -> Tuple[str, List[str]]:\n",
    "#     ... (user-provided function, unchanged) ...\n",
    "\n",
    "def _restaurant_key_from_clean_name(clean_name: str) -> str:\n",
    "    if clean_name is None:\n",
    "        return \"\"\n",
    "    k = str(clean_name).lower()\n",
    "    k = re.sub(r\"\\s+\", \" \", k).strip()\n",
    "    return k\n",
    "\n",
    "def parse_restaurants_using_user_cleaners_v3(html_string: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Corrected parser that uses the user's _clean_res primarily, with safe fallbacks.\n",
    "    Fixes the bug where <strong> content (sometimes containing both restaurant + guest)\n",
    "    was naively removed and left restaurant blank.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_string, \"html.parser\")\n",
    "\n",
    "    rows: List[Dict] = []\n",
    "\n",
    "    for h3 in soup.find_all(\"h3\"):\n",
    "        region_header = h3.get_text(\" \", strip=True)\n",
    "        container = h3.find_parent(\"div\") or h3.parent\n",
    "\n",
    "        current_subtitle = \"\"\n",
    "\n",
    "        # iterate in document order\n",
    "        for tag in container.find_all(recursive=True):\n",
    "            # subtitle detection: <p><strong> that isn't inside an <li>\n",
    "            if tag.name == \"p\" and tag.find_parent(\"li\") is None:\n",
    "                strong = tag.find(\"strong\")\n",
    "                if strong and strong.get_text(strip=True):\n",
    "                    current_subtitle = strong.get_text(\" \", strip=True)\n",
    "                    continue\n",
    "\n",
    "            # process <li> entries\n",
    "            if tag.name == \"li\":\n",
    "                li = tag\n",
    "                li_text = li.get_text(\" \", strip=True)\n",
    "\n",
    "                # --- Primary path: use the user's _clean_res (trusted) ---\n",
    "                cleaned_name = \"\"\n",
    "                mentions = []\n",
    "                try:\n",
    "                    cleaned_name, mentions = _clean_res(li)\n",
    "                except Exception:\n",
    "                    # If the user's cleaner throws for some reason, we'll fallback below\n",
    "                    cleaned_name, mentions = \"\", []\n",
    "\n",
    "                # --- If _clean_res produced an empty restaurant name, use safer fallbacks ---\n",
    "                if not cleaned_name:\n",
    "                    # 1) Prefer <a> anchor text if present\n",
    "                    a = li.find(\"a\")\n",
    "                    if a and a.get_text(strip=True):\n",
    "                        candidate = a.get_text(\" \", strip=True)\n",
    "                    else:\n",
    "                        # 2) Otherwise take the text before the first '(' or before an em-dash/hyphen\n",
    "                        # This mirrors the heuristic your original cleaner used but without removing STRONG text.\n",
    "                        if \"(\" in li_text:\n",
    "                            candidate = li_text.split(\"(\", 1)[0].strip()\n",
    "                        else:\n",
    "                            candidate = re.split(r\"[—–-]\", li_text, maxsplit=1)[0].strip()\n",
    "\n",
    "                    # apply a light cleaning similar to your _clean_res (but conservative)\n",
    "                    candidate_clean = re.sub(r\"[^\\w\\s]\", \"\", candidate.strip().replace(\"&\", \"and\"))\n",
    "                    candidate_clean = candidate_clean.replace(\"é\", \"e\").replace(\"ô\", \"o\")\n",
    "                    cleaned_name = candidate_clean\n",
    "\n",
    "                    # Try to extract mentions from parentheses if not captured earlier\n",
    "                    paren_matches = re.findall(r\"\\(([^)]*)\\)\", li_text)\n",
    "                    if paren_matches:\n",
    "                        # take last parentheses group (usually guest list)\n",
    "                        last_paren = paren_matches[-1]\n",
    "                        mentions = [m.strip() for m in re.split(r\",\\s*\", last_paren) if m.strip()]\n",
    "                    else:\n",
    "                        # fallback: try to use <strong> inside li if present (last strong often contains guest)\n",
    "                        strongs = [s.get_text(\" \", strip=True) for s in li.find_all(\"strong\")]\n",
    "                        if strongs:\n",
    "                            last = strongs[-1]\n",
    "                            mentions = [m.strip() for m in re.split(r\",\\s*\", last) if m.strip()]\n",
    "\n",
    "                # Build normalized key from the cleaned name\n",
    "                rest_key = _restaurant_key_from_clean_name(cleaned_name)\n",
    "\n",
    "                rows.append({\n",
    "                    \"region_header\": region_header,\n",
    "                    \"subtitle\": current_subtitle,\n",
    "                    \"restaurant\": cleaned_name,\n",
    "                    \"guests\": mentions,\n",
    "                    \"restaurant_key\": rest_key,\n",
    "                    \"raw_html_segment\": str(li)\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if not df.empty:\n",
    "        df = df.drop_duplicates(subset=[\"restaurant_key\", \"region_header\"])\n",
    "\n",
    "    df = df[[\"region_header\", \"subtitle\", \"restaurant\", \"guests\", \"restaurant_key\", \"raw_html_segment\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0de86e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test: Make dataframe of restaurants from restaurants html --- \n",
    "\n",
    "restaurants_html = try_read_html_string_from_filepath(test_restaurants_html_filepath)\n",
    "\n",
    "restaurants_and_regions_df = parse_restaurants_using_user_cleaners_v3(restaurants_html)\n",
    "\n",
    "restaurants_and_regions_df.to_parquet(os.path.join(ANALYTICS_DATA_DIR, \"res_and_regions_df.parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c28cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function to match and merge restaurants/regions with top mentions dataframe\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def _normalize_for_match(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Simple normalizer for exact matching:\n",
    "      - NFKC unicode normalize\n",
    "      - lower-case\n",
    "      - remove punctuation except ampersand (&)\n",
    "      - collapse whitespace\n",
    "      - strip\n",
    "    \"\"\"\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = s.lower()\n",
    "    # keep letters, numbers, spaces, ampersand\n",
    "    s = re.sub(r\"[^0-9a-z&\\s]\", \"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def exact_merge_restaurants(scraped_df: pd.DataFrame,\n",
    "                            mentions_df: pd.DataFrame,\n",
    "                            mentions_rest_col: str = \"Restaurant\",\n",
    "                            scraped_rest_col: str = \"restaurant\"):\n",
    "    \"\"\"\n",
    "    Exact (normalized) match between your mentions dataframe and the scraped restaurants.\n",
    "\n",
    "    Args:\n",
    "        scraped_df: DataFrame produced by your scraper. Must contain column named by scraped_rest_col.\n",
    "        mentions_df: Your top_mentions dataframe. Must contain a column named mentions_rest_col.\n",
    "        mentions_rest_col: column name in mentions_df with restaurant names (default \"Restaurant\")\n",
    "        scraped_rest_col: column name in scraped_df with restaurant names (default \"restaurant\")\n",
    "\n",
    "    Returns:\n",
    "        matched_df: mentions rows that found an exact normalized match, with scraped columns merged (left-join).\n",
    "        unmatched_mentions_df: mentions rows that did NOT match any scraped restaurant.\n",
    "        report: dict with counts {total_mentions, matched, unmatched}\n",
    "    \"\"\"\n",
    "    # Work on copies to avoid mutating original frames\n",
    "    s = scraped_df.copy()\n",
    "    m = mentions_df.copy()\n",
    "\n",
    "    # Ensure columns exist\n",
    "    if scraped_rest_col not in s.columns:\n",
    "        raise KeyError(f\"scraped_df must contain column '{scraped_rest_col}'\")\n",
    "    if mentions_rest_col not in m.columns:\n",
    "        raise KeyError(f\"mentions_df must contain column '{mentions_rest_col}'\")\n",
    "\n",
    "    # Create normalized keys\n",
    "    s[\"_restaurant_key\"] = s[scraped_rest_col].astype(str).apply(_normalize_for_match)\n",
    "    m[\"_restaurant_key\"] = m[mentions_rest_col].astype(str).apply(_normalize_for_match)\n",
    "\n",
    "    # Remove empty keys from scraped (optional)\n",
    "    s = s[s[\"_restaurant_key\"] != \"\"]\n",
    "\n",
    "    # Perform exact (normalized) left join: mentions -> scraped\n",
    "    merged = m.merge(s.drop_duplicates(\"_restaurant_key\"), on=\"_restaurant_key\", how=\"left\", suffixes=(\"_mention\", \"_scrape\"))\n",
    "\n",
    "    # Split matched / unmatched\n",
    "    matched = merged[merged[scraped_rest_col].notna()].copy()\n",
    "    unmatched = merged[merged[scraped_rest_col].isna()].copy()\n",
    "\n",
    "    report = {\n",
    "        \"total_mentions\": len(m),\n",
    "        \"matched\": len(matched),\n",
    "        \"unmatched\": len(unmatched)\n",
    "    }\n",
    "\n",
    "    # Drop internal key column from returned frames to keep things tidy (but keep if you want)\n",
    "    for df in (matched, unmatched):\n",
    "        if \"_restaurant_key\" in df.columns:\n",
    "            df.drop(columns=[\"_restaurant_key\"], inplace=True)\n",
    "\n",
    "    return matched, unmatched, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ba3ebcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report: {'total_mentions': 519, 'matched': 519, 'unmatched': 0}\n",
      "\n",
      " unmatched df head: Empty DataFrame\n",
      "Columns: [Episode ID, Restaurant, Mention text, Match Score, Match Type, Timestamp, transcript_sample, region_header, subtitle, restaurant, guests, restaurant_key, raw_html_segment]\n",
      "Index: []\n",
      "\n",
      " matched df head:      Episode ID           Restaurant  \\\n",
      "0    john-early  princes hot chicken   \n",
      "1    john-early            hattie bs   \n",
      "2  kunal-nayyar           moti mahal   \n",
      "3  kunal-nayyar     the tamil prince   \n",
      "4  kunal-nayyar            the dover   \n",
      "\n",
      "                                        Mention text  Match Score  \\\n",
      "0  you can go to prince's hot chicken? it's not n...          100   \n",
      "1                                               None            0   \n",
      "2                                                 ah          100   \n",
      "3  there's a pub, an indian pub called the tamil ...          100   \n",
      "4                                               None            0   \n",
      "\n",
      "       Match Type Timestamp  \\\n",
      "0   full, over 90  00:58:14   \n",
      "1  No match found      None   \n",
      "2   full, over 90  01:00:27   \n",
      "3   full, over 90  00:32:33   \n",
      "4  No match found      None   \n",
      "\n",
      "                                   transcript_sample  \\\n",
      "0  starting point is 00:00:00 oh no, it's james a...   \n",
      "1  starting point is 00:00:00 oh no, it's james a...   \n",
      "2  starting point is 00:00:00 oh no, it's james a...   \n",
      "3  starting point is 00:00:00 oh no, it's james a...   \n",
      "4  starting point is 00:00:00 oh no, it's james a...   \n",
      "\n",
      "                region_header           subtitle           restaurant  \\\n",
      "0                       WORLD    Nashville (USA)  Princes Hot Chicken   \n",
      "1                       WORLD             Chains            Hattie Bs   \n",
      "2                       WORLD  New Delhi (India)           Moti Mahal   \n",
      "3  UNITED KINGDOM AND IRELAND             London     The Tamil Prince   \n",
      "4  UNITED KINGDOM AND IRELAND             London            The Dover   \n",
      "\n",
      "           guests       restaurant_key  \\\n",
      "0    [John Early]  princes hot chicken   \n",
      "1    [John Early]            hattie bs   \n",
      "2  [Kunal Nayyar]           moti mahal   \n",
      "3  [Kunal Nayyar]     the tamil prince   \n",
      "4  [Kunal Nayyar]            the dover   \n",
      "\n",
      "                                    raw_html_segment  \n",
      "0  <li><p class=\"\" style=\"white-space:pre-wrap;\">...  \n",
      "1  <li><p class=\"\" style=\"white-space:pre-wrap;\">...  \n",
      "2  <li><p class=\"\" style=\"white-space:pre-wrap;\">...  \n",
      "3  <li><p class=\"\" style=\"white-space:pre-wrap;\">...  \n",
      "4  <li><p class=\"\" style=\"white-space:pre-wrap;\">...  \n",
      "\n",
      " matched df cols: Index(['Episode ID', 'Restaurant', 'Mention text', 'Match Score', 'Match Type',\n",
      "       'Timestamp', 'transcript_sample', 'region_header', 'subtitle',\n",
      "       'restaurant', 'guests', 'restaurant_key', 'raw_html_segment'],\n",
      "      dtype='object')\n",
      "Empty DataFrame\n",
      "Columns: [Restaurant, Episode ID, Mention text]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# --- Test: Matching and merging top_matches_df\n",
    "\n",
    "easy_wins_inspection_path = os.path.join(PROCESSED_DATA_DIR, \"easy_win_mention_search_df.parquet\")\n",
    "\n",
    "easy_wins_df = try_read_parquet(easy_wins_inspection_path)\n",
    "\n",
    "matched, unmatched, report = exact_merge_restaurants(restaurants_and_regions_df, easy_wins_df)\n",
    "\n",
    "print(f\"report: {report}\")\n",
    "print(f\"\\n unmatched df head: {unmatched.head()}\")\n",
    "print(f\"\\n matched df head: {matched.head()}\")\n",
    "print(f\"\\n matched df cols: {matched.columns}\")\n",
    "\n",
    "print(unmatched[['Restaurant','Episode ID','Mention text']].head(20).to_string())\n",
    "\n",
    "\n",
    "top_mentions_with_regions_path = os.path.join(PROCESSED_DATA_DIR, \"top_mentions_with_regions.csv\")\n",
    "matched.to_csv(top_mentions_with_regions_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af11624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Streamlit app to display the restaurants by region dataframe ---\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "script_dir = Path(__file__).resolve().parent\n",
    "\n",
    "relative_path = Path(\"..\") / \"data\" / \"analytics\" / \"top_mentions_with_regions.csv\"\n",
    "\n",
    "FINAL_PATH = (script_dir / relative_path).resolve()\n",
    "\n",
    "st.text(str(FINAL_PATH))\n",
    "\n",
    "df = pd.read_parquet(FINAL_PATH)\n",
    "\n",
    "st.write(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e7690",
   "metadata": {},
   "source": [
    "## Pulling 500 characters of text for each mention, adjustments to functions:\n",
    " - find_top_match\n",
    " - additional helpers: _create_highlighted_html ; _expand_context_around_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "61444e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper functions _expand_context_aroudn_match and _create_highlighted_html ---\n",
    "# These will support extraction of more text and highlighting of original match\n",
    "# Adds several entries to each mention dict:\n",
    "#   \"Mention text full\": mention_text_full\n",
    "#   \"Mention text highlighted\": mention_text_highlighted_html\n",
    "# _timecode_to_seconds and _episode_end_seconds for calculating if a match is in final 10% of ep\n",
    "\n",
    "import html\n",
    "\n",
    "def _expand_context_around_match(\n",
    "    transcript: str,\n",
    "    match_start: int,\n",
    "    match_end: int,\n",
    "    target_chars: int = 500\n",
    ") -> Tuple[int, int, str]:\n",
    "    \"\"\"\n",
    "    Expand a context window around [match_start:match_end] inside transcript,\n",
    "    aiming to return approximately target_chars characters (if available).\n",
    "    Returns (context_start, context_end, context_str).\n",
    "    Ensures we don't cut off mid-word at the ends (trim to whitespace boundaries).\n",
    "    \"\"\"\n",
    "    if transcript is None:\n",
    "        return 0, 0, \"\"\n",
    "\n",
    "    n = len(transcript)\n",
    "    # make sure indices are in bounds\n",
    "    match_start = max(0, int(match_start))\n",
    "    match_end = min(n, int(match_end))\n",
    "\n",
    "    # if transcript shorter than target, return whole thing\n",
    "    if n <= target_chars:\n",
    "        return 0, n, transcript\n",
    "\n",
    "    # Aim to give about half before and half after\n",
    "    half = target_chars // 2\n",
    "    start_candidate = max(0, match_start - half)\n",
    "    end_candidate = min(n, match_end + half)\n",
    "\n",
    "    # If we hit boundaries, expand the other side\n",
    "    avail_left = match_start - start_candidate\n",
    "    avail_right = end_candidate - match_end\n",
    "    shortfall = target_chars - (avail_left + (match_end - match_start) + avail_right)\n",
    "    if shortfall > 0:\n",
    "        # extend left if possible, else extend right\n",
    "        extra_left = min(start_candidate, shortfall)\n",
    "        start_candidate = max(0, start_candidate - extra_left)\n",
    "        shortfall -= extra_left\n",
    "    # after attempt above, if still short, extend right\n",
    "    if shortfall > 0:\n",
    "        end_candidate = min(n, end_candidate + shortfall)\n",
    "\n",
    "    # Trim context to nearest word boundary (avoid cutting words)\n",
    "    # Move start forward to next whitespace (if not at 0) and end backward to previous whitespace (if not at n)\n",
    "    if start_candidate > 0:\n",
    "        # move forward to next whitespace to avoid cutting a word\n",
    "        m = re.search(r\"\\s\", transcript[start_candidate:match_start])\n",
    "        if m:\n",
    "            start_candidate = start_candidate + m.start()\n",
    "    if end_candidate < n:\n",
    "        # move backward to previous whitespace to avoid cutting a word\n",
    "        m = re.search(r\"\\s(?=[^\\s]*$)\", transcript[match_end:end_candidate])\n",
    "        # the above is tricky; simpler: find last whitespace before end_candidate\n",
    "        rev = transcript[match_end:end_candidate]\n",
    "        last_space = rev.rfind(\" \")\n",
    "        if last_space != -1:\n",
    "            end_candidate = match_end + last_space\n",
    "\n",
    "    # safety clamping\n",
    "    start_candidate = max(0, int(start_candidate))\n",
    "    end_candidate = min(n, int(end_candidate))\n",
    "    return start_candidate, end_candidate, transcript[start_candidate:end_candidate]\n",
    "\n",
    "\n",
    "def _create_highlighted_html(context_str: str, match_rel_start: int, match_rel_end: int) -> str:\n",
    "    \"\"\"\n",
    "    Escape HTML in context_str, then wrap the slice [match_rel_start:match_rel_end] with <mark>.\n",
    "    match_rel_* are indices relative to context_str.\n",
    "    Returns the HTML string safe to insert with unsafe_allow_html=True in Streamlit.\n",
    "    \"\"\"\n",
    "    if not context_str:\n",
    "        return \"\"\n",
    "\n",
    "    # Ensure indices within bounds\n",
    "    match_rel_start = max(0, int(match_rel_start))\n",
    "    match_rel_end = min(len(context_str), int(match_rel_end))\n",
    "\n",
    "    # Escape full context to avoid accidental HTML injections\n",
    "    esc = html.escape(context_str)\n",
    "\n",
    "    # But escaping shifts indices because certain chars become entities (e.g. '&' -> '&amp;').\n",
    "    # To robustly highlight, simpler approach: split the original context into three parts and escape each.\n",
    "    before = html.escape(context_str[:match_rel_start])\n",
    "    middle = html.escape(context_str[match_rel_start:match_rel_end])\n",
    "    after = html.escape(context_str[match_rel_end:])\n",
    "\n",
    "    # Wrap middle in <mark>\n",
    "    return f\"{before}<mark>{middle}</mark>{after}\"\n",
    "\n",
    "def _strip_starting_point(transcript: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove phrases like 'starting point is 00:00:00' (case-insensitive) from transcript.\n",
    "    Accepts hours with 1 or 2 digits and standard MM:SS, and removes optional trailing punctuation and whitespace.\n",
    "    \"\"\"\n",
    "    if not isinstance(transcript, str) or not transcript:\n",
    "        return transcript or \"\"\n",
    "    # Pattern: starting point is <H>HH:MM:SS  optionally followed by punctuation/spaces\n",
    "    pattern = r\"starting point is\\s*\\d{1,2}:\\d{2}:\\d{2}\\s*[.,;:!?-]?\\s*\"\n",
    "    cleaned = re.sub(pattern, \"\", transcript, flags=re.IGNORECASE)\n",
    "    # Run twice to remove repeated occurrences (idempotent)\n",
    "    cleaned = re.sub(pattern, \"\", cleaned, flags=re.IGNORECASE)\n",
    "    return cleaned\n",
    "\n",
    "# ---------- ADDED: helpers to compute time in seconds and episode end ----------\n",
    "def _timecode_to_seconds(ts: str) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Convert a timecode like '01:23:45' or '1:23:45' to integer seconds.\n",
    "    Returns None if parse fails or ts is falsy.\n",
    "    \"\"\"\n",
    "    if not ts or not isinstance(ts, str):\n",
    "        return None\n",
    "    ts = ts.strip()\n",
    "    m = re.match(r\"^(\\d{1,2}):(\\d{2}):(\\d{2})$\", ts)\n",
    "    if not m:\n",
    "        return None\n",
    "    hours, mins, secs = m.groups()\n",
    "    try:\n",
    "        return int(hours) * 3600 + int(mins) * 60 + int(secs)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _episode_end_seconds(periodic_timestamps: Any) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Robustly return episode length in seconds using the last timestamp entry from periodic_timestamps.\n",
    "\n",
    "    Accepts:\n",
    "      - None -> returns None\n",
    "      - list/tuple/np.ndarray/pd.Series of dicts or strings\n",
    "      - a single dict or string (will treat as length-1 list)\n",
    "    Returns: seconds (int) or None\n",
    "    \"\"\"\n",
    "    if periodic_timestamps is None:\n",
    "        return None\n",
    "\n",
    "    # If it's a single dict or string, wrap it\n",
    "    if isinstance(periodic_timestamps, dict) or isinstance(periodic_timestamps, str):\n",
    "        periodic_timestamps = [periodic_timestamps]\n",
    "\n",
    "    # If it's a numpy array or pandas Series, convert to list\n",
    "    if isinstance(periodic_timestamps, (np.ndarray, pd.Series)):\n",
    "        try:\n",
    "            periodic_timestamps = list(periodic_timestamps)\n",
    "        except Exception:\n",
    "            # fallback\n",
    "            periodic_timestamps = None\n",
    "\n",
    "    # If after conversion it's falsy or not iterable, return None\n",
    "    try:\n",
    "        if not periodic_timestamps or len(periodic_timestamps) == 0:\n",
    "            return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    # Get the last element\n",
    "    last = periodic_timestamps[-1]\n",
    "\n",
    "    # If last is a dict, try to find a timestamp-like key\n",
    "    if isinstance(last, dict):\n",
    "        for key in (\"timestamp\", \"time\", \"ts\", \"start_time\"):\n",
    "            val = last.get(key)\n",
    "            if isinstance(val, str) and re.match(r\"^\\d{1,2}:\\d{2}:\\d{2}$\", val.strip()):\n",
    "                return _timecode_to_seconds(val.strip())\n",
    "        # fallback: scan values for timecode-looking strings\n",
    "        for v in last.values():\n",
    "            if isinstance(v, str) and re.match(r\"^\\d{1,2}:\\d{2}:\\d{2}$\", v.strip()):\n",
    "                return _timecode_to_seconds(v.strip())\n",
    "        # if dict has numeric 'start_index' and there is 'timestamp' in another element earlier, you might need custom logic\n",
    "        return None\n",
    "\n",
    "    # If last is a string, try to parse it. It may contain a comma-separated list; take the last token.\n",
    "    if isinstance(last, str):\n",
    "        # sometimes it's \"00:00:00, 00:01:00, 00:02:00\" - split on comma and strip\n",
    "        parts = [p.strip() for p in re.split(r\"[,\\|;]\", last) if p.strip()]\n",
    "        candidate = parts[-1] if parts else last.strip()\n",
    "        if re.match(r\"^\\d{1,2}:\\d{2}:\\d{2}$\", candidate):\n",
    "            return _timecode_to_seconds(candidate)\n",
    "        # maybe it's not standard format\n",
    "        return None\n",
    "\n",
    "    # If it's numeric or other unhandled type, return None\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4fe9f7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function New find_top_match_and_timestamps with code to gather addiitonal text when gathering mention text\n",
    "\n",
    "def find_top_match_and_timestamps_v3(\n",
    "    combined_df: pd.DataFrame, min_match_score: int = 90\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Finds fuzzy matches for restaurant mentions in episode transcripts and associates them with timestamps.\n",
    "\n",
    "    This function iterates through each episode's metadata and transcript data. For each mentioned\n",
    "    restaurant, it performs a fuzzy search within the transcript. It then returns a DataFrame\n",
    "    of the top matches and their corresponding timestamps, or notes if no match was found.\n",
    "\n",
    "    Args:\n",
    "        combined_df (pd.DataFrame): A DataFrame containing episode metadata, cleaned transcripts,\n",
    "                                    and periodic timestamps.\n",
    "        min_match_score (int): The minimum fuzzy match score (0-100) required to consider\n",
    "                               a match valid.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame where each row represents a restaurant mention. It contains\n",
    "                      the following columns:\n",
    "                          - 'slug': The episode slug e.g. ep-217-ross-noble or elle-fanning\n",
    "                          - 'Restaurant': The name of the restaurant mentioned.\n",
    "                          - 'Mention text': The original sentence where the mention was found.\n",
    "                          - 'Match Score': The fuzzy match score.\n",
    "                          - 'Match Type': The type of match (e.g., 'full, over 90' or 'No match found').\n",
    "                          - 'Timestamp': The nearest preceding timestamp for the mention.\n",
    "                          - 'Transcript sample': A short sample of the transcript text.\n",
    "    \"\"\"\n",
    "    all_mentions_collected = []\n",
    "\n",
    "    for index, combined_row in combined_df.iterrows():\n",
    "        slug = combined_row.get(\"slug\")\n",
    "        guest_name = combined_row.get(\"guest_name\")\n",
    "        clean_transcript_text = combined_row.get(\"clean_transcript_text\")\n",
    "        periodic_timestamps = combined_row.get(\"periodic_timestamps\")\n",
    "\n",
    "        restaurants_data = combined_row.get(\"restaurants_mentioned\", [])\n",
    "        transcript_sample = (\n",
    "            clean_transcript_text[:200]\n",
    "            if isinstance(clean_transcript_text, str)\n",
    "            else \"No Transcript Found\"\n",
    "        )\n",
    "\n",
    "        # Unsure what data type the res mentions are, hence need for this\n",
    "        restaurants_list = []\n",
    "        # print(f\"The data type of the restaurant matches is{type(restaurants_data)}\")\n",
    "        # print(f\"restaurants data (raw) : {restaurants_data}\")\n",
    "        if isinstance(restaurants_data, list):\n",
    "            restaurants_list = restaurants_data\n",
    "        elif isinstance(restaurants_data, np.ndarray) and restaurants_data.size > 0:\n",
    "            # Flatten the array and convert it to a standard Python list of strings\n",
    "            restaurants_raw_list = restaurants_data.flatten().tolist()\n",
    "            restaurants_list = [\n",
    "                name.strip().lower() for name in restaurants_raw_list if name.strip()\n",
    "            ]\n",
    "            # print(f\"\\n restauratns list (processed): {restaurants_list}\")\n",
    "        elif isinstance(restaurants_data, str):\n",
    "            restaurants_list = [\n",
    "                name.strip() for name in restaurants_data.split(\",\") if name.strip()\n",
    "            ]\n",
    "        # Searching starts here\n",
    "        if restaurants_list:\n",
    "            episode_sentences_data = _create_list_tuple_clean_sen_og_sen_og_index(\n",
    "                clean_transcript_text\n",
    "            )\n",
    "            searchable_sentences = [\n",
    "                item[0] for item in episode_sentences_data\n",
    "            ]  # This is to select the cleaned sentence from the list of tuple\n",
    "            # of cleaned sentence, original, and true start index that create_sentence_list creates\n",
    "\n",
    "            all_matches_for_episode = _matches_by_res_name_from_list_of_res_names(\n",
    "                restaurants_list, searchable_sentences, 90\n",
    "            )\n",
    "            # --- all_matches_for_episode is a dict with key res_name and value lists of matches (matches r tuples of quote, score)\n",
    "            for (\n",
    "                restaurant_name_query,\n",
    "                match_list_for_query,\n",
    "            ) in all_matches_for_episode.items():\n",
    "                if match_list_for_query:\n",
    "                    top_match = match_list_for_query[0]\n",
    "                    # Unpack the top match's data\n",
    "                    matched_cleaned_text, score, matched_sentence_index = top_match\n",
    "                    original_sentence_data = episode_sentences_data[\n",
    "                        matched_sentence_index\n",
    "                    ]  # This takes you back to episode sentences data for the sentence index\n",
    "                    # Which is a tuple of clean sentence, original, and index of sentence within sen list\n",
    "                    original_sentence_text = original_sentence_data[\n",
    "                        1\n",
    "                    ]  # The og sentence is at index 1 in this tuple\n",
    "                    original_start_index = original_sentence_data[\n",
    "                        2\n",
    "                    ]  # The og start index is at index 2 in this tuple\n",
    "\n",
    "                    # --- Inside find_top_match_and_timestamps, after computing:\n",
    "                    # original_sentence_text and original_start_index (and original_end) ---\n",
    "\n",
    "                    original_end_index = original_start_index + len(original_sentence_text)\n",
    "\n",
    "                    # Use the raw transcript string that was split to make episode_sentences_data.\n",
    "                    # In your code, you used `clean_transcript_text` to create episode_sentences_data, so reuse it here:\n",
    "                    transcript_full = clean_transcript_text if isinstance(clean_transcript_text, str) else \"\"\n",
    "\n",
    "                    # Expand to ~500 chars (or desired length)\n",
    "                    ctx_start, ctx_end, mention_text_full = _expand_context_around_match(\n",
    "                        transcript_full, original_start_index, original_end_index, target_chars=500\n",
    "                    )\n",
    "\n",
    "                    # Compute highlight indices relative to mention_text_full\n",
    "                    highlight_rel_start = original_start_index - ctx_start\n",
    "                    highlight_rel_end = highlight_rel_start + len(original_sentence_text)\n",
    "\n",
    "                    # Create HTML highlighted version (safe)\n",
    "                    mention_text_highlighted_html = _create_highlighted_html(mention_text_full, highlight_rel_start, highlight_rel_end)\n",
    "\n",
    "                    timestamp = _find_timestamp(original_start_index, periodic_timestamps)\n",
    "\n",
    "                    mention = {\n",
    "                        \"Episode ID\": slug,\n",
    "                        \"Restaurant\": restaurant_name_query,\n",
    "                        \"Mention text\": original_sentence_text,        # keep the short sentence too\n",
    "                        \"Mention text full\": mention_text_full,        # new full context\n",
    "                        \"Mention text highlighted\": mention_text_highlighted_html,  # new: safe HTML with <mark>\n",
    "                        \"Match Score\": score,\n",
    "                        \"Match Type\": f\"full, over {min_match_score}\",\n",
    "                        \"Timestamp\": timestamp,\n",
    "                        \"transcript_sample\": transcript_sample,\n",
    "                    }\n",
    "                    all_mentions_collected.append(mention)\n",
    "        else:\n",
    "            print(\n",
    "                f\"  No raw mentions found in 'restaurants_mentioned' list for Episode {slug}. Skipping\"\n",
    "            )\n",
    "    combined_df = pd.DataFrame(all_mentions_collected)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8abe643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function New find_top_match_and_timestamps to remove starting point is 00:00...\n",
    "\n",
    "def find_top_match_and_timestamps_v4(\n",
    "    combined_df: pd.DataFrame, min_match_score: int = 90\n",
    ") -> pd.DataFrame:\n",
    "    all_mentions_collected = []\n",
    "\n",
    "    for index, combined_row in combined_df.iterrows():\n",
    "        slug = combined_row.get(\"slug\")\n",
    "        guest_name = combined_row.get(\"guest_name\")\n",
    "        clean_transcript_text = combined_row.get(\"clean_transcript_text\")\n",
    "        periodic_timestamps = combined_row.get(\"periodic_timestamps\")\n",
    "\n",
    "        # *** CHANGED: strip starting-point markers BEFORE we split into sentences\n",
    "        # This ensures the sentence start indices align with the transcript we will slice later.\n",
    "        if isinstance(clean_transcript_text, str):\n",
    "            clean_transcript_text = _strip_starting_point(clean_transcript_text)  # *** CHANGED\n",
    "\n",
    "        transcript_sample = (\n",
    "            clean_transcript_text[:200]\n",
    "            if isinstance(clean_transcript_text, str)\n",
    "            else \"No Transcript Found\"\n",
    "        )\n",
    "\n",
    "        # Unsure what data type the res mentions are, hence need for this\n",
    "        restaurants_list = []\n",
    "        if isinstance(restaurants_data := combined_row.get(\"restaurants_mentioned\", []), list):\n",
    "            restaurants_list = restaurants_data\n",
    "        elif isinstance(restaurants_data, np.ndarray) and restaurants_data.size > 0:\n",
    "            restaurants_raw_list = restaurants_data.flatten().tolist()\n",
    "            restaurants_list = [\n",
    "                name.strip().lower() for name in restaurants_raw_list if name.strip()\n",
    "            ]\n",
    "        elif isinstance(restaurants_data, str):\n",
    "            restaurants_list = [\n",
    "                name.strip() for name in restaurants_data.split(\",\") if name.strip()\n",
    "            ]\n",
    "\n",
    "        # Searching starts here\n",
    "        if restaurants_list:\n",
    "            # IMPORTANT: use the cleaned transcript_text for sentence splitting (we already stripped markers).\n",
    "            episode_sentences_data = _create_list_tuple_clean_sen_og_sen_og_index(\n",
    "                clean_transcript_text\n",
    "            )\n",
    "            searchable_sentences = [item[0] for item in episode_sentences_data]\n",
    "\n",
    "            all_matches_for_episode = _matches_by_res_name_from_list_of_res_names(\n",
    "                restaurants_list, searchable_sentences, min_match_score\n",
    "            )\n",
    "\n",
    "            for (restaurant_name_query, match_list_for_query) in all_matches_for_episode.items():\n",
    "                if match_list_for_query:\n",
    "                    top_match = match_list_for_query[0]\n",
    "                    matched_cleaned_text, score, matched_sentence_index = top_match\n",
    "\n",
    "                    original_sentence_data = episode_sentences_data[matched_sentence_index]\n",
    "                    original_sentence_text = original_sentence_data[1]\n",
    "                    original_start_index = original_sentence_data[2]\n",
    "\n",
    "                    original_end_index = original_start_index + len(original_sentence_text)\n",
    "\n",
    "                    # Use the cleaned transcript (already had markers removed) for expansion too.\n",
    "                    transcript_full = clean_transcript_text if isinstance(clean_transcript_text, str) else \"\"\n",
    "                    # *** CHANGED: ensure transcript_full also has markers removed (idempotent)\n",
    "                    if transcript_full:\n",
    "                        transcript_full = _strip_starting_point(transcript_full)  # *** CHANGED (safe/idempotent)\n",
    "\n",
    "                    # Expand to ~500 chars (or desired length)\n",
    "                    ctx_start, ctx_end, mention_text_full = _expand_context_around_match(\n",
    "                        transcript_full, original_start_index, original_end_index, target_chars=500\n",
    "                    )\n",
    "\n",
    "                    # Compute highlight indices relative to mention_text_full\n",
    "                    highlight_rel_start = original_start_index - ctx_start\n",
    "                    highlight_rel_end = highlight_rel_start + len(original_sentence_text)\n",
    "\n",
    "                    # Create HTML highlighted version (safe)\n",
    "                    mention_text_highlighted_html = _create_highlighted_html(\n",
    "                        mention_text_full, highlight_rel_start, highlight_rel_end\n",
    "                    )\n",
    "\n",
    "                    timestamp = _find_timestamp(original_start_index, periodic_timestamps)\n",
    "\n",
    "                    mention = {\n",
    "                        \"Episode ID\": slug,\n",
    "                        \"Restaurant\": restaurant_name_query,\n",
    "                        \"Mention text\": original_sentence_text,\n",
    "                        \"Mention text full\": mention_text_full,\n",
    "                        \"Mention text highlighted\": mention_text_highlighted_html,\n",
    "                        \"Match Score\": score,\n",
    "                        \"Match Type\": f\"full, over {min_match_score}\",\n",
    "                        \"Timestamp\": timestamp,\n",
    "                        \"transcript_sample\": transcript_sample,\n",
    "                    }\n",
    "                    all_mentions_collected.append(mention)\n",
    "        else:\n",
    "            print(\n",
    "                f\"  No raw mentions found in 'restaurants_mentioned' list for Episode {slug}. Skipping\"\n",
    "            )\n",
    "    combined_df = pd.DataFrame(all_mentions_collected)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7d220d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function New find_top_match inc. flagging final 10% of ep matches for filtering (likely bad matches, in final roundup rather than first mention)\n",
    "\n",
    "def find_top_match_and_timestamps_v5(\n",
    "    combined_df: pd.DataFrame, min_match_score: int = 90\n",
    ") -> pd.DataFrame:\n",
    "    all_mentions_collected = []\n",
    "\n",
    "    for index, combined_row in combined_df.iterrows():\n",
    "        slug = combined_row.get(\"slug\")\n",
    "        guest_name = combined_row.get(\"guest_name\")\n",
    "        clean_transcript_text = combined_row.get(\"clean_transcript_text\")\n",
    "        periodic_timestamps = combined_row.get(\"periodic_timestamps\")\n",
    "\n",
    "        # *** CHANGED: ensure transcripts have starting point markers stripped earlier (if you added that)\n",
    "        if isinstance(clean_transcript_text, str):\n",
    "            clean_transcript_text = _strip_starting_point(clean_transcript_text)\n",
    "\n",
    "        restaurants_data = combined_row.get(\"restaurants_mentioned\", [])\n",
    "        transcript_sample = (\n",
    "            clean_transcript_text[:200]\n",
    "            if isinstance(clean_transcript_text, str)\n",
    "            else \"No Transcript Found\"\n",
    "        )\n",
    "\n",
    "        restaurants_list = []\n",
    "        if isinstance(restaurants_data, list):\n",
    "            restaurants_list = restaurants_data\n",
    "        elif isinstance(restaurants_data, np.ndarray) and restaurants_data.size > 0:\n",
    "            restaurants_raw_list = restaurants_data.flatten().tolist()\n",
    "            restaurants_list = [\n",
    "                name.strip().lower() for name in restaurants_raw_list if name.strip()\n",
    "            ]\n",
    "        elif isinstance(restaurants_data, str):\n",
    "            restaurants_list = [\n",
    "                name.strip() for name in restaurants_data.split(\",\") if name.strip()\n",
    "            ]\n",
    "\n",
    "        if restaurants_list:\n",
    "            episode_sentences_data = _create_list_tuple_clean_sen_og_sen_og_index(\n",
    "                clean_transcript_text\n",
    "            )\n",
    "            searchable_sentences = [item[0] for item in episode_sentences_data]\n",
    "\n",
    "            all_matches_for_episode = _matches_by_res_name_from_list_of_res_names(\n",
    "                restaurants_list, searchable_sentences, min_match_score\n",
    "            )\n",
    "\n",
    "            # *** ADDED: compute episode duration seconds once per episode (useful for final-10% flag)\n",
    "            ep_seconds = _episode_end_seconds(periodic_timestamps)  # *** ADDED\n",
    "\n",
    "            for (restaurant_name_query, match_list_for_query) in all_matches_for_episode.items():\n",
    "                if match_list_for_query:\n",
    "                    top_match = match_list_for_query[0]\n",
    "                    matched_cleaned_text, score, matched_sentence_index = top_match\n",
    "\n",
    "                    original_sentence_data = episode_sentences_data[matched_sentence_index]\n",
    "                    original_sentence_text = original_sentence_data[1]\n",
    "                    original_start_index = original_sentence_data[2]\n",
    "\n",
    "                    original_end_index = original_start_index + len(original_sentence_text)\n",
    "\n",
    "                    transcript_full = clean_transcript_text if isinstance(clean_transcript_text, str) else \"\"\n",
    "                    if transcript_full:\n",
    "                        transcript_full = _strip_starting_point(transcript_full)\n",
    "\n",
    "                    ctx_start, ctx_end, mention_text_full = _expand_context_around_match(\n",
    "                        transcript_full, original_start_index, original_end_index, target_chars=500\n",
    "                    )\n",
    "\n",
    "                    highlight_rel_start = original_start_index - ctx_start\n",
    "                    highlight_rel_end = highlight_rel_start + len(original_sentence_text)\n",
    "\n",
    "                    mention_text_highlighted_html = _create_highlighted_html(\n",
    "                        mention_text_full, highlight_rel_start, highlight_rel_end\n",
    "                    )\n",
    "\n",
    "                    timestamp = _find_timestamp(original_start_index, periodic_timestamps)\n",
    "\n",
    "                    # --- ADDED: compute whether match is in final 10% of episode ---\n",
    "                    match_in_final_10pct = False  # default\n",
    "                    if timestamp and ep_seconds:\n",
    "                        mention_seconds = _timecode_to_seconds(timestamp)\n",
    "                        if mention_seconds is not None and ep_seconds > 0:\n",
    "                            try:\n",
    "                                fraction = float(mention_seconds) / float(ep_seconds)\n",
    "                                if fraction >= 0.8:\n",
    "                                    match_in_final_10pct = True\n",
    "                            except Exception:\n",
    "                                match_in_final_10pct = False\n",
    "                    # --- ADDED end ---\n",
    "\n",
    "                    mention = {\n",
    "                        \"Episode ID\": slug,\n",
    "                        \"Restaurant\": restaurant_name_query,\n",
    "                        \"Mention text\": original_sentence_text,\n",
    "                        \"Mention text full\": mention_text_full,\n",
    "                        \"Mention text highlighted\": mention_text_highlighted_html,\n",
    "                        \"Match Score\": score,\n",
    "                        \"Match Type\": f\"full, over {min_match_score}\",\n",
    "                        \"Timestamp\": timestamp,\n",
    "                        \"transcript_sample\": transcript_sample,\n",
    "                        \"match_in_final_10pct\": match_in_final_10pct,  # *** ADDED\n",
    "                    }\n",
    "                    all_mentions_collected.append(mention)\n",
    "        else:\n",
    "            print(\n",
    "                f\"  No raw mentions found in 'restaurants_mentioned' list for Episode {slug}. Skipping\"\n",
    "            )\n",
    "    combined_df = pd.DataFrame(all_mentions_collected)\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ac366c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function New find_top_match inc. including restaurants without matches\n",
    "def find_top_match_and_timestamps_v6(\n",
    "    combined_df: pd.DataFrame, min_match_score: int = 90\n",
    ") -> pd.DataFrame:\n",
    "    all_mentions_collected = []\n",
    "\n",
    "    for index, combined_row in combined_df.iterrows():\n",
    "        slug = combined_row.get(\"slug\")\n",
    "        guest_name = combined_row.get(\"guest_name\")\n",
    "        clean_transcript_text = combined_row.get(\"clean_transcript_text\")\n",
    "        periodic_timestamps = combined_row.get(\"periodic_timestamps\")\n",
    "\n",
    "        # *** CHANGED: ensure transcripts have starting point markers stripped earlier (if you added that)\n",
    "        if isinstance(clean_transcript_text, str):\n",
    "            clean_transcript_text = _strip_starting_point(clean_transcript_text)\n",
    "\n",
    "        restaurants_data = combined_row.get(\"restaurants_mentioned\", [])\n",
    "        transcript_sample = (\n",
    "            clean_transcript_text[:200]\n",
    "            if isinstance(clean_transcript_text, str)\n",
    "            else \"No Transcript Found\"\n",
    "        )\n",
    "\n",
    "        # --- Standardize restaurants_list creation (UNCHANGED) ---\n",
    "        restaurants_list = []\n",
    "        if isinstance(restaurants_data, list):\n",
    "            restaurants_list = restaurants_data\n",
    "        elif isinstance(restaurants_data, np.ndarray) and restaurants_data.size > 0:\n",
    "            restaurants_raw_list = restaurants_data.flatten().tolist()\n",
    "            restaurants_list = [\n",
    "                name.strip().lower() for name in restaurants_raw_list if name.strip()\n",
    "            ]\n",
    "        elif isinstance(restaurants_data, str):\n",
    "            restaurants_list = [\n",
    "                name.strip() for name in restaurants_data.split(\",\") if name.strip()\n",
    "            ]\n",
    "        # --- End Standardization ---\n",
    "\n",
    "        if restaurants_list:\n",
    "            episode_sentences_data = _create_list_tuple_clean_sen_og_sen_og_index(\n",
    "                clean_transcript_text\n",
    "            )\n",
    "            searchable_sentences = [item[0] for item in episode_sentences_data]\n",
    "\n",
    "            all_matches_for_episode = _matches_by_res_name_from_list_of_res_names(\n",
    "                restaurants_list, searchable_sentences, min_match_score\n",
    "            )\n",
    "\n",
    "            # *** ADDED: compute episode duration seconds once per episode\n",
    "            ep_seconds = _episode_end_seconds(periodic_timestamps) \n",
    "\n",
    "            for (restaurant_name_query, match_list_for_query) in all_matches_for_episode.items():\n",
    "                \n",
    "                # Default mention structure for both SUCCESS and FAILURE\n",
    "                base_mention = {\n",
    "                    \"Episode ID\": slug,\n",
    "                    \"Restaurant\": restaurant_name_query,\n",
    "                    \"Mention text\": \"\",\n",
    "                    \"Mention text full\": \"\",\n",
    "                    \"Mention text highlighted\": \"\",\n",
    "                    \"Match Score\": 0,\n",
    "                    \"Timestamp\": \"\",\n",
    "                    \"transcript_sample\": transcript_sample,\n",
    "                    \"match_in_final_10pct\": False,\n",
    "                }\n",
    "                \n",
    "                # --- NEW LOGIC: Check for matches ---\n",
    "                if match_list_for_query:\n",
    "                    # Success: Match Found\n",
    "                    top_match = match_list_for_query[0]\n",
    "                    matched_cleaned_text, score, matched_sentence_index = top_match\n",
    "\n",
    "                    original_sentence_data = episode_sentences_data[matched_sentence_index]\n",
    "                    original_sentence_text = original_sentence_data[1]\n",
    "                    original_start_index = original_sentence_data[2]\n",
    "                    original_end_index = original_start_index + len(original_sentence_text)\n",
    "\n",
    "                    transcript_full = clean_transcript_text if isinstance(clean_transcript_text, str) else \"\"\n",
    "                    if transcript_full:\n",
    "                        transcript_full = _strip_starting_point(transcript_full)\n",
    "\n",
    "                    ctx_start, ctx_end, mention_text_full = _expand_context_around_match(\n",
    "                        transcript_full, original_start_index, original_end_index, target_chars=500\n",
    "                    )\n",
    "\n",
    "                    highlight_rel_start = original_start_index - ctx_start\n",
    "                    highlight_rel_end = highlight_rel_start + len(original_sentence_text)\n",
    "\n",
    "                    mention_text_highlighted_html = _create_highlighted_html(\n",
    "                        mention_text_full, highlight_rel_start, highlight_rel_end\n",
    "                    )\n",
    "\n",
    "                    timestamp = _find_timestamp(original_start_index, periodic_timestamps)\n",
    "\n",
    "                    # Compute whether match is in final 10% of episode\n",
    "                    match_in_final_10pct = False\n",
    "                    if timestamp and ep_seconds:\n",
    "                        mention_seconds = _timecode_to_seconds(timestamp)\n",
    "                        if mention_seconds is not None and ep_seconds > 0:\n",
    "                            try:\n",
    "                                fraction = float(mention_seconds) / float(ep_seconds)\n",
    "                                # NOTE: Fraction >= 0.8 means final 20%, but based on the column name \n",
    "                                # 'match_in_final_10pct' you might want to adjust this to 0.9. \n",
    "                                # Keeping at 0.8 as in your original code.\n",
    "                                if fraction >= 0.8: \n",
    "                                    match_in_final_10pct = True\n",
    "                            except Exception:\n",
    "                                match_in_final_10pct = False\n",
    "                    \n",
    "                    # Update base_mention with successful match data\n",
    "                    mention = base_mention.copy()\n",
    "                    mention.update({\n",
    "                        \"Mention text\": original_sentence_text,\n",
    "                        \"Mention text full\": mention_text_full,\n",
    "                        \"Mention text highlighted\": mention_text_highlighted_html,\n",
    "                        \"Match Score\": score,\n",
    "                        \"Match Type\": f\"full, over {min_match_score}\",\n",
    "                        \"Timestamp\": timestamp,\n",
    "                        \"match_in_final_10pct\": match_in_final_10pct,\n",
    "                    })\n",
    "\n",
    "                else:\n",
    "                    # Failure: No Match Found (The essential fix)\n",
    "                    mention = base_mention.copy()\n",
    "                    mention[\"Match Type\"] = \"No match found\" # <--- The key difference\n",
    "                \n",
    "                all_mentions_collected.append(mention)\n",
    "        \n",
    "        else:\n",
    "            print(\n",
    "                f\"  No raw mentions found in 'restaurants_mentioned' list for Episode {slug}. Skipping\"\n",
    "            )\n",
    "            \n",
    "    combined_df = pd.DataFrame(all_mentions_collected)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "adc4b3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-299-katherine-parkinson-live-in-london. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-278-john-kearns-tasting-menu. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-258-phil-dunster. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-248-huge-davies. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-238-katy-wix. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-218-jada-pinkett-smith. Skipping\n",
      "\n",
      "--- TOP COLLECTED ---\n",
      "Top Mentions DataFrame created with 8 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Episode ID</th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Mention text</th>\n",
       "      <th>Mention text full</th>\n",
       "      <th>Mention text highlighted</th>\n",
       "      <th>Match Score</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>transcript_sample</th>\n",
       "      <th>match_in_final_10pct</th>\n",
       "      <th>Match Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>john-early</td>\n",
       "      <td>princes hot chicken</td>\n",
       "      <td>you can go to prince's hot chicken? it's not n...</td>\n",
       "      <td>it's not the same. it's our fault for not try...</td>\n",
       "      <td>it&amp;#x27;s not the same. it&amp;#x27;s our fault f...</td>\n",
       "      <td>100</td>\n",
       "      <td>00:52:59</td>\n",
       "      <td>oh no, it's james acaster from the off-menu po...</td>\n",
       "      <td>False</td>\n",
       "      <td>full, over 90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>john-early</td>\n",
       "      <td>hattie bs</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>oh no, it's james acaster from the off-menu po...</td>\n",
       "      <td>False</td>\n",
       "      <td>No match found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nina-conti</td>\n",
       "      <td>di palos</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>hello, it's ed gamble here from the off menu p...</td>\n",
       "      <td>False</td>\n",
       "      <td>No match found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ep-288-bridget-christie-tasting-menu</td>\n",
       "      <td>soho hotel refuel bar</td>\n",
       "      <td>oh</td>\n",
       "      <td>of this menu. did you? yeah, and i was lookin...</td>\n",
       "      <td>of this menu. did you? yeah, and i was lookin...</td>\n",
       "      <td>100</td>\n",
       "      <td>00:52:16</td>\n",
       "      <td>huge news from off-menu towers, james. big ann...</td>\n",
       "      <td>True</td>\n",
       "      <td>full, over 90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ep-268-jessica-hynes</td>\n",
       "      <td>everest cash and carry</td>\n",
       "      <td>there's a great, there's a great shop near, wh...</td>\n",
       "      <td>bit for breakfast. that's almost granola. it'...</td>\n",
       "      <td>bit for breakfast. that&amp;#x27;s almost granola...</td>\n",
       "      <td>100</td>\n",
       "      <td>00:18:26</td>\n",
       "      <td>we get it. life gets busy. luckily with palato...</td>\n",
       "      <td>False</td>\n",
       "      <td>full, over 90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ep-228-ray-winstone</td>\n",
       "      <td>clock tower cafe</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>hello, it's ed gamble here from the off menu p...</td>\n",
       "      <td>False</td>\n",
       "      <td>No match found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ep-228-ray-winstone</td>\n",
       "      <td>scotts</td>\n",
       "      <td>but my favourite, favourite fish restaurant th...</td>\n",
       "      <td>you like that? would you like that? 100%. it'...</td>\n",
       "      <td>you like that? would you like that? 100%. it&amp;...</td>\n",
       "      <td>100</td>\n",
       "      <td>00:53:16</td>\n",
       "      <td>hello, it's ed gamble here from the off menu p...</td>\n",
       "      <td>True</td>\n",
       "      <td>full, over 90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ep-228-ray-winstone</td>\n",
       "      <td>smiths</td>\n",
       "      <td>in fact, once going back about ten years ago i...</td>\n",
       "      <td>into your dream menu proper, your dream start...</td>\n",
       "      <td>into your dream menu proper, your dream start...</td>\n",
       "      <td>100</td>\n",
       "      <td>00:24:44</td>\n",
       "      <td>hello, it's ed gamble here from the off menu p...</td>\n",
       "      <td>False</td>\n",
       "      <td>full, over 90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Episode ID              Restaurant  \\\n",
       "0                            john-early     princes hot chicken   \n",
       "1                            john-early               hattie bs   \n",
       "2                            nina-conti                di palos   \n",
       "3  ep-288-bridget-christie-tasting-menu   soho hotel refuel bar   \n",
       "4                  ep-268-jessica-hynes  everest cash and carry   \n",
       "5                   ep-228-ray-winstone        clock tower cafe   \n",
       "6                   ep-228-ray-winstone                  scotts   \n",
       "7                   ep-228-ray-winstone                  smiths   \n",
       "\n",
       "                                        Mention text  \\\n",
       "0  you can go to prince's hot chicken? it's not n...   \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                                 oh   \n",
       "4  there's a great, there's a great shop near, wh...   \n",
       "5                                                      \n",
       "6  but my favourite, favourite fish restaurant th...   \n",
       "7  in fact, once going back about ten years ago i...   \n",
       "\n",
       "                                   Mention text full  \\\n",
       "0   it's not the same. it's our fault for not try...   \n",
       "1                                                      \n",
       "2                                                      \n",
       "3   of this menu. did you? yeah, and i was lookin...   \n",
       "4   bit for breakfast. that's almost granola. it'...   \n",
       "5                                                      \n",
       "6   you like that? would you like that? 100%. it'...   \n",
       "7   into your dream menu proper, your dream start...   \n",
       "\n",
       "                            Mention text highlighted  Match Score Timestamp  \\\n",
       "0   it&#x27;s not the same. it&#x27;s our fault f...          100  00:52:59   \n",
       "1                                                               0             \n",
       "2                                                               0             \n",
       "3   of this menu. did you? yeah, and i was lookin...          100  00:52:16   \n",
       "4   bit for breakfast. that&#x27;s almost granola...          100  00:18:26   \n",
       "5                                                               0             \n",
       "6   you like that? would you like that? 100%. it&...          100  00:53:16   \n",
       "7   into your dream menu proper, your dream start...          100  00:24:44   \n",
       "\n",
       "                                   transcript_sample  match_in_final_10pct  \\\n",
       "0  oh no, it's james acaster from the off-menu po...                 False   \n",
       "1  oh no, it's james acaster from the off-menu po...                 False   \n",
       "2  hello, it's ed gamble here from the off menu p...                 False   \n",
       "3  huge news from off-menu towers, james. big ann...                  True   \n",
       "4  we get it. life gets busy. luckily with palato...                 False   \n",
       "5  hello, it's ed gamble here from the off menu p...                 False   \n",
       "6  hello, it's ed gamble here from the off menu p...                  True   \n",
       "7  hello, it's ed gamble here from the off menu p...                 False   \n",
       "\n",
       "       Match Type  \n",
       "0   full, over 90  \n",
       "1  No match found  \n",
       "2  No match found  \n",
       "3   full, over 90  \n",
       "4   full, over 90  \n",
       "5  No match found  \n",
       "6   full, over 90  \n",
       "7   full, over 90  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Test: New fuzzymatching with expanded match text on 10 sample eps ---\n",
    "\n",
    "top_mentions_expanded_df = find_top_match_and_timestamps_v6(first_ten_metadata_timestamps_df , 90)\n",
    "\n",
    "# --- Convert list into dataframe, print output ---\n",
    "\n",
    "print(f\"\\n--- TOP COLLECTED ---\")\n",
    "print(f\"Top Mentions DataFrame created with {len(top_mentions_expanded_df)} rows.\")\n",
    "top_mentions_expanded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f4eacb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting expanded easy win mention search\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode joy-crookes. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode lucia-keskin. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode jen-brister-tasting-menu. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode gillian-anderson. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode neil-hannon. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode sharon-wanjohi. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-299-katherine-parkinson-live-in-london. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-297-joanne-mcnally. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-296-self-esteem-live-in-london. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-294-carey-mulligan. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-292-dermot-oleary. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-290-daisy-ridley. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-287-santiago-lastra. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-285-sally-phillips. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-281-david-tennant. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-279-stephen-graham. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-278-john-kearns-tasting-menu. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-276-emily-campbell. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-265-rick-astley. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-264-rukmini-iyer. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-263-josh-widdicombe. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-260-rachel-stevens. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-259-will-ospreay. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-258-phil-dunster. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-257-amy-annette. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-256-hammed-animashaun. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-254-michelle-de-swarte. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-250-patti-harrison. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-248-huge-davies. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-247-ardal-ohanlon-live-in-dublin. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-243-joe-wilkinson-live-in-brighton. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-241-mike-wozniak-live-in-london. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-238-katy-wix. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-235-amelia-dimoldenberg-live-in-manchester. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-233-frankie-boyle-live-in-glasgow. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-219-jamelia-live-in-birmingham. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-218-jada-pinkett-smith. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-217-ross-noble-christmas-special. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-214-steve-o. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-212-garth-marenghi. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-209-dr-maggie-aderin-pocock. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-208-izuka-hoyle. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-203-paul-foot. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-202-jimi-famurewa. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-200-ed-gamble-and-james-acaster-with-guest-genie-rylan-clark. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-197-jenny-eclair. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-195-judi-love. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-194-tim-minchin. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-190-kiell-smith-bynoe. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-189-roisin-murphy. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-188-alex-jones. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-186-john-kearns. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-185-florence-pugh. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-184-nick-mohammed. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode menus-to-buried-with-judgement-day. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-180-reece-shearsmith. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-177-kathy-burke. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode the-christmas-dinner-party-live-at-southbank-centres-royal-festival-hall. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-175-alex-horne-christmas-special. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-174-mel-giedroyc-christmas-special. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-173-chris-redd-live-from-just-for-laughs-montreal. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-172-stanley-tucci. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-169-ania-magliano. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-166-paul-chowdhry. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-162-matt-lucas. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-161-flo-joan. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-157-professor-brian-cox. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-151-rob-brydon. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-150-angela-hartnett. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-149-adam-buxton. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-144-josh-thomas. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-143-siobhan-mcsweeney. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-141-asma-khan. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-136-fatiha-el-ghorri. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-134-charlotte-church. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-133-tim-key. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-132-harry-hill-christmas-special. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-131-sarah-kendall-christmas-special. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-128-miquita-oliver. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-127-timothy-spall. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-126-jack-dee. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-122-meera-syal. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-117-ainsley-harriott. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-106-josh-gondelman. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-101-julie-adenuga. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-97-munya-chawawa. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-96-thanyia-moore. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-95-rosie-jones. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode menus-to-be-buried-with-bonus-episode. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-92-sue-perkins. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-91-joel-kim-booster. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-90-paul-scheer. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-89-anne-marie. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-88-mae-martin. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode bonus-ed-and-james-and-willie-and-joes-perfect-chocolate-christmas. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-87-sarah-millican-christmas-special. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-86-russell-howard-christmas-special. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-85-jo-brand. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-82-thomasina-miers. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-81-david-odoherty-bonus-episode. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-78-dolly-alderton. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-77-ovie-soko. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-76-claudia-winkleman. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-75-corey-taylor. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-71-amy-hoggart. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-68-gok-wan. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-66-louis-theroux. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-63-jen-brister. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-61-reggie-watts. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-59-roisin-conaty-bonus-episode. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-57-simon-rogan. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-56-jean-grae. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-55-natasha-leggero-and-moshe-kasher. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-53-richard-herring-bonus. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-51-paul-f-tompkins. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-50-joe-thomas. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-48-arabella-weir. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-46-noah-schnapp. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-44-armando-iannucci. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-33-jess-phillips-mp. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-30-cerys-matthews. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-26-kerry-godliman. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-23-dynamo. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-17-victoria-coren-mitchell. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-14-jack-mcbrayer. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-10-lou-sanders. Skipping\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-6-joel-dommett. Skipping\n",
      "saving easy win mentions to filepath c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\processed\\expanded_easy_win_mention_search_df.parquet\n"
     ]
    }
   ],
   "source": [
    "# --- Running expanded match on full data from pipeline --- \n",
    "\n",
    "cleaned_transcript_timestamps_filepath = os.path.join(\n",
    "    PROCESSED_DATA_DIR, \"cleaned_transcripts_timestamps_df.parquet\"\n",
    ")\n",
    "\n",
    "# Note: cleaned_transcrtip_timestamps_df has metadata and transcripts/timestamps\n",
    "# So, no need to combine with metadat\n",
    "# Combination function results in two restaurant_mentions columns, restaurants_mentions_x and y\n",
    "# Which then cannot be read by the matching function\n",
    "cleaned_transcript_timestamps_df = try_read_parquet(cleaned_transcript_timestamps_filepath)\n",
    "\n",
    "print(\"\\nAttempting expanded easy win mention search\")\n",
    "expanded_easy_win_mention_search_df = find_top_match_and_timestamps_v6(cleaned_transcript_timestamps_df)\n",
    "\n",
    "expanded_easy_win_mention_search_path = os.path.join(\n",
    "    PROCESSED_DATA_DIR, \"expanded_easy_win_mention_search_df.parquet\"\n",
    ")\n",
    "print(f\"saving easy win mentions to filepath {expanded_easy_win_mention_search_path}\")\n",
    "expanded_easy_win_mention_search_df.to_parquet(\n",
    "    expanded_easy_win_mention_search_path, index=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff87c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save expanded mentions dataframe to CSV\n",
    "\n",
    "expanded_easy_win_mention_search_path_csv = os.path.join(\n",
    "    PROCESSED_DATA_DIR, \"expanded_easy_win_mention_search_df.csv\"\n",
    ")\n",
    "\n",
    "expanded_easy_win_mention_search_df.to_csv(\n",
    "    expanded_easy_win_mention_search_path_csv\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2c53d216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report: {'total_mentions': 519, 'matched': 519, 'unmatched': 0}\n",
      "\n",
      " unmatched df head: Empty DataFrame\n",
      "Columns: [Episode ID, Restaurant, Mention text, Mention text full, Mention text highlighted, Match Score, Timestamp, transcript_sample, match_in_final_10pct, Match Type, region_header, subtitle, restaurant, guests, restaurant_key, raw_html_segment]\n",
      "Index: []\n",
      "\n",
      " matched df head:      Episode ID           Restaurant  \\\n",
      "0    john-early  princes hot chicken   \n",
      "1    john-early            hattie bs   \n",
      "2  kunal-nayyar           moti mahal   \n",
      "3  kunal-nayyar     the tamil prince   \n",
      "4  kunal-nayyar            the dover   \n",
      "\n",
      "                                        Mention text  \\\n",
      "0  you can go to prince's hot chicken? it's not n...   \n",
      "1                                                      \n",
      "2                                                 ah   \n",
      "3  there's a pub, an indian pub called the tamil ...   \n",
      "4                                                      \n",
      "\n",
      "                                   Mention text full  \\\n",
      "0   it's not the same. it's our fault for not try...   \n",
      "1                                                      \n",
      "2   i thought that was bad. yeah. but actually, y...   \n",
      "3   she will hear this. she listens to every epis...   \n",
      "4                                                      \n",
      "\n",
      "                            Mention text highlighted  Match Score Timestamp  \\\n",
      "0   it&#x27;s not the same. it&#x27;s our fault f...          100  00:52:59   \n",
      "1                                                               0             \n",
      "2   i thought that was bad. yeah. but actually, y...          100  00:55:12   \n",
      "3   she will hear this. she listens to every epis...          100  00:29:37   \n",
      "4                                                               0             \n",
      "\n",
      "                                   transcript_sample  match_in_final_10pct  \\\n",
      "0  oh no, it's james acaster from the off-menu po...                 False   \n",
      "1  oh no, it's james acaster from the off-menu po...                 False   \n",
      "2  oh no, it's james acaster from the off-menu po...                  True   \n",
      "3  oh no, it's james acaster from the off-menu po...                 False   \n",
      "4  oh no, it's james acaster from the off-menu po...                 False   \n",
      "\n",
      "       Match Type               region_header           subtitle  \\\n",
      "0   full, over 90                       WORLD    Nashville (USA)   \n",
      "1  No match found                       WORLD             Chains   \n",
      "2   full, over 90                       WORLD  New Delhi (India)   \n",
      "3   full, over 90  UNITED KINGDOM AND IRELAND             London   \n",
      "4  No match found  UNITED KINGDOM AND IRELAND             London   \n",
      "\n",
      "            restaurant          guests       restaurant_key  \\\n",
      "0  Princes Hot Chicken    [John Early]  princes hot chicken   \n",
      "1            Hattie Bs    [John Early]            hattie bs   \n",
      "2           Moti Mahal  [Kunal Nayyar]           moti mahal   \n",
      "3     The Tamil Prince  [Kunal Nayyar]     the tamil prince   \n",
      "4            The Dover  [Kunal Nayyar]            the dover   \n",
      "\n",
      "                                    raw_html_segment  \n",
      "0  <li><p class=\"\" style=\"white-space:pre-wrap;\">...  \n",
      "1  <li><p class=\"\" style=\"white-space:pre-wrap;\">...  \n",
      "2  <li><p class=\"\" style=\"white-space:pre-wrap;\">...  \n",
      "3  <li><p class=\"\" style=\"white-space:pre-wrap;\">...  \n",
      "4  <li><p class=\"\" style=\"white-space:pre-wrap;\">...  \n",
      "\n",
      " matched df cols: Index(['Episode ID', 'Restaurant', 'Mention text', 'Mention text full',\n",
      "       'Mention text highlighted', 'Match Score', 'Timestamp',\n",
      "       'transcript_sample', 'match_in_final_10pct', 'Match Type',\n",
      "       'region_header', 'subtitle', 'restaurant', 'guests', 'restaurant_key',\n",
      "       'raw_html_segment'],\n",
      "      dtype='object')\n",
      "Empty DataFrame\n",
      "Columns: [Restaurant, Episode ID, Mention text]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Merge expanded dataframe with regions\n",
    "\n",
    "# --- Test: Matching and merging top_matches_df\n",
    "\n",
    "\n",
    "expanded_easy_win_mention_search_path = os.path.join(\n",
    "    PROCESSED_DATA_DIR, \"expanded_easy_win_mention_search_df.parquet\")\n",
    "\n",
    "easy_wins_expanded_df_for_merge = try_read_parquet(expanded_easy_win_mention_search_path)\n",
    "\n",
    "matched, unmatched, report = exact_merge_restaurants(restaurants_and_regions_df, easy_wins_expanded_df_for_merge)\n",
    "\n",
    "print(f\"report: {report}\")\n",
    "print(f\"\\n unmatched df head: {unmatched.head()}\")\n",
    "print(f\"\\n matched df head: {matched.head()}\")\n",
    "print(f\"\\n matched df cols: {matched.columns}\")\n",
    "\n",
    "print(unmatched[['Restaurant','Episode ID','Mention text']].head(20).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "641960da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving expanded mentions to CSV\n",
    "\n",
    "top_mentions_expanded_with_regions_path = os.path.join(PROCESSED_DATA_DIR, \"top_mentions_expanded_with_regions.csv\")\n",
    "matched.to_csv(top_mentions_expanded_with_regions_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Off Menu Project Venv",
   "language": "python",
   "name": "off_menu_project_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
