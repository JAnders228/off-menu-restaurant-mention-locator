{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "460c7477",
   "metadata": {},
   "source": [
    "## Creating episodes metadata \n",
    "### Metadata consists of:\n",
    " - Formerly: numbers, names, urls\n",
    " - Presently: raw title, slug, guest name, url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bfd3f0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root Set to: c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\n",
      "V2 Test Directory Set to: c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Project Root and Imports ---\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import os.path \n",
    "from pathlib import Path\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz, process # <--- THE CRITICAL FIX\n",
    "import requests\n",
    "\n",
    "# Get the path of the directory containing this notebook (e.g., /project/notebooks)\n",
    "# os.getcwd() typically works well in notebooks for this purpose.\n",
    "notebook_dir = os.getcwd() \n",
    "\n",
    "# Go UP one directory level to find the Project Root (e.g., /project)\n",
    "# NOTE: If your notebook is deeper, you might need another '../'\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "\n",
    "# Add the Project Root to Python's search path (sys.path)\n",
    "# This allows Python to find and import modules like 'utils' and 'off_menu'\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Now, imports should work\n",
    "from off_menu.utils import try_read_html_string_from_filepath, try_read_parquet, extract_html, save_text_to_file\n",
    "from off_menu.config import episodes_list_url, transcript_base_url, restaurants_url\n",
    "from off_menu.data_extraction import extract_and_save_html\n",
    "from off_menu.data_processing import create_mentions_by_res_name_dict, create_return_exploded_res_mentions_df, _clean_transcript_str_from_html\n",
    "\n",
    "# --- 2. Define Data Paths ---\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, \"raw\")\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, \"processed\")\n",
    "ANALYTICS_DATA_DIR = os.path.join(DATA_DIR, \"analytics\")\n",
    "\n",
    "# --- 3. Define and Create Test Temp Directory (V2_tests) ---\n",
    "Test_data_dir = os.path.join(DATA_DIR, \"test_temp\")\n",
    "new_test_folder = \"V2_tests\"\n",
    "V2_tests_dir = os.path.join(Test_data_dir, new_test_folder)\n",
    "\n",
    "# Create the directory structure, avoiding errors if it already exists\n",
    "os.makedirs(V2_tests_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Project Root Set to: {PROJECT_ROOT}\")\n",
    "print(f\"V2 Test Directory Set to: {V2_tests_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ea43b26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test HTML data downloaded.\n"
     ]
    }
   ],
   "source": [
    "# --- Access and save test data (all episodes html, all restaurants html) ---\n",
    "    \n",
    "test_episodes_html_filepath = os.path.join(V2_tests_dir, \"episodes.html\")\n",
    "test_restaurants_html_filepath = os.path.join(V2_tests_dir, \"restaurants.html\")\n",
    "\n",
    "extract_and_save_html(episodes_list_url, test_episodes_html_filepath)\n",
    "extract_and_save_html(restaurants_url, test_restaurants_html_filepath)\n",
    "\n",
    "print(\"Test HTML data downloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d809b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exceptions list\n",
      "['Best of 2024: Live', 'Best of 2024: Part 2', 'Best of 2024: Part 1', 'Best of 2023: Part 2', 'Best of 2023: Part 1', 'Best of 2022: Part 2', 'Best of 2022: Part 1', 'Best of 2021: Part 2', 'Best of 2021: Part 1', 'Best of 2020', 'Best of 2019']\n",
      "Names List\n",
      "[{'raw_title': 'John Early', 'slug': 'john-early', 'guest_name': 'John Early'}, {'raw_title': 'Kunal Nayyar', 'slug': 'kunal-nayyar', 'guest_name': 'Kunal Nayyar'}, {'raw_title': 'Joy Crookes', 'slug': 'joy-crookes', 'guest_name': 'Joy Crookes'}, {'raw_title': 'Elle Fanning', 'slug': 'elle-fanning', 'guest_name': 'Elle Fanning'}, {'raw_title': 'Lucia Keskin', 'slug': 'lucia-keskin', 'guest_name': 'Lucia Keskin'}, {'raw_title': 'Ian Smith', 'slug': 'ian-smith', 'guest_name': 'Ian Smith'}, {'raw_title': 'Jen Brister (Tasting Menu)', 'slug': 'jen-brister-tasting-menu', 'guest_name': 'Jen Brister'}, {'raw_title': 'Gillian Anderson', 'slug': 'gillian-anderson', 'guest_name': 'Gillian Anderson'}, {'raw_title': 'Greg James', 'slug': 'greg-james', 'guest_name': 'Greg James'}, {'raw_title': 'Rhys James', 'slug': 'rhys-james', 'guest_name': 'Rhys James'}]\n",
      "[{'raw_title': 'Ep 218: Jada Pinkett Smith', 'slug': 'ep-218-jada-pinkett-smith', 'guest_name': 'Jada Pinkett Smith'}, {'raw_title': 'Ep 217: Ross Noble (Christmas Special)', 'slug': 'ep-217-ross-noble-christmas-special', 'guest_name': 'Ross Noble'}, {'raw_title': 'Ep 216: Dawn French (Christmas Special)', 'slug': 'ep-216-dawn-french-christmas-special', 'guest_name': 'Dawn French'}, {'raw_title': 'Ep 215: Paul Rudd', 'slug': 'ep-215-paul-rudd', 'guest_name': 'Paul Rudd'}, {'raw_title': 'Ep 214: Steve-O', 'slug': 'ep-214-steve-o', 'guest_name': 'Steve-O'}, {'raw_title': 'Ep 213: Harriet Kemsley', 'slug': 'ep-213-harriet-kemsley', 'guest_name': 'Harriet Kemsley'}, {'raw_title': 'Ep 212: Garth Marenghi', 'slug': 'ep-212-garth-marenghi', 'guest_name': 'Garth Marenghi'}, {'raw_title': 'Ep 211: Steve Coogan', 'slug': 'ep-211-steve-coogan', 'guest_name': 'Steve Coogan'}, {'raw_title': 'Ep 210: Paapa Essiedu', 'slug': 'ep-210-paapa-essiedu', 'guest_name': 'Paapa Essiedu'}, {'raw_title': 'Ep 209: Dr Maggie Aderin-Pocock', 'slug': 'ep-209-dr-maggie-aderin-pocock', 'guest_name': 'Dr Maggie Aderin-Pocock'}]\n"
     ]
    }
   ],
   "source": [
    "# --- Converting \"create_numbers_names_dict_from_html\" into => create_ep_names_slugs_list_from_html ---\n",
    "# Note: this function calls _create_epnumber_epname_dict, which will also need editing\n",
    "\n",
    "# -------------------------\n",
    "# 1 slugify helper\n",
    "# -------------------------\n",
    "def slugify(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert text to a simple dash-separated, lowercase slug.\n",
    "    Example: \"Richard Herring (Bonus Episode)\" -> \"richard-herring-bonus-episode\"\n",
    "    \"\"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", text or \"\")\n",
    "    # remove parentheses but keep their content separated by space\n",
    "    s = s.replace(\"(\", \" \").replace(\")\", \" \")\n",
    "    # remove all characters except word chars, whitespace and hyphen\n",
    "    s = re.sub(r\"[^\\w\\s-]\", \"\", s)\n",
    "    # collapse whitespace to single dash and strip leading/trailing dashes\n",
    "    s = re.sub(r\"\\s+\", \"-\", s).strip(\"-\")\n",
    "    return s.lower()\n",
    "\n",
    "# -------------------------\n",
    "# 2 extract guest name\n",
    "# -------------------------\n",
    "def extract_guest_name(raw_title: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract guest name using the simple rule:\n",
    "      - split on first colon ':'\n",
    "      - take the right hand side if a separator exists\n",
    "      - remove any trailing parenthetical content e.g. ' (Bonus Episode)'\n",
    "      - strip whitespace\n",
    "    \"\"\"\n",
    "    if not raw_title:\n",
    "        return \"\"\n",
    "\n",
    "    s = raw_title.strip()\n",
    "\n",
    "    # Split on the first recognized separator in the remaining string.\n",
    "    # We prefer colon first as your original method did; then hyphens or em-dash.\n",
    "    if \":\" in s:\n",
    "        parts = s.split(\":\", 1)\n",
    "        candidate = parts[1].strip()\n",
    "    else:\n",
    "        # no separator found: either the whole string *is* the guest (as for new episodes)\n",
    "        candidate = s\n",
    "\n",
    "    # remove any parenthetical content at end or inside e.g \"Name (Live) extra\"\n",
    "    candidate = re.sub(r\"\\(.*?\\)\", \"\", candidate).strip()\n",
    "\n",
    "    # final clean: collapse multiple spaces\n",
    "    candidate = re.sub(r\"\\s+\", \" \", candidate).strip()\n",
    "\n",
    "    return candidate\n",
    "\n",
    "\n",
    "def create_tuple_inc_ep_slugs_guests_list_from_html(html_string: str) -> Tuple[List[Dict[str, Any]], List[str]]:\n",
    "    \"\"\"\n",
    "    Parse episodes HTML and return a tuple:\n",
    "      (\n",
    "        [list of valid episode records],\n",
    "        [list of raw_titles for excluded 'Best of' episodes]\n",
    "      )\n",
    "    \"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html_string, \"html.parser\")\n",
    "    episode_divs = soup.find_all(\"div\", class_=\"image-slide-title\")\n",
    "\n",
    "    # 1. Initialize two separate lists\n",
    "    records: List[Dict[str, Any]] = []\n",
    "    exceptions: List[str] = [] \n",
    "\n",
    "    for div in episode_divs:\n",
    "        raw_title = div.get_text(separator=\" \", strip=True)\n",
    "        \n",
    "        # 2. Check the condition using the string method\n",
    "        if raw_title.startswith(\"Best of\"):\n",
    "            # 3. If it is a \"Best of\" episode, append the title to the exceptions list\n",
    "            exceptions.append(raw_title)\n",
    "            # Skip the rest of the loop for this title and move to the next 'div'\n",
    "            continue\n",
    "        # menus to be buried with exception?\n",
    "        # christmas dinner party exception?\n",
    "            \n",
    "        # If the 'if' condition was false (i.e., it's a regular episode), the code continues here:\n",
    "        \n",
    "        guest_name = extract_guest_name(raw_title)\n",
    "        slug_full = slugify(raw_title)\n",
    "\n",
    "        records.append({\n",
    "            \"raw_title\": raw_title,\n",
    "            \"slug\": slug_full,\n",
    "            \"guest_name\": guest_name\n",
    "        })\n",
    "\n",
    "    # 4. Return both lists as a tuple\n",
    "    return records, exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be0b537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test: Creating raw title, slug, guest name list of dicts using create_tuple_inc_ep_slugs_guests_list_from_html --- \n",
    "\n",
    "episodes_html_str = try_read_html_string_from_filepath(test_episodes_html_filepath)\n",
    "\n",
    "test_episodes_list = create_tuple_inc_ep_slugs_guests_list_from_html(episodes_html_str)\n",
    "\n",
    "print(\"Exceptions list\")\n",
    "print(test_episodes_list[1])\n",
    "\n",
    "print(\"Names List\")\n",
    "print(test_episodes_list[0][:10])\n",
    "print(test_episodes_list[0][100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73392a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_title</th>\n",
       "      <th>slug</th>\n",
       "      <th>guest_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Early</td>\n",
       "      <td>john-early</td>\n",
       "      <td>John Early</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kunal Nayyar</td>\n",
       "      <td>kunal-nayyar</td>\n",
       "      <td>Kunal Nayyar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Joy Crookes</td>\n",
       "      <td>joy-crookes</td>\n",
       "      <td>Joy Crookes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elle Fanning</td>\n",
       "      <td>elle-fanning</td>\n",
       "      <td>Elle Fanning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lucia Keskin</td>\n",
       "      <td>lucia-keskin</td>\n",
       "      <td>Lucia Keskin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Ep 5: Aisling Bea</td>\n",
       "      <td>ep-5-aisling-bea</td>\n",
       "      <td>Aisling Bea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Ep 4: Nish Kumar</td>\n",
       "      <td>ep-4-nish-kumar</td>\n",
       "      <td>Nish Kumar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>Ep 3: Richard Osman</td>\n",
       "      <td>ep-3-richard-osman</td>\n",
       "      <td>Richard Osman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Ep 2: Grace Dent</td>\n",
       "      <td>ep-2-grace-dent</td>\n",
       "      <td>Grace Dent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>Ep 1: Scroobius Pip</td>\n",
       "      <td>ep-1-scroobius-pip</td>\n",
       "      <td>Scroobius Pip</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>322 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               raw_title                slug     guest_name\n",
       "0             John Early          john-early     John Early\n",
       "1           Kunal Nayyar        kunal-nayyar   Kunal Nayyar\n",
       "2            Joy Crookes         joy-crookes    Joy Crookes\n",
       "3           Elle Fanning        elle-fanning   Elle Fanning\n",
       "4           Lucia Keskin        lucia-keskin   Lucia Keskin\n",
       "..                   ...                 ...            ...\n",
       "317    Ep 5: Aisling Bea    ep-5-aisling-bea    Aisling Bea\n",
       "318     Ep 4: Nish Kumar     ep-4-nish-kumar     Nish Kumar\n",
       "319  Ep 3: Richard Osman  ep-3-richard-osman  Richard Osman\n",
       "320     Ep 2: Grace Dent     ep-2-grace-dent     Grace Dent\n",
       "321  Ep 1: Scroobius Pip  ep-1-scroobius-pip  Scroobius Pip\n",
       "\n",
       "[322 rows x 3 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Function converting list of dicts (of raw title, slug, guest name) into dataframe + test\n",
    "\n",
    "def create_slugs_guests_df_from_list_of_dict(titles_list: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes the list of dicts of raw titles, slugs and guest names and returns a dataframe\n",
    "    \"\"\"\n",
    "    df_episodes_metadata = pd.DataFrame(titles_list)\n",
    "    return df_episodes_metadata\n",
    "\n",
    "# --- test generating all episodes raw title, slug, guest name dataframe\n",
    "test_eps_metadata_df = create_slugs_guests_df_from_list_of_dict(test_episodes_list[0])\n",
    "test_eps_metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195fbb7d",
   "metadata": {},
   "source": [
    "### Checking for duplicate names (e.g. Ed and James have multiple eps; 100, 200, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "156a1c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             raw_title  \\\n",
      "19   Ep 300: Ed Gamble and James Acaster (with spec...   \n",
      "220  Ep 100: Ed Gamble and James Acaster (with Gues...   \n",
      "\n",
      "                                                  slug  \\\n",
      "19   ep-300-ed-gamble-and-james-acaster-with-specia...   \n",
      "220  ep-100-ed-gamble-and-james-acaster-with-guest-...   \n",
      "\n",
      "                      guest_name  \n",
      "19   Ed Gamble and James Acaster  \n",
      "220  Ed Gamble and James Acaster  \n"
     ]
    }
   ],
   "source": [
    "# --- Test to check for duplicate Ed and James GUEST NAME, confirmed duplicate. Use slugs as unique IDs ---\n",
    "\n",
    "filter_condition = test_eps_metadata_df['guest_name'] == \"Ed Gamble and James Acaster\"\n",
    "\n",
    "# 2. Apply the Filter to the DataFrame\n",
    "# When you pass the filter_condition to the DataFrame, \n",
    "# Pandas only returns the rows where the condition is True.\n",
    "specific_guest_rows = test_eps_metadata_df[filter_condition]\n",
    "\n",
    "# 3. View the results\n",
    "print(specific_guest_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b823a184",
   "metadata": {},
   "source": [
    "## Creating and adding URLs to metadata dataframe (currently consisting of raw title, slug, guest name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6fab82ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Functions to create a URL from a given row, and to orchestrate doing this for each row ---\n",
    "\n",
    "def _create_url_from_row(row: pd.Series) -> str:\n",
    "    \"\"\"Creates a podscripts transcript URL from an episode's metadata.\"\"\"\n",
    "    slug = row[\"slug\"]\n",
    "    url = f\"{transcript_base_url}{slug}\"\n",
    "    return url\n",
    "\n",
    "def create_urls_and_save_to_slugs_guests_df(\n",
    "    input_dataframe: pd.DataFrame, output_filepath: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates transcript URLs for a DataFrame of episode metadata and saves it.\n",
    "\n",
    "    This function adds a new column 'url' to the input DataFrame by applying\n",
    "    a helper function to each row. The modified DataFrame is then saved as a\n",
    "    Parquet file to the specified path.\n",
    "\n",
    "    Args:\n",
    "        input_dataframe (pd.DataFrame): The DataFrame containing episode metadata\n",
    "                                        with 'episode_number' and 'guest_name' columns.\n",
    "        output_filepath (str): The full file path where the resulting DataFrame\n",
    "                               will be saved in Parquet format.\n",
    "\n",
    "    Returns:\n",
    "        None: The function modifies the input DataFrame and saves a file to disk,\n",
    "              but does not return a value.\n",
    "    \"\"\"\n",
    "    df = input_dataframe\n",
    "    df[\"url\"] = df.apply(_create_url_from_row, axis=1)\n",
    "    df.to_parquet(output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "50c8284d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_title</th>\n",
       "      <th>slug</th>\n",
       "      <th>guest_name</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Early</td>\n",
       "      <td>john-early</td>\n",
       "      <td>John Early</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kunal Nayyar</td>\n",
       "      <td>kunal-nayyar</td>\n",
       "      <td>Kunal Nayyar</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Joy Crookes</td>\n",
       "      <td>joy-crookes</td>\n",
       "      <td>Joy Crookes</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elle Fanning</td>\n",
       "      <td>elle-fanning</td>\n",
       "      <td>Elle Fanning</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lucia Keskin</td>\n",
       "      <td>lucia-keskin</td>\n",
       "      <td>Lucia Keskin</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Ep 5: Aisling Bea</td>\n",
       "      <td>ep-5-aisling-bea</td>\n",
       "      <td>Aisling Bea</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Ep 4: Nish Kumar</td>\n",
       "      <td>ep-4-nish-kumar</td>\n",
       "      <td>Nish Kumar</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>Ep 3: Richard Osman</td>\n",
       "      <td>ep-3-richard-osman</td>\n",
       "      <td>Richard Osman</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Ep 2: Grace Dent</td>\n",
       "      <td>ep-2-grace-dent</td>\n",
       "      <td>Grace Dent</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>Ep 1: Scroobius Pip</td>\n",
       "      <td>ep-1-scroobius-pip</td>\n",
       "      <td>Scroobius Pip</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>322 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               raw_title                slug     guest_name  \\\n",
       "0             John Early          john-early     John Early   \n",
       "1           Kunal Nayyar        kunal-nayyar   Kunal Nayyar   \n",
       "2            Joy Crookes         joy-crookes    Joy Crookes   \n",
       "3           Elle Fanning        elle-fanning   Elle Fanning   \n",
       "4           Lucia Keskin        lucia-keskin   Lucia Keskin   \n",
       "..                   ...                 ...            ...   \n",
       "317    Ep 5: Aisling Bea    ep-5-aisling-bea    Aisling Bea   \n",
       "318     Ep 4: Nish Kumar     ep-4-nish-kumar     Nish Kumar   \n",
       "319  Ep 3: Richard Osman  ep-3-richard-osman  Richard Osman   \n",
       "320     Ep 2: Grace Dent     ep-2-grace-dent     Grace Dent   \n",
       "321  Ep 1: Scroobius Pip  ep-1-scroobius-pip  Scroobius Pip   \n",
       "\n",
       "                                                   url  \n",
       "0    https://podscripts.co/podcasts/off-menu-with-e...  \n",
       "1    https://podscripts.co/podcasts/off-menu-with-e...  \n",
       "2    https://podscripts.co/podcasts/off-menu-with-e...  \n",
       "3    https://podscripts.co/podcasts/off-menu-with-e...  \n",
       "4    https://podscripts.co/podcasts/off-menu-with-e...  \n",
       "..                                                 ...  \n",
       "317  https://podscripts.co/podcasts/off-menu-with-e...  \n",
       "318  https://podscripts.co/podcasts/off-menu-with-e...  \n",
       "319  https://podscripts.co/podcasts/off-menu-with-e...  \n",
       "320  https://podscripts.co/podcasts/off-menu-with-e...  \n",
       "321  https://podscripts.co/podcasts/off-menu-with-e...  \n",
       "\n",
       "[322 rows x 4 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Test: Adding URLS to full epiosdes metadata (raw title, slug, guest name) ---\n",
    "\n",
    "# Create filepath, run function\n",
    "test_processed_metadata_filepath_for_saving = os.path.join(\n",
    "            V2_tests_dir, \"test_metadata.parquet\")\n",
    "\n",
    "create_urls_and_save_to_slugs_guests_df(test_eps_metadata_df, test_processed_metadata_filepath_for_saving)\n",
    "\n",
    "# Read dataframe, display dataframe\n",
    "test_eps_metadata_urls = try_read_parquet(test_processed_metadata_filepath_for_saving)\n",
    "test_eps_metadata_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be749730",
   "metadata": {},
   "source": [
    "## Merging restaurant mentions with episodes metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c122a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurant_name</th>\n",
       "      <th>guest_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Red Chilli</td>\n",
       "      <td>Sophie Duker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Orana</td>\n",
       "      <td>Ian Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barbacoa El Primo</td>\n",
       "      <td>Finn Wolfhard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>La Taberna Del Gourmet</td>\n",
       "      <td>Rhod Gilbert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Gastrobar</td>\n",
       "      <td>James Acaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>Estelle Manor</td>\n",
       "      <td>AJ Odudu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>Partisan</td>\n",
       "      <td>CMAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>The Black Swan</td>\n",
       "      <td>Maisie Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>The Black Swan</td>\n",
       "      <td>Ed Gamble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>Bettys Cafe Tea Rooms</td>\n",
       "      <td>Elis James</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>842 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            restaurant_name     guest_name\n",
       "0                Red Chilli   Sophie Duker\n",
       "1                     Orana      Ian Smith\n",
       "2         Barbacoa El Primo  Finn Wolfhard\n",
       "3    La Taberna Del Gourmet   Rhod Gilbert\n",
       "4             Ron Gastrobar  James Acaster\n",
       "..                      ...            ...\n",
       "741           Estelle Manor       AJ Odudu\n",
       "742                Partisan           CMAT\n",
       "743          The Black Swan    Maisie Adam\n",
       "743          The Black Swan      Ed Gamble\n",
       "744   Bettys Cafe Tea Rooms     Elis James\n",
       "\n",
       "[842 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Generate restaurant mentions dataframe ready for new merging function test --- \n",
    "\n",
    "# Create dict of mentions with res name as keys and list of guests who mention as values\n",
    "guests_who_mention_res_by_res_name_dict = create_mentions_by_res_name_dict(\n",
    "            test_restaurants_html_filepath\n",
    ")\n",
    "# Convert dict of res names and guests who mention into exploded dataframe (one line per guest who mentions the restaurant)\n",
    "exploded_res_mentions_df = create_return_exploded_res_mentions_df(\n",
    "    guests_who_mention_res_by_res_name_dict\n",
    ")\n",
    "\n",
    "exploded_res_mentions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "79d0cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Creating new merging function (merging mentions with guest name, url, slug ; slug based) ---\n",
    "\n",
    "def combine_save_mentions_and_ep_metadata_dfs(\n",
    "    exploded_restaurants_guest_df: pd.DataFrame,\n",
    "    ep_metadata_filepath: str,\n",
    "    output_df_filepath: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Takes in exploded (one line per guest/mention) mentions/guest df, and ep metadata (numbers, names, url) dataframe\n",
    "    filepath, and output filepath, and combines the dataframes. The combined dataframe is then saved as a\n",
    "    Parquet file to the specified path.\n",
    "\n",
    "    Args:\n",
    "        exploded_restaurants_guest_df (pd.DataFrame): A dataframe with 1 row for each mention of a restaurant (exploded)\n",
    "        ep_metadata_filepath (str): String filepath for the episode metadata dataframe\n",
    "        output_df_filepath (str): String filepath for where to save the combined dataframe\n",
    "\n",
    "    Returns:\n",
    "        None: The function combines the dataframes, and saves to a parquet.\n",
    "    \"\"\"\n",
    "    # Fetch metadata filepath\n",
    "    df_episodes_metadata = try_read_parquet(ep_metadata_filepath)\n",
    "    # Left merge on guest, with numbers, names, url (df_episodes_metadata)\n",
    "    merged_df = pd.merge(\n",
    "        df_episodes_metadata, exploded_restaurants_guest_df, on=\"guest_name\", how=\"left\"\n",
    "    )\n",
    "    # Aggregating rows so we have one row per episode, with a list of restaurant mentions\n",
    "    # Note groupby creates groups based on the args (three identical in this case). as_index False means also have an index col (don't use first col as index)\n",
    "    # Note .agg aggregates the data, it creates a new col called restaurants mentioned, from the col 'restaurant_name', applying the method 'dropna' to each group (restuarants that were in the restaurant_name cell), dropna gets rid of the NaN's\n",
    "    # Note NaN's are placeholders for missing data (means ilterally not a number, which is confusing as it could be text...)\n",
    "    ep_meta_and_mentions_df = (\n",
    "        merged_df.groupby([\"guest_name\", \"url\", \"slug\"], as_index=False, sort=False)\n",
    "        .agg(restaurants_mentioned=(\"restaurant_name\", lambda x: list(x.dropna())))\n",
    "        .rename(columns={\"restaurant_name\": \"restaurants_mentioned\"})\n",
    "    )\n",
    "    # Save the dataframe\n",
    "    ep_meta_and_mentions_df.to_parquet(output_df_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1684a0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guest_name</th>\n",
       "      <th>url</th>\n",
       "      <th>slug</th>\n",
       "      <th>restaurants_mentioned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Early</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>john-early</td>\n",
       "      <td>[Princes Hot Chicken, Hattie Bs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kunal Nayyar</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>kunal-nayyar</td>\n",
       "      <td>[Moti Mahal, The Tamil Prince, The Dover, Kutir]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Joy Crookes</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>joy-crookes</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elle Fanning</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>elle-fanning</td>\n",
       "      <td>[Lady M, Red Lobster, Popeyes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lucia Keskin</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>lucia-keskin</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Aisling Bea</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-5-aisling-bea</td>\n",
       "      <td>[Cafe Gratitude, Burger and Lobster]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Nish Kumar</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-4-nish-kumar</td>\n",
       "      <td>[Bademiya, The Owl and The Pussycat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>Richard Osman</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-3-richard-osman</td>\n",
       "      <td>[Five Guys, Cora Pearl, Berners Tavern]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Grace Dent</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-2-grace-dent</td>\n",
       "      <td>[Little Owl, Trullo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>Scroobius Pip</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-1-scroobius-pip</td>\n",
       "      <td>[Oli Babas Kerb Camden]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>322 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        guest_name                                                url  \\\n",
       "0       John Early  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "1     Kunal Nayyar  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "2      Joy Crookes  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "3     Elle Fanning  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "4     Lucia Keskin  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "..             ...                                                ...   \n",
       "317    Aisling Bea  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "318     Nish Kumar  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "319  Richard Osman  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "320     Grace Dent  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "321  Scroobius Pip  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "\n",
       "                   slug                             restaurants_mentioned  \n",
       "0            john-early                  [Princes Hot Chicken, Hattie Bs]  \n",
       "1          kunal-nayyar  [Moti Mahal, The Tamil Prince, The Dover, Kutir]  \n",
       "2           joy-crookes                                                []  \n",
       "3          elle-fanning                    [Lady M, Red Lobster, Popeyes]  \n",
       "4          lucia-keskin                                                []  \n",
       "..                  ...                                               ...  \n",
       "317    ep-5-aisling-bea              [Cafe Gratitude, Burger and Lobster]  \n",
       "318     ep-4-nish-kumar              [Bademiya, The Owl and The Pussycat]  \n",
       "319  ep-3-richard-osman           [Five Guys, Cora Pearl, Berners Tavern]  \n",
       "320     ep-2-grace-dent                              [Little Owl, Trullo]  \n",
       "321  ep-1-scroobius-pip                           [Oli Babas Kerb Camden]  \n",
       "\n",
       "[322 rows x 4 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- GENERATE TEST DATA: All episodes metadata combined with res mentions, later slices into test batches --- \n",
    "# --- Test: New merging function - merging full res mentions with full episodes metadata (raw title, slug, guest name, urls)\n",
    "\n",
    "test_full_episodes_metadata_path = os.path.join(V2_tests_dir, \"test_episodes_metadata_full.parquet\")\n",
    "\n",
    "combine_save_mentions_and_ep_metadata_dfs(\n",
    "        exploded_res_mentions_df,\n",
    "        test_processed_metadata_filepath_for_saving,\n",
    "        test_full_episodes_metadata_path,\n",
    "    )\n",
    "\n",
    "full_episodes_metadata_test_df = try_read_parquet(test_full_episodes_metadata_path)\n",
    "full_episodes_metadata_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec127737",
   "metadata": {},
   "source": [
    "## New web scraper\n",
    "### Needed to generate test data from the above html + to improve functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c9c29e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced DataFrame created with 11 rows and saved to: c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_eps_metadata.parquet\n",
      "Sliced DataFrame created with 11 rows and saved to: c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\second_ten_eps_metadata.parquet\n",
      "\n",
      "First ten eps metadata:\n",
      "             guest_name                                                url  \\\n",
      "0            John Early  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "10           Nina Conti  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "20  Katherine Parkinson  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "30     Bridget Christie  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "40          John Kearns  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "\n",
      "                                         slug  \\\n",
      "0                                  john-early   \n",
      "10                                 nina-conti   \n",
      "20  ep-299-katherine-parkinson-live-in-london   \n",
      "30       ep-288-bridget-christie-tasting-menu   \n",
      "40            ep-278-john-kearns-tasting-menu   \n",
      "\n",
      "               restaurants_mentioned  \n",
      "0   [Princes Hot Chicken, Hattie Bs]  \n",
      "10                        [Di Palos]  \n",
      "20                                []  \n",
      "30           [Soho Hotel Refuel Bar]  \n",
      "40                                []  \n",
      "\n",
      "Second ten eps metadata:\n",
      "             guest_name                                                url  \\\n",
      "100  Jada Pinkett Smith  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "110         Izuka Hoyle  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "120        Graham Coxon  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "130          Alex Jones  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "140    Yotam Ottolenghi  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "\n",
      "                          slug  \\\n",
      "100  ep-218-jada-pinkett-smith   \n",
      "110         ep-208-izuka-hoyle   \n",
      "120        ep-198-graham-coxon   \n",
      "130          ep-188-alex-jones   \n",
      "140    ep-179-yotam-ottolenghi   \n",
      "\n",
      "                                 restaurants_mentioned  \n",
      "100                                                 []  \n",
      "110                                                 []  \n",
      "120  [Cannons, Toffs of Muswell Hill, Sitwell Fish ...  \n",
      "130                                                 []  \n",
      "140                                [The Dusty Knuckle]  \n"
     ]
    }
   ],
   "source": [
    "# --- GENERATE TEST DATA: Full episodes metadata selected rows (inc. guest name, url, slug, MENTIONS (already combined)) ---\n",
    "# --- Batch 1: rows 0, 10, 20, 30, 40, 50...100 ---\n",
    "# --- Batch 2: \n",
    "\n",
    "# --- Batch 1: rows 0, 10, 20, 30, 40, 50...100 ---\n",
    "ten_test_episodes_metadata_output_path = os.path.join(V2_tests_dir, \"first_ten_eps_metadata.parquet\")\n",
    "\n",
    "indices_to_slice = range(0, 101, 10)\n",
    "\n",
    "# 2. Slice the DataFrame by position using .iloc\n",
    "# .iloc stands for 'integer location' and is used for positional indexing.\n",
    "first_ten_eps_metadata_df = full_episodes_metadata_test_df.iloc[indices_to_slice]\n",
    "\n",
    "# 3. Save the sliced DataFrame to a Parquet file\n",
    "# index=False ensures the default Pandas index (0, 1, 2, ...) is not saved as a column\n",
    "first_ten_eps_metadata_df.to_parquet(ten_test_episodes_metadata_output_path, index=False)\n",
    "\n",
    "print(f\"Sliced DataFrame created with {len(first_ten_eps_metadata_df)} rows and saved to: {ten_test_episodes_metadata_output_path}\")\n",
    "first_ten_eps_metadata_df\n",
    "\n",
    "# --- Batch 2: Rows 200, 210, ... 300 ---\n",
    "\n",
    "second_ten_test_episodes_metadata_output_path = os.path.join(V2_tests_dir, \"second_ten_eps_metadata.parquet\")\n",
    "\n",
    "indices_to_slice_2 = range(100, 201, 10)\n",
    "\n",
    "second_ten_eps_metadata_df = full_episodes_metadata_test_df.iloc[indices_to_slice_2]\n",
    "\n",
    "second_ten_eps_metadata_df.to_parquet(second_ten_test_episodes_metadata_output_path, index=False)\n",
    "\n",
    "print(f\"Sliced DataFrame created with {len(second_ten_eps_metadata_df)} rows and saved to: {second_ten_test_episodes_metadata_output_path}\")\n",
    "\n",
    "print(\"\\nFirst ten eps metadata:\")\n",
    "print(first_ten_eps_metadata_df.head())\n",
    "\n",
    "print(\"\\nSecond ten eps metadata:\")\n",
    "print(second_ten_eps_metadata_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ffd5940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---Original scraper with some edits (unsuccessful) ---\n",
    "\n",
    "def _save_transcripts_html(eps_dataframe, directory):\n",
    "    \"\"\"\n",
    "    Iterates through a DataFrame of episodes, downloads the HTML content from\n",
    "    the episode URL, and saves it to a specified directory.\n",
    "\n",
    "    Skips files that already exist and includes a random delay to be\n",
    "    polite to the server.\n",
    "\n",
    "    Args:\n",
    "        eps_dataframe (pd.DataFrame): DataFrame containing episode metadata\n",
    "                                      (including 'episode_number' and 'url').\n",
    "        directory (str): The directory to save the HTML files to.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    for index, row in eps_dataframe.iterrows():\n",
    "        guest_name = row[\"guest_name\"]\n",
    "        episode_url = row[\"url\"]\n",
    "        filename = f\"{guest_name}.html\"\n",
    "        filepath = os.path.join(directory, filename)\n",
    "\n",
    "        # Skip episodes that already exist\n",
    "        if os.path.exists(filepath):\n",
    "            print(\n",
    "                f\"  Skipping Episode {guest_name}, at index{index}: File already exists at {filepath}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Delay to be polite to the server and avoid 429 errors\n",
    "        sleep_time = random.uniform(1, 3)  # Sleep for 1 to 3 seconds\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "        html_content_str = extract_html(episode_url)\n",
    "\n",
    "        # Check for None before attempting to save\n",
    "        # The extract_html function returns None on failure (like a 429 error)\n",
    "        if html_content_str:\n",
    "            save_text_to_file(html_content_str, filename, directory)\n",
    "        else:\n",
    "            print(\n",
    "                f\"  Skipping save for Episode {episode_num} due to failed extraction.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa82cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GPT scraper V1 annotated --- \n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ---- Simple logger ----\n",
    "# logger = logging.getLogger(\"scraper\")\n",
    "if not logger.handlers:\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s %(message)s\"))\n",
    "    logger.addHandler(ch)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# ---- Helper: random-ish UA list (small) ----\n",
    "_SIMPLE_USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "]\n",
    "\n",
    "\n",
    "def _choose_headers():\n",
    "    return {\"User-Agent\": random.choice(_SIMPLE_USER_AGENTS)}\n",
    "\n",
    "\n",
    "# ---- Downloader with retries, backoff, persistence ----\n",
    "def download_transcripts(\n",
    "    url_map: Dict[str, str], # links the url to the guest name via a dict, because it uses name as filename (note needs to use slug as some names repeate.g. ed & james)\n",
    "    out_dir: str, # Directory to save html to\n",
    "    status_path: str, # Path to status JSON file\n",
    "    max_attempts_per_url: int = 5,\n",
    "    backoff_base: float = 1.0,\n",
    "    max_workers: int = 3, # Number of \"workers\" (threads , things that try to run concurrently in a single overarching process)\n",
    "    session: Optional[requests.Session] = None, # The session if we have one open for some reason (single session = more effieicnt)\n",
    "    timeout: float = 12.0,\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Download a set of URLs and save the HTML files locally.\n",
    "\n",
    "    Args:\n",
    "        url_map: mapping slug_or_filename -> url. Eg {\"paul-rudd\": \"https://.../ep-215-paul-rudd\"}\n",
    "                 Or you can map guest_name -> url.\n",
    "        out_dir: directory to save files (will be created).\n",
    "        status_path: path to JSON status file to persist attempts and outcomes.\n",
    "        max_attempts_per_url: maximum attempts per url before giving up.\n",
    "        backoff_base: base seconds for exponential backoff (1.0 is a reasonable default).\n",
    "        max_workers: number of concurrent download workers (1..6 recommended).\n",
    "        session: optional requests.Session() - if None a new one is created.\n",
    "        timeout: request timeout in seconds.\n",
    "\n",
    "    Returns:\n",
    "        status dict mapping key -> { \"url\", \"attempts\", \"status\", \"saved_path\", \"last_error\" }\n",
    "    \"\"\"\n",
    "\n",
    "    out_dir = Path(out_dir) \n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    status_path = Path(status_path) # Turns strings into paths for use saving/reading\n",
    "\n",
    "    # Load existing status if present (allows resume)\n",
    "    if status_path.exists():\n",
    "        try:\n",
    "            with open(status_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                status = json.load(f)\n",
    "        except Exception:\n",
    "            status = {}\n",
    "    else:\n",
    "        status = {}\n",
    "\n",
    "    # Initialize status entries for any missing keys\n",
    "    # Makes statuses for all \"keys\" (guest names/episodes)\n",
    "    for key, url in url_map.items():\n",
    "        if key not in status:\n",
    "            status[key] = {\n",
    "                \"url\": url,\n",
    "                \"attempts\": 0,\n",
    "                \"status\": \"pending\",  # pending | success | failed\n",
    "                \"saved_path\": None,\n",
    "                \"last_error\": None,\n",
    "            }\n",
    "\n",
    "    # Use a single session for pooling - more efficient that starting multiple sessions apparently\n",
    "    session = session or requests.Session()\n",
    "\n",
    "    def _attempt_download(key: str, meta: Dict) -> Dict:\n",
    "        url = meta[\"url\"]\n",
    "        attempts = meta[\"attempts\"]\n",
    "        result = dict(meta)\n",
    "        # If already succeeded, skip\n",
    "        if meta.get(\"status\") == \"success\":\n",
    "            return result\n",
    "\n",
    "        # If we've already reached max attempts, mark failed and skip\n",
    "        if attempts >= max_attempts_per_url:\n",
    "            result[\"status\"] = \"failed\"\n",
    "            result[\"last_error\"] = \"max_attempts_reached\"\n",
    "            return result\n",
    "\n",
    "        try:\n",
    "            # Build headers and request - headers are in the request and say what browser I'm using, we're faking three diff ones to rotate between\n",
    "            # To look less bot like\n",
    "            # Resp is a response object which is what comes back from the request, and contains the html text among other things\n",
    "            headers = _choose_headers()\n",
    "            resp = session.get(url, headers=headers, timeout=timeout)\n",
    "            # If success\n",
    "            if resp.status_code == 200:\n",
    "                # Save file (deterministic name using key)\n",
    "                # note needs changing to slug\n",
    "                filename = f\"{key}.html\"\n",
    "                saved_path = str(out_dir / filename)\n",
    "                with open(saved_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "                    fh.write(resp.text)\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"success\",\n",
    "                    \"saved_path\": saved_path,\n",
    "                    \"last_error\": None,\n",
    "                })\n",
    "                logger.info(\"Saved %s -> %s\", url, saved_path) # The logger lets us know whats going on, better than prints as level of detail can be\n",
    "                # changed dynamically\n",
    "                return result\n",
    "\n",
    "            # Retryable status codes (429 Too Many Requests, 5xx)\n",
    "            if resp.status_code in (429, 500, 502, 503, 504):\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"pending\",\n",
    "                    \"last_error\": f\"status_{resp.status_code}\"\n",
    "                })\n",
    "                logger.warning(\"Retryable HTTP %s for %s (attempt %s)\", resp.status_code, url, attempts + 1)\n",
    "                return result\n",
    "\n",
    "            # Non-retryable: mark failed with info\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"failed\",\n",
    "                \"last_error\": f\"status_{resp.status_code}\"\n",
    "            })\n",
    "            logger.error(\"Non-retryable HTTP %s for %s\", resp.status_code, url)\n",
    "            return result\n",
    "\n",
    "        except requests.RequestException as e: # network level errors, considered retryable\n",
    "            # Network error: retryable\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"pending\",\n",
    "                \"last_error\": repr(e)\n",
    "            })\n",
    "            logger.warning(\"RequestException for %s (attempt %s): %s\", url, attempts + 1, e)\n",
    "            return result\n",
    "\n",
    "    # Worker function wraps attempts + backoff\n",
    "    # Note meta is the status for this key, and it may be changed throuhg attemotin downloads to new meta\n",
    "    def _worker_task(key):\n",
    "        meta = status[key]\n",
    "        # If already success or permanently failed, return\n",
    "        if meta.get(\"status\") == \"success\" or meta.get(\"attempts\", 0) >= max_attempts_per_url:\n",
    "            return key, meta\n",
    "\n",
    "        # attempt download\n",
    "        new_meta = _attempt_download(key, meta)\n",
    "\n",
    "        # If still pending (retry-worthy), sleep exponential backoff before returning\n",
    "        if new_meta[\"status\"] == \"pending\":\n",
    "            # compute sleep: base * 2^(attempts-1) + jitter\n",
    "            sleep = backoff_base * (2 ** (new_meta[\"attempts\"] - 1))\n",
    "            jitter = random.uniform(0, 1.0)\n",
    "            sleep_time = min(sleep + jitter, 60)  # cap at 60s\n",
    "            logger.info(\"Backing off %0.2fs for %s (attempt %s)\", sleep_time, new_meta[\"url\"], new_meta[\"attempts\"])\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        return key, new_meta\n",
    "\n",
    "    # Main loop: do rounds where each round runs up to max_workers concurrent attempts on pending items.\n",
    "    pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "    # List comprehension selects k's to include, where k is the status and v the attempts, if status is pending and attempts below threshhld\n",
    "    round_idx = 0 # counter\n",
    "    while pending_keys:\n",
    "        round_idx += 1\n",
    "        logger.info(\"Download round %d: %d pending\", round_idx, len(pending_keys)) # How does %d work?\n",
    "\n",
    "        # Limit concurrency to not overload server\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futures = {ex.submit(_worker_task, key): key for key in pending_keys} # Futures represent the future result of the task - recall \n",
    "            # recall that threads designed to run tasks concerrently\n",
    "            # Note with... as ex is a context manager, opens/closes the thread pool\n",
    "            # submit passes a task (a certain key/guest name to try and download) to the thread pool\n",
    "            # futures = {} creates a dict where the future objects are keys and the values are the...keys, confusingly        \n",
    "            for fut in as_completed(futures): # as completed yields futuer obkects 1 by 1 as they're completed\n",
    "                key = futures[fut] # This accesses the future\n",
    "                try:\n",
    "                    k, new_meta = fut.result() # This makes k and new meta the results of the future (output of worker task, which is attempted download: a key and new meta which is the status entry for the key, inc. save path for html)\n",
    "                    status[k].update(new_meta)\n",
    "                except Exception as e:\n",
    "                    logger.exception(\"Unhandled exception for key %s: %s\", key, e)\n",
    "                    status[key][\"attempts\"] = status[key].get(\"attempts\", 0) + 1\n",
    "                    status[key][\"last_error\"] = repr(e)\n",
    "\n",
    "        # persist status to disk after every round\n",
    "        try:\n",
    "            with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(status, f, indent=2)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Failed to write status file: %s\", e)\n",
    "\n",
    "        # Prepare next round: only keys still pending and under attempts limit\n",
    "        pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "\n",
    "        # If there are pending keys, optionally small delay between rounds\n",
    "        if pending_keys:\n",
    "            logger.info(\"Sleeping 2s between rounds to be polite...\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    # final persist - saves the JSON again (unsure how dump method works)\n",
    "    with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(status, f, indent=2)\n",
    "\n",
    "    # return status mapping\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445304c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GPT scraper V2 using slugs ---\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ---- Simple logger ----\n",
    "#logger = logging.getLogger(\"scraper\")\n",
    "if not logger.handlers:\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s %(message)s\"))\n",
    "    logger.addHandler(ch)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# ---- Helper: random-ish UA list (small) ----\n",
    "_SIMPLE_USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "]\n",
    "\n",
    "\n",
    "def _choose_headers():\n",
    "    return {\"User-Agent\": random.choice(_SIMPLE_USER_AGENTS)}\n",
    "\n",
    "\n",
    "def _sanitize_key(key: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert a key (expected to be a slug) into a safe filename slug:\n",
    "      - lowercase\n",
    "      - replace any sequence of characters NOT a-z0-9 or '-' or '_' with '-'\n",
    "      - collapse multiple '-' into one\n",
    "      - strip leading/trailing '-' or '_'\n",
    "    This ensures keys like \"Paul Rudd\" become \"paul-rudd\" and already-correct slugs remain stable.\n",
    "    \"\"\"\n",
    "    if not isinstance(key, str):\n",
    "        key = str(key)\n",
    "    s = key.strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\-_]+\", \"-\", s)\n",
    "    s = re.sub(r\"-{2,}\", \"-\", s)\n",
    "    return s.strip(\"-_\")\n",
    "\n",
    "\n",
    "# ---- Downloader with retries, backoff, persistence ----\n",
    "def download_transcripts(\n",
    "    url_map: Dict[str, str],  # mapping slug_or_filename -> url (keys should be your episode slugs)\n",
    "    out_dir: str,  # Directory to save html to\n",
    "    status_path: str,  # Path to status JSON file\n",
    "    max_attempts_per_url: int = 5,\n",
    "    backoff_base: float = 1.0,\n",
    "    max_workers: int = 3,  # Number of concurrent download workers\n",
    "    session: Optional[requests.Session] = None,  # Optional shared requests.Session\n",
    "    timeout: float = 12.0,\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Download a set of URLs and save the HTML files locally.\n",
    "\n",
    "    Args:\n",
    "        url_map: mapping slug_or_filename -> url. Keys should be the episode slugs you want to use as identifiers.\n",
    "        out_dir: directory to save files (created if missing).\n",
    "        status_path: path to JSON status file to persist attempts and outcomes.\n",
    "        max_attempts_per_url: maximum attempts per url before giving up.\n",
    "        backoff_base: base seconds for exponential backoff.\n",
    "        max_workers: number of concurrent download workers.\n",
    "        session: optional requests.Session() - if None a new one is created.\n",
    "        timeout: request timeout in seconds.\n",
    "\n",
    "    Returns:\n",
    "        status dict mapping key -> { \"url\", \"attempts\", \"status\", \"saved_path\", \"last_error\" }\n",
    "    \"\"\"\n",
    "\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    status_path = Path(status_path)\n",
    "\n",
    "    # Load existing status if present (allows resume)\n",
    "    if status_path.exists():\n",
    "        try:\n",
    "            with open(status_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                status = json.load(f)\n",
    "        except Exception:\n",
    "            status = {}\n",
    "    else:\n",
    "        status = {}\n",
    "\n",
    "    # Initialize status entries for any missing keys\n",
    "    for key, url in url_map.items():\n",
    "        if key not in status:\n",
    "            status[key] = {\n",
    "                \"url\": url,\n",
    "                \"attempts\": 0,\n",
    "                \"status\": \"pending\",  # pending | success | failed\n",
    "                \"saved_path\": None,\n",
    "                \"last_error\": None,\n",
    "            }\n",
    "\n",
    "    # Use a single session for pooling\n",
    "    session = session or requests.Session()\n",
    "\n",
    "    def _attempt_download(key: str, meta: Dict) -> Dict:\n",
    "        url = meta[\"url\"]\n",
    "        attempts = meta[\"attempts\"]\n",
    "        result = dict(meta)\n",
    "        # If already succeeded, skip\n",
    "        if meta.get(\"status\") == \"success\":\n",
    "            return result\n",
    "\n",
    "        # If we've already reached max attempts, mark failed and skip\n",
    "        if attempts >= max_attempts_per_url:\n",
    "            result[\"status\"] = \"failed\"\n",
    "            result[\"last_error\"] = \"max_attempts_reached\"\n",
    "            return result\n",
    "\n",
    "        try:\n",
    "            # Build headers and request\n",
    "            headers = _choose_headers()\n",
    "            resp = session.get(url, headers=headers, timeout=timeout)\n",
    "            # If success\n",
    "            if resp.status_code == 200:\n",
    "                # Save file using sanitized key (ensure filesystem-safe slug)\n",
    "                safe_key = _sanitize_key(key)\n",
    "                filename = f\"{safe_key}.html\"\n",
    "                saved_path = str(out_dir / filename)\n",
    "                with open(saved_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "                    fh.write(resp.text)\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"success\",\n",
    "                    \"saved_path\": saved_path,\n",
    "                    \"last_error\": None,\n",
    "                })\n",
    "                logger.info(\"Saved %s -> %s\", url, saved_path)\n",
    "                return result\n",
    "\n",
    "            # Retryable status codes (429 Too Many Requests, 5xx)\n",
    "            if resp.status_code in (429, 500, 502, 503, 504):\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"pending\",\n",
    "                    \"last_error\": f\"status_{resp.status_code}\"\n",
    "                })\n",
    "                logger.warning(\"Retryable HTTP %s for %s (attempt %s)\", resp.status_code, url, attempts + 1)\n",
    "                return result\n",
    "\n",
    "            # Non-retryable: mark failed with info\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"failed\",\n",
    "                \"last_error\": f\"status_{resp.status_code}\"\n",
    "            })\n",
    "            logger.error(\"Non-retryable HTTP %s for %s\", resp.status_code, url)\n",
    "            return result\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            # Network error: retryable\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"pending\",\n",
    "                \"last_error\": repr(e)\n",
    "            })\n",
    "            logger.warning(\"RequestException for %s (attempt %s): %s\", url, attempts + 1, e)\n",
    "            return result\n",
    "\n",
    "    # Worker function wraps attempts + backoff\n",
    "    def _worker_task(key):\n",
    "        meta = status[key]\n",
    "        # If already success or permanently failed, return\n",
    "        if meta.get(\"status\") == \"success\" or meta.get(\"attempts\", 0) >= max_attempts_per_url:\n",
    "            return key, meta\n",
    "\n",
    "        # attempt download\n",
    "        new_meta = _attempt_download(key, meta)\n",
    "\n",
    "        # If still pending (retry-worthy), sleep exponential backoff before returning\n",
    "        if new_meta[\"status\"] == \"pending\":\n",
    "            # compute sleep: base * 2^(attempts-1) + jitter\n",
    "            sleep = backoff_base * (2 ** (new_meta[\"attempts\"] - 1))\n",
    "            jitter = random.uniform(0, 1.0)\n",
    "            sleep_time = min(sleep + jitter, 60)  # cap at 60s\n",
    "            logger.info(\"Backing off %0.2fs for %s (attempt %s)\", sleep_time, new_meta[\"url\"], new_meta[\"attempts\"])\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        return key, new_meta\n",
    "\n",
    "    # Main loop: do rounds where each round runs up to max_workers concurrent attempts on pending items.\n",
    "    pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "    round_idx = 0\n",
    "    while pending_keys:\n",
    "        round_idx += 1\n",
    "        logger.info(\"Download round %d: %d pending\", round_idx, len(pending_keys))\n",
    "\n",
    "        # Limit concurrency to not overload server\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futures = {ex.submit(_worker_task, key): key for key in pending_keys}\n",
    "            for fut in as_completed(futures):\n",
    "                key = futures[fut]\n",
    "                try:\n",
    "                    k, new_meta = fut.result()\n",
    "                    status[k].update(new_meta)\n",
    "                except Exception as e:\n",
    "                    logger.exception(\"Unhandled exception for key %s: %s\", key, e)\n",
    "                    status[key][\"attempts\"] = status[key].get(\"attempts\", 0) + 1\n",
    "                    status[key][\"last_error\"] = repr(e)\n",
    "\n",
    "        # persist status to disk after every round\n",
    "        try:\n",
    "            with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(status, f, indent=2)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Failed to write status file: %s\", e)\n",
    "\n",
    "        # Prepare next round: only keys still pending and under attempts limit\n",
    "        pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "\n",
    "        # If there are pending keys, optionally small delay between rounds\n",
    "        if pending_keys:\n",
    "            logger.info(\"Sleeping 2s between rounds to be polite...\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    # final persist\n",
    "    with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(status, f, indent=2)\n",
    "\n",
    "    # return status mapping\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf65b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GPT scraper V4 (collects server wait times from the server and uses these to prevent overload errors)\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Dict, Optional\n",
    "from email.utils import parsedate_to_datetime\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# -------------------\n",
    "# Logger configuration helper\n",
    "# -------------------\n",
    "def configure_logger(log_file: Optional[str] = None, level: int = logging.DEBUG):\n",
    "    \"\"\"\n",
    "    Configure a compact logger for the scraper.\n",
    "    - Console handler always enabled.\n",
    "    - Optional file handler if log_file provided.\n",
    "    - Default level: DEBUG for maximum visibility while testing.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"scraper\")\n",
    "    logger.setLevel(level)\n",
    "\n",
    "    # Avoid adding handlers multiple times when running multiple times in a notebook\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    # Console handler (clear, one-line format)\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(level)\n",
    "    ch.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s: %(message)s\"))\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    # Optional file handler (rotating not necessary here — keep simple)\n",
    "    if log_file:\n",
    "        fh = logging.FileHandler(log_file, encoding=\"utf-8\")\n",
    "        fh.setLevel(level)\n",
    "        fh.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s: %(message)s\"))\n",
    "        logger.addHandler(fh)\n",
    "\n",
    "    return logger\n",
    "\n",
    "# Initialize logger (call this in your notebook before running download_transcripts)\n",
    "logger = configure_logger()  # or configure_logger(\"data/scraper.log\")\n",
    "\n",
    "# -------------------\n",
    "# small sanitize helper (same as before)\n",
    "# -------------------\n",
    "def _sanitize_key(key: str) -> str:\n",
    "    if not isinstance(key, str):\n",
    "        key = str(key)\n",
    "    s = key.strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\-_]+\", \"-\", s)\n",
    "    s = re.sub(r\"-{2,}\", \"-\", s)\n",
    "    return s.strip(\"-_\")\n",
    "\n",
    "# ---- Helper: random-ish UA list (small) ----\n",
    "_SIMPLE_USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "]\n",
    "\n",
    "def _choose_headers():\n",
    "    return {\"User-Agent\": random.choice(_SIMPLE_USER_AGENTS)}\n",
    "\n",
    "# ----- Helper to access retry limits from the server (for use in scraper)\n",
    "def _parse_retry_after(header_value: Optional[str]) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Parse Retry-After header. It can be:\n",
    "      - an integer number of seconds, e.g. \"120\"\n",
    "      - a HTTP-date string, e.g. \"Wed, 21 Oct 2015 07:28:00 GMT\"\n",
    "    Return number of seconds to wait (float), or None if not parseable.\n",
    "    \"\"\"\n",
    "    if not header_value:\n",
    "        return None\n",
    "    header_value = header_value.strip()\n",
    "    # try integer seconds\n",
    "    if header_value.isdigit():\n",
    "        try:\n",
    "            return float(header_value)\n",
    "        except Exception:\n",
    "            return None\n",
    "    # try HTTP-date\n",
    "    try:\n",
    "        dt = parsedate_to_datetime(header_value)\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        now = datetime.now(timezone.utc)\n",
    "        delta = (dt - now).total_seconds()\n",
    "        return max(0.0, float(delta))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# -------------------\n",
    "# download_transcripts with extra logging (no other behavioural changes)\n",
    "# -------------------\n",
    "def download_transcripts(\n",
    "    url_map: Dict[str, str],\n",
    "    out_dir: str,\n",
    "    status_path: str,\n",
    "    max_attempts_per_url: int = 5,\n",
    "    backoff_base: float = 1.0,\n",
    "    max_workers: int = 3,\n",
    "    session: Optional[requests.Session] = None,\n",
    "    timeout: float = 12.0,\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Download URLs to out_dir using url_map (keys are slugs used as filenames).\n",
    "    Added logging provides visibility into what the function does on each run.\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    status_path = Path(status_path)\n",
    "\n",
    "    logger.info(\"Starting download_transcripts: %d urls, out_dir=%s, status_path=%s\",\n",
    "                len(url_map), out_dir, status_path)\n",
    "\n",
    "    # Load existing status if present (allows resume)\n",
    "    if status_path.exists():\n",
    "        try:\n",
    "            with open(status_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                status = json.load(f)\n",
    "            logger.debug(\"Loaded existing status.json with %d entries\", len(status))\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Failed to load status.json (%s). Starting with empty status.\", e)\n",
    "            status = {}\n",
    "    else:\n",
    "        logger.debug(\"No status.json file found at %s. Starting fresh.\", status_path)\n",
    "        status = {}\n",
    "\n",
    "    # Initialize status entries for any missing keys (log each new init)\n",
    "    for key, url in url_map.items():\n",
    "        if key not in status:\n",
    "            status[key] = {\n",
    "                \"url\": url,\n",
    "                \"attempts\": 0,\n",
    "                \"status\": \"pending\",  # pending | success | failed\n",
    "                \"saved_path\": None,\n",
    "                \"last_error\": None,\n",
    "            }\n",
    "            logger.debug(\"Initialized status for key='%s' -> %s\", key, url)\n",
    "\n",
    "    # Use a single session for pooling\n",
    "    session = session or requests.Session()\n",
    "\n",
    "    def _attempt_download(key: str, meta: Dict) -> Dict:\n",
    "        url = meta[\"url\"]\n",
    "        attempts = meta[\"attempts\"]\n",
    "        result = dict(meta)\n",
    "\n",
    "        # If already succeeded, skip and log reason\n",
    "        if meta.get(\"status\") == \"success\":\n",
    "            logger.debug(\"Skipping key='%s' (already success, saved_path=%s)\", key, meta.get(\"saved_path\"))\n",
    "            return result\n",
    "\n",
    "        # If max attempts reached, log and skip\n",
    "        if attempts >= max_attempts_per_url:\n",
    "            result[\"status\"] = \"failed\"\n",
    "            result[\"last_error\"] = \"max_attempts_reached\"\n",
    "            logger.info(\"Key='%s' reached max attempts (%d). Marking failed.\", key, attempts)\n",
    "            return result\n",
    "\n",
    "        # Log the attempt about to be made\n",
    "        logger.debug(\"Attempting key='%s' (attempt %d) -> %s\", key, attempts + 1, url)\n",
    "        try:\n",
    "            headers = _choose_headers()\n",
    "            resp = session.get(url, headers=headers, timeout=timeout)\n",
    "\n",
    "            # If success (200)\n",
    "            if resp.status_code == 200:\n",
    "                safe_key = _sanitize_key(key)\n",
    "                filename = f\"{safe_key}.html\"\n",
    "                saved_path = str(out_dir / filename)\n",
    "\n",
    "                # If file already exists, log that we're overwriting (helps debug)\n",
    "                if Path(saved_path).exists():\n",
    "                    logger.debug(\"File %s already exists and will be overwritten by key='%s'\", saved_path, key)\n",
    "\n",
    "                with open(saved_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "                    fh.write(resp.text)\n",
    "\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"success\",\n",
    "                    \"saved_path\": saved_path,\n",
    "                    \"last_error\": None,\n",
    "                })\n",
    "                logger.info(\"Saved %s -> %s (key=%s)\", url, saved_path, key)\n",
    "                return result\n",
    "\n",
    "            # Retryable status codes\n",
    "            if resp.status_code in (429, 500, 502, 503, 504):\n",
    "                # Parse Retry-After header if present and include in result\n",
    "                retry_after_raw = resp.headers.get(\"Retry-After\")\n",
    "                retry_after_seconds = _parse_retry_after(retry_after_raw)\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"pending\",\n",
    "                    \"last_error\": f\"status_{resp.status_code}\",\n",
    "                    \"retry_after_seconds\": retry_after_seconds,\n",
    "                })\n",
    "                logger.warning(\"Retryable HTTP %s for key='%s' url=%s (attempt %s)\",\n",
    "                               resp.status_code, key, url, attempts + 1)\n",
    "                # Log headers optionally for 429 to see Retry-After\n",
    "                if resp.status_code == 429:\n",
    "                    logger.debug(\"429 response headers for key='%s': Retry-After=%s\", key, retry_after_raw)\n",
    "                    logger.debug(\"Parsed Retry-After seconds for key='%s': %s\", key, retry_after_seconds)\n",
    "                return result\n",
    "\n",
    "            # Non-retryable\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"failed\",\n",
    "                \"last_error\": f\"status_{resp.status_code}\"\n",
    "            })\n",
    "            logger.error(\"Non-retryable HTTP %s for key='%s' url=%s\", resp.status_code, key, url)\n",
    "            return result\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            # Network error: retryable\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"pending\",\n",
    "                \"last_error\": repr(e)\n",
    "            })\n",
    "            logger.warning(\"RequestException for key='%s' url=%s (attempt %s): %s\", key, url, attempts + 1, e)\n",
    "            return result\n",
    "\n",
    "    # Worker wrapper with backoff\n",
    "    def _worker_task(key):\n",
    "        meta = status[key]\n",
    "        if meta.get(\"status\") == \"success\" or meta.get(\"attempts\", 0) >= max_attempts_per_url:\n",
    "            return key, meta\n",
    "\n",
    "        new_meta = _attempt_download(key, meta)\n",
    "\n",
    "        if new_meta[\"status\"] == \"pending\":\n",
    "            # computed exponential backoff (what we would do)\n",
    "            comp_sleep = backoff_base * (2 ** (new_meta[\"attempts\"] - 1))\n",
    "            jitter = random.uniform(0, 1.0)\n",
    "            computed_sleep = comp_sleep + jitter\n",
    "\n",
    "            # server-provided advice (if any)\n",
    "            retry_after = new_meta.get(\"retry_after_seconds\")\n",
    "            if retry_after is not None:\n",
    "                # use the server's suggestion if it's longer than our computed wait\n",
    "                sleep_time = max(computed_sleep, float(retry_after))\n",
    "            else:\n",
    "                sleep_time = computed_sleep\n",
    "\n",
    "            # cap to avoid runaway sleeps (adjust cap as desired)\n",
    "            sleep_time = min(sleep_time, 600.0)\n",
    "\n",
    "            logger.info(\"Backing off %0.2fs for key='%s' (attempt %s) [computed=%0.2fs, server=%s]\",\n",
    "                        sleep_time, key, new_meta[\"attempts\"], computed_sleep, retry_after)\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        return key, new_meta\n",
    "\n",
    "    # Main loop\n",
    "    pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "    round_idx = 0\n",
    "    while pending_keys:\n",
    "        round_idx += 1\n",
    "        logger.info(\"Download round %d: %d pending\", round_idx, len(pending_keys))\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futures = {ex.submit(_worker_task, key): key for key in pending_keys}\n",
    "            for fut in as_completed(futures):\n",
    "                key = futures[fut]\n",
    "                try:\n",
    "                    k, new_meta = fut.result()\n",
    "                    status[k].update(new_meta)\n",
    "                except Exception as e:\n",
    "                    logger.exception(\"Unhandled exception for key %s: %s\", key, e)\n",
    "                    status[key][\"attempts\"] = status[key].get(\"attempts\", 0) + 1\n",
    "                    status[key][\"last_error\"] = repr(e)\n",
    "\n",
    "        # persist status to disk after every round\n",
    "        try:\n",
    "            with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(status, f, indent=2)\n",
    "            logger.debug(\"Persisted status.json (round %d).\", round_idx)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Failed to write status file: %s\", e)\n",
    "\n",
    "        # Prepare next round\n",
    "        pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "\n",
    "        if pending_keys:\n",
    "            logger.info(\"Sleeping 2s between rounds to be polite...\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    # final persist and summary\n",
    "    with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(status, f, indent=2)\n",
    "\n",
    "    # Final summary counts\n",
    "    succ = sum(1 for v in status.values() if v.get(\"status\") == \"success\")\n",
    "    failed = sum(1 for v in status.values() if v.get(\"status\") == \"failed\")\n",
    "    pending = sum(1 for v in status.values() if v.get(\"status\") == \"pending\")\n",
    "    logger.info(\"Download finished. success=%d failed=%d pending=%d\", succ, failed, pending)\n",
    "\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd5ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,\n",
       " [('john-early',\n",
       "   'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/john-early'),\n",
       "  ('nina-conti',\n",
       "   'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/nina-conti'),\n",
       "  ('ep-299-katherine-parkinson-live-in-london',\n",
       "   'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-299-katherine-parkinson-live-in-london')])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- First ten eps test batch, using metadata to scrape. Cell A, build URL map ---\n",
    "# URL map is a dict of slugs and urls, which feels a bit unnecessary given they're already linked by row\n",
    "\n",
    "# Replace base_url below if you need to construct URLs from slugs:\n",
    "base_url = \"https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster\"\n",
    "\n",
    "# If sliced_df has a 'url' column already:\n",
    "if \"url\" in first_ten_eps_metadata_df.columns:\n",
    "    url_map = {row[\"slug\"]: row[\"url\"] for _, row in first_ten_eps_metadata_df.iterrows()}\n",
    "else:\n",
    "    # build urls by joining base_url and slug (only do this if that matches the website)\n",
    "    url_map = {row[\"slug\"]: base_url.rstrip(\"/\") + \"/\" + row[\"slug\"].lstrip(\"/\") for _, row in first_ten_eps_metadata_df.iterrows()}\n",
    "\n",
    "len(url_map), list(url_map.items())[:3]  # quick check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c133e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 10:52:40,457 INFO: Starting download_transcripts: 11 urls, out_dir=c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts, status_path=c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\status.json\n",
      "2025-12-01 10:52:40,489 DEBUG: Loaded existing status.json with 21 entries\n",
      "2025-12-01 10:52:40,489 DEBUG: Initialized status for key='john-early' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/john-early\n",
      "2025-12-01 10:52:40,489 DEBUG: Initialized status for key='nina-conti' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/nina-conti\n",
      "2025-12-01 10:52:40,489 DEBUG: Initialized status for key='ep-299-katherine-parkinson-live-in-london' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-299-katherine-parkinson-live-in-london\n",
      "2025-12-01 10:52:40,496 DEBUG: Initialized status for key='ep-288-bridget-christie-tasting-menu' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-288-bridget-christie-tasting-menu\n",
      "2025-12-01 10:52:40,496 DEBUG: Initialized status for key='ep-278-john-kearns-tasting-menu' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-278-john-kearns-tasting-menu\n",
      "2025-12-01 10:52:40,497 DEBUG: Initialized status for key='ep-268-jessica-hynes' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-268-jessica-hynes\n",
      "2025-12-01 10:52:40,497 DEBUG: Initialized status for key='ep-258-phil-dunster' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-258-phil-dunster\n",
      "2025-12-01 10:52:40,500 DEBUG: Initialized status for key='ep-248-huge-davies' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-248-huge-davies\n",
      "2025-12-01 10:52:40,500 DEBUG: Initialized status for key='ep-238-katy-wix' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-238-katy-wix\n",
      "2025-12-01 10:52:40,500 DEBUG: Initialized status for key='ep-228-ray-winstone' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-228-ray-winstone\n",
      "2025-12-01 10:52:40,502 DEBUG: Initialized status for key='ep-218-jada-pinkett-smith' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-218-jada-pinkett-smith\n",
      "2025-12-01 10:52:40,502 INFO: Download round 1: 11 pending\n",
      "2025-12-01 10:52:40,505 DEBUG: Attempting key='john-early' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/john-early\n",
      "2025-12-01 10:52:40,509 DEBUG: Attempting key='nina-conti' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/nina-conti\n",
      "2025-12-01 10:52:42,913 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/john-early -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\john-early.html (key=john-early)\n",
      "2025-12-01 10:52:42,916 DEBUG: Attempting key='ep-299-katherine-parkinson-live-in-london' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-299-katherine-parkinson-live-in-london\n",
      "2025-12-01 10:52:43,421 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/nina-conti -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\nina-conti.html (key=nina-conti)\n",
      "2025-12-01 10:52:43,423 DEBUG: Attempting key='ep-288-bridget-christie-tasting-menu' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-288-bridget-christie-tasting-menu\n",
      "2025-12-01 10:52:44,423 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-299-katherine-parkinson-live-in-london -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-299-katherine-parkinson-live-in-london.html (key=ep-299-katherine-parkinson-live-in-london)\n",
      "2025-12-01 10:52:44,425 DEBUG: Attempting key='ep-278-john-kearns-tasting-menu' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-278-john-kearns-tasting-menu\n",
      "2025-12-01 10:52:45,339 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-288-bridget-christie-tasting-menu -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-288-bridget-christie-tasting-menu.html (key=ep-288-bridget-christie-tasting-menu)\n",
      "2025-12-01 10:52:45,341 DEBUG: Attempting key='ep-268-jessica-hynes' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-268-jessica-hynes\n",
      "2025-12-01 10:52:45,924 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-278-john-kearns-tasting-menu -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-278-john-kearns-tasting-menu.html (key=ep-278-john-kearns-tasting-menu)\n",
      "2025-12-01 10:52:45,927 DEBUG: Attempting key='ep-258-phil-dunster' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-258-phil-dunster\n",
      "2025-12-01 10:52:46,879 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-268-jessica-hynes -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-268-jessica-hynes.html (key=ep-268-jessica-hynes)\n",
      "2025-12-01 10:52:46,883 DEBUG: Attempting key='ep-248-huge-davies' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-248-huge-davies\n",
      "2025-12-01 10:52:46,900 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-258-phil-dunster -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-258-phil-dunster.html (key=ep-258-phil-dunster)\n",
      "2025-12-01 10:52:46,902 DEBUG: Attempting key='ep-238-katy-wix' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-238-katy-wix\n",
      "2025-12-01 10:52:48,105 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-248-huge-davies -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-248-huge-davies.html (key=ep-248-huge-davies)\n",
      "2025-12-01 10:52:48,109 DEBUG: Attempting key='ep-228-ray-winstone' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-228-ray-winstone\n",
      "2025-12-01 10:52:48,915 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-238-katy-wix -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-238-katy-wix.html (key=ep-238-katy-wix)\n",
      "2025-12-01 10:52:48,915 DEBUG: Attempting key='ep-218-jada-pinkett-smith' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-218-jada-pinkett-smith\n",
      "2025-12-01 10:52:49,019 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-228-ray-winstone -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-228-ray-winstone.html (key=ep-228-ray-winstone)\n",
      "2025-12-01 10:52:50,637 WARNING: Retryable HTTP 429 for key='ep-218-jada-pinkett-smith' url=https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-218-jada-pinkett-smith (attempt 1)\n",
      "2025-12-01 10:52:50,639 DEBUG: 429 response headers for key='ep-218-jada-pinkett-smith': Retry-After=51\n",
      "2025-12-01 10:52:50,643 DEBUG: Parsed Retry-After seconds for key='ep-218-jada-pinkett-smith': 51.0\n",
      "2025-12-01 10:52:50,645 INFO: Backing off 51.00s for key='ep-218-jada-pinkett-smith' (attempt 1) [computed=2.74s, server=51.0]\n",
      "2025-12-01 10:53:41,656 DEBUG: Persisted status.json (round 1).\n",
      "2025-12-01 10:53:41,658 INFO: Sleeping 2s between rounds to be polite...\n",
      "2025-12-01 10:53:43,661 INFO: Download round 2: 1 pending\n",
      "2025-12-01 10:53:43,664 DEBUG: Attempting key='ep-218-jada-pinkett-smith' (attempt 2) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-218-jada-pinkett-smith\n",
      "2025-12-01 10:53:47,036 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-218-jada-pinkett-smith -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-218-jada-pinkett-smith.html (key=ep-218-jada-pinkett-smith)\n",
      "2025-12-01 10:53:47,053 DEBUG: Persisted status.json (round 2).\n",
      "2025-12-01 10:53:47,053 INFO: Download finished. success=32 failed=0 pending=0\n"
     ]
    }
   ],
   "source": [
    "# --- Test: First ten eps test batch, using metadata to scrape. Cell B, run download_transcripts ---\n",
    "test_transcripts_dir = os.path.join(V2_tests_dir, \"test_transcripts\")      # where HTMLs will be saved \n",
    "status_path = os.path.join(test_transcripts_dir, \"status.json\")\n",
    "\n",
    "# tune these for a polite test run\n",
    "max_attempts_per_url = 8\n",
    "backoff_base = 2.0\n",
    "max_workers = 2   # start low while testing\n",
    "\n",
    "# call the function (assumes download_transcripts is in scope)\n",
    "status = download_transcripts(\n",
    "    url_map=url_map,\n",
    "    out_dir=test_transcripts_dir,\n",
    "    status_path=status_path,\n",
    "    max_attempts_per_url=max_attempts_per_url,\n",
    "    backoff_base=backoff_base,\n",
    "    max_workers=max_workers,\n",
    "    timeout=12.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1dc56ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,\n",
       " [('ep-218-jada-pinkett-smith',\n",
       "   'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-218-jada-pinkett-smith'),\n",
       "  ('ep-208-izuka-hoyle',\n",
       "   'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-208-izuka-hoyle'),\n",
       "  ('ep-198-graham-coxon',\n",
       "   'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-198-graham-coxon')])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Test: Second ten eps test batch, using metadata to scrape. Cell A, build URL map ---\n",
    "\n",
    "# Cell A — Build url_map\n",
    "# Replace base_url below if you need to construct URLs from slugs:\n",
    "base_url = \"https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster\"\n",
    "\n",
    "# If second_ten_test_data_df has a 'url' column already:\n",
    "if \"url\" in second_ten_eps_metadata_df.columns:\n",
    "    url_map_2 = {row[\"slug\"]: row[\"url\"] for _, row in second_ten_eps_metadata_df.iterrows()}\n",
    "else:\n",
    "    # build urls by joining base_url and slug (only do this if that matches the website)\n",
    "    url_map_2 = {row[\"slug\"]: base_url.rstrip(\"/\") + \"/\" + row[\"slug\"].lstrip(\"/\") for _, row in second_ten_eps_metadata_df.iterrows()}\n",
    "\n",
    "len(url_map_2), list(url_map_2.items())[:3]  # quick check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda992d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 10:53:47,114 INFO: Starting download_transcripts: 11 urls, out_dir=c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts, status_path=c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\status.json\n",
      "2025-12-01 10:53:47,158 DEBUG: Loaded existing status.json with 32 entries\n",
      "2025-12-01 10:53:47,158 DEBUG: Initialized status for key='ep-208-izuka-hoyle' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-208-izuka-hoyle\n",
      "2025-12-01 10:53:47,160 DEBUG: Initialized status for key='ep-198-graham-coxon' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-198-graham-coxon\n",
      "2025-12-01 10:53:47,160 DEBUG: Initialized status for key='ep-188-alex-jones' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-188-alex-jones\n",
      "2025-12-01 10:53:47,165 DEBUG: Initialized status for key='ep-179-yotam-ottolenghi' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-179-yotam-ottolenghi\n",
      "2025-12-01 10:53:47,169 DEBUG: Initialized status for key='ep-170-babatunde-aleshe' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-170-babatunde-aleshe\n",
      "2025-12-01 10:53:47,171 DEBUG: Initialized status for key='ep-160-sir-lenny-henry' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-160-sir-lenny-henry\n",
      "2025-12-01 10:53:47,171 DEBUG: Initialized status for key='ep-150-angela-hartnett' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-150-angela-hartnett\n",
      "2025-12-01 10:53:47,173 DEBUG: Initialized status for key='ep-140-claudia-jessie' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-140-claudia-jessie\n",
      "2025-12-01 10:53:47,175 DEBUG: Initialized status for key='ep-130-bridget-christie' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-130-bridget-christie\n",
      "2025-12-01 10:53:47,175 DEBUG: Initialized status for key='ep-120-miriam-margolyes' -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-120-miriam-margolyes\n",
      "2025-12-01 10:53:47,178 INFO: Download round 1: 10 pending\n",
      "2025-12-01 10:53:47,180 DEBUG: Attempting key='ep-208-izuka-hoyle' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-208-izuka-hoyle\n",
      "2025-12-01 10:53:47,204 DEBUG: Attempting key='ep-198-graham-coxon' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-198-graham-coxon\n",
      "2025-12-01 10:53:49,040 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-198-graham-coxon -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-198-graham-coxon.html (key=ep-198-graham-coxon)\n",
      "2025-12-01 10:53:49,041 DEBUG: Attempting key='ep-188-alex-jones' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-188-alex-jones\n",
      "2025-12-01 10:53:49,958 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-208-izuka-hoyle -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-208-izuka-hoyle.html (key=ep-208-izuka-hoyle)\n",
      "2025-12-01 10:53:49,958 DEBUG: Attempting key='ep-179-yotam-ottolenghi' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-179-yotam-ottolenghi\n",
      "2025-12-01 10:53:50,681 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-188-alex-jones -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-188-alex-jones.html (key=ep-188-alex-jones)\n",
      "2025-12-01 10:53:50,683 DEBUG: Attempting key='ep-170-babatunde-aleshe' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-170-babatunde-aleshe\n",
      "2025-12-01 10:53:51,554 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-179-yotam-ottolenghi -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-179-yotam-ottolenghi.html (key=ep-179-yotam-ottolenghi)\n",
      "2025-12-01 10:53:51,554 DEBUG: Attempting key='ep-160-sir-lenny-henry' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-160-sir-lenny-henry\n",
      "2025-12-01 10:53:52,056 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-170-babatunde-aleshe -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-170-babatunde-aleshe.html (key=ep-170-babatunde-aleshe)\n",
      "2025-12-01 10:53:52,060 DEBUG: Attempting key='ep-150-angela-hartnett' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-150-angela-hartnett\n",
      "2025-12-01 10:53:53,037 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-160-sir-lenny-henry -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-160-sir-lenny-henry.html (key=ep-160-sir-lenny-henry)\n",
      "2025-12-01 10:53:53,039 DEBUG: Attempting key='ep-140-claudia-jessie' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-140-claudia-jessie\n",
      "2025-12-01 10:53:53,119 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-150-angela-hartnett -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-150-angela-hartnett.html (key=ep-150-angela-hartnett)\n",
      "2025-12-01 10:53:53,124 DEBUG: Attempting key='ep-130-bridget-christie' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-130-bridget-christie\n",
      "2025-12-01 10:53:54,236 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-130-bridget-christie -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-130-bridget-christie.html (key=ep-130-bridget-christie)\n",
      "2025-12-01 10:53:54,238 DEBUG: Attempting key='ep-120-miriam-margolyes' (attempt 1) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-120-miriam-margolyes\n",
      "2025-12-01 10:53:54,403 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-140-claudia-jessie -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-140-claudia-jessie.html (key=ep-140-claudia-jessie)\n",
      "2025-12-01 10:53:54,543 WARNING: Retryable HTTP 429 for key='ep-120-miriam-margolyes' url=https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-120-miriam-margolyes (attempt 1)\n",
      "2025-12-01 10:53:54,543 DEBUG: 429 response headers for key='ep-120-miriam-margolyes': Retry-After=49\n",
      "2025-12-01 10:53:54,546 DEBUG: Parsed Retry-After seconds for key='ep-120-miriam-margolyes': 49.0\n",
      "2025-12-01 10:53:54,549 INFO: Backing off 49.00s for key='ep-120-miriam-margolyes' (attempt 1) [computed=2.77s, server=49.0]\n",
      "2025-12-01 10:54:43,552 DEBUG: Persisted status.json (round 1).\n",
      "2025-12-01 10:54:43,554 INFO: Sleeping 2s between rounds to be polite...\n",
      "2025-12-01 10:54:45,558 INFO: Download round 2: 1 pending\n",
      "2025-12-01 10:54:45,562 DEBUG: Attempting key='ep-120-miriam-margolyes' (attempt 2) -> https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-120-miriam-margolyes\n",
      "2025-12-01 10:54:46,886 INFO: Saved https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-120-miriam-margolyes -> c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\ep-120-miriam-margolyes.html (key=ep-120-miriam-margolyes)\n",
      "2025-12-01 10:54:46,891 DEBUG: Persisted status.json (round 2).\n",
      "2025-12-01 10:54:46,897 INFO: Download finished. success=42 failed=0 pending=0\n"
     ]
    }
   ],
   "source": [
    "# --- Test: Second ten eps test batch, using metadata to scrape. Cell B, run download_transcripts ---\n",
    "\n",
    "test_transcripts_dir = os.path.join(V2_tests_dir, \"test_transcripts\")      # where HTMLs will be saved \n",
    "status_path = os.path.join(test_transcripts_dir, \"status.json\")\n",
    "\n",
    "# tune these for a polite test run\n",
    "max_attempts_per_url = 8\n",
    "backoff_base = 2.0\n",
    "max_workers = 2   # start low while testing\n",
    "\n",
    "# call the function (assumes download_transcripts is in scope)\n",
    "status = download_transcripts(\n",
    "    url_map=url_map_2,\n",
    "    out_dir=test_transcripts_dir,\n",
    "    status_path=status_path,\n",
    "    max_attempts_per_url=max_attempts_per_url,\n",
    "    backoff_base=backoff_base,\n",
    "    max_workers=max_workers,\n",
    "    timeout=12.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9d2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 10:54:47,104 INFO: Starting download_transcripts: 11 urls, out_dir=c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts, status_path=c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\test_transcripts\\status.json\n",
      "2025-12-01 10:54:47,131 DEBUG: Loaded existing status.json with 42 entries\n",
      "2025-12-01 10:54:47,158 INFO: Download finished. success=42 failed=0 pending=0\n"
     ]
    }
   ],
   "source": [
    "# --- Scraper orchestration function notes (everything we need to do first) ---\n",
    "\n",
    "logger = configure_logger()\n",
    "\n",
    "# If sliced_df has a 'url' column already:\n",
    "if \"url\" in first_ten_eps_metadata_df.columns:\n",
    "    url_map = {row[\"slug\"]: row[\"url\"] for _, row in first_ten_eps_metadata_df.iterrows()}\n",
    "else:\n",
    "    # build urls by joining base_url and slug (only do this if that matches the website)\n",
    "    url_map = {row[\"slug\"]: base_url.rstrip(\"/\") + \"/\" + row[\"slug\"].lstrip(\"/\") for _, row in first_ten_eps_metadata_df.iterrows()}\n",
    "\n",
    "out_dir = os.path.join(V2_tests_dir, \"test_transcripts\")      # where HTMLs will be saved \n",
    "status_path = os.path.join(out_dir, \"status.json\")\n",
    "max_attempts_per_url = 8\n",
    "backoff_base = 2.0\n",
    "max_workers = 2   # start low while testing\n",
    "\n",
    "# call the function (assumes download_transcripts is in scope)\n",
    "status = download_transcripts(\n",
    "    url_map=url_map_2,\n",
    "    out_dir=out_dir,\n",
    "    status_path=status_path,\n",
    "    max_attempts_per_url=max_attempts_per_url,\n",
    "    backoff_base=backoff_base,\n",
    "    max_workers=max_workers,\n",
    "    timeout=12.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b2b5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GPT scraper orchestration function ---\n",
    "\n",
    "def orchestrate_scraper(\n",
    "    df,                     # DataFrame with 'slug' and optionally 'url'\n",
    "    base_url,               # base URL for constructing URLs if df has no 'url' column\n",
    "    out_dir,                # folder to save HTML transcripts\n",
    "    max_attempts_per_url=5,\n",
    "    backoff_base=1.0,\n",
    "    max_workers=3,\n",
    "    timeout=12.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Orchestrates the scraping process:\n",
    "      1. Prepares a slug → URL map\n",
    "      2. Ensures output folder exists\n",
    "      3. Calls download_transcripts() with sensible defaults\n",
    "      4. Returns the status dict for all downloads\n",
    "    \"\"\"\n",
    "    # ---------------------\n",
    "    # Setup logger for this run\n",
    "    # ---------------------\n",
    "    logger = configure_logger()\n",
    "    logger.info(\"Starting scraper orchestration for %d episodes\", len(df))\n",
    "\n",
    "    # ---------------------\n",
    "    # Prepare URL map\n",
    "    # ---------------------\n",
    "    if \"url\" in df.columns:\n",
    "        url_map = {row[\"slug\"]: row[\"url\"] for _, row in df.iterrows()}\n",
    "        logger.info(\"Using existing URLs from DataFrame\")\n",
    "    else:\n",
    "        url_map = {row[\"slug\"]: base_url.rstrip(\"/\") + \"/\" + row[\"slug\"].lstrip(\"/\") for _, row in df.iterrows()}\n",
    "        logger.info(\"Constructed URLs from base_url and slugs\")\n",
    "\n",
    "    # ---------------------\n",
    "    # Ensure output folder exists\n",
    "    # ---------------------\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    status_path = out_dir / \"status.json\"\n",
    "\n",
    "    # ---------------------\n",
    "    # Call the scraper\n",
    "    # ---------------------\n",
    "    logger.info(\"Running download_transcripts with %d URLs\", len(url_map))\n",
    "    status = download_transcripts(\n",
    "        url_map=url_map,\n",
    "        out_dir=out_dir,\n",
    "        status_path=status_path,\n",
    "        max_attempts_per_url=max_attempts_per_url,\n",
    "        backoff_base=backoff_base,\n",
    "        max_workers=max_workers,\n",
    "        timeout=timeout\n",
    "    )\n",
    "\n",
    "    logger.info(\"Scraper orchestration finished\")\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e8635b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstatus = orchestrate_scraper(\\n    df=second_ten_test_data_df,\\n    base_url=\"https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster\",\\n    out_dir=os.path.join(V2_tests_dir, \"test_transcripts\"),\\n    max_attempts_per_url=8,\\n    backoff_base=2.0,\\n    max_workers=2\\n)\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Testing orchstrator (COMMENTED OUT) ---\n",
    "\"\"\"\n",
    "status = orchestrate_scraper(\n",
    "    df=second_ten_test_data_df,\n",
    "    base_url=\"https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster\",\n",
    "    out_dir=os.path.join(V2_tests_dir, \"test_transcripts\"),\n",
    "    max_attempts_per_url=8,\n",
    "    backoff_base=2.0,\n",
    "    max_workers=2\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15374254",
   "metadata": {},
   "source": [
    "## New extracting clean text and timestamps + combining into dataframe\n",
    "\n",
    "### Do we need to change any prior functions for the slug change? No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f4900179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function extracting timestamps from one ep, as list of dicts ---\n",
    "\n",
    "def _extract_timestamps_as_list_of_dicts(\n",
    "    transcript_str: str, slug: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Finds all 'starting point is HH:MM:SS' timestamps in a transcript string.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of dictionaries, where each dict contains the episode\n",
    "                               number, timestamp string, and its starting index.\n",
    "    \"\"\"\n",
    "    timestamp_pattern = re.compile(r\"starting point is (\\d{2}:\\d{2}:\\d{2})\")\n",
    "    all_timestamps_in_transcript = []\n",
    "    for match in timestamp_pattern.finditer(transcript_str):\n",
    "        # Get the captured timestamp string (e.g., \"00:00:05\")\n",
    "        actual_time_string = match.group(1)\n",
    "        # We use group(1) because that's our (HH:MM:SS) part, group(0) refers to the whole string by default\n",
    "\n",
    "        # Get the starting index of the entire match\n",
    "        start_position_in_text = match.start()\n",
    "        # Store this as a dict with episode_slug as key\n",
    "        stamp_dict = {\n",
    "            \"slug\": slug,\n",
    "            \"timestamp\": actual_time_string,\n",
    "            \"start_index\": start_position_in_text,\n",
    "        }\n",
    "        # Store this extracted data (the timestamp string and its position)\n",
    "        all_timestamps_in_transcript.append(stamp_dict)\n",
    "    return all_timestamps_in_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "47f27907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function orchestrating extration and saving of clean transcripts and timestamps from metadata df ---\n",
    "# Takes in episodes dataframe path, directory of transcripts, and output directory to save to\n",
    "\n",
    "def extract_save_clean_text_and_periodic_timestamps(\n",
    "    full_episodes_metadata_path: str, transcripts_dir: str, output_filepath: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Takes the full episodes metadata filepath, the transcripts html directory, and an output filepath, and iterates\n",
    "    through the episodes, processing the html into clean transcript text and collating the periodic timestamps.\n",
    "\n",
    "    These transcripts and periodic timestamps are saved in a dataframe, which is saves as a parquet file to the\n",
    "    output filepath.\n",
    "\n",
    "    Args:\n",
    "        full_episodes_metadata_path (str): The full episodes metadata dataframe filepath\n",
    "        transcripts_dir (str): The directory containing the html of each episode.\n",
    "        output_filepath (str): The filepath the output df is saved to.\n",
    "    Returns:\n",
    "        None: A dataframe containing the clean text and the timestamps (a list of Dicts) is saved to the\n",
    "        output filepath as a parquet.\n",
    "    \"\"\"\n",
    "    # 1. Load episodes meta_data\n",
    "    episodes_df = try_read_parquet(full_episodes_metadata_path)\n",
    "    if episodes_df is None or episodes_df.empty:\n",
    "        print(\n",
    "            \"INFO: No unprocessed episodes found. Nothing to do. (Either all episodes processed or metadata empty.)\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    processed_records = []  # To store data for the final DataFrame\n",
    "\n",
    "    # 2. Iterate through each episode's metadata\n",
    "    for index, row in episodes_df.iterrows():\n",
    "        episode_slug = row.get(\"slug\")\n",
    "        print(f\"Episode slug is {episode_slug}\") # DEBUGGING\n",
    "        guest_name = row.get(\"guest_name\")\n",
    "        transcript_filename = f\"{episode_slug}.html\"\n",
    "        transcript_filepath = os.path.join(transcripts_dir, transcript_filename)\n",
    "        restaurants_mentioned = row.get(\"restaurants_mentioned\")\n",
    "        # Confirm file exists and skip if not\n",
    "        if not os.path.exists(transcript_filepath):\n",
    "            print(\n",
    "                f\"  WARNING: Transcript file not found for Episode {guest_name}, slug: {episode_slug} at {transcript_filepath}. Skipping.\"\n",
    "            )\n",
    "            continue  # Skip to the next episode\n",
    "        try:\n",
    "            clean_transcript_str = _clean_transcript_str_from_html(transcript_filepath)\n",
    "            timestamps = _extract_timestamps_as_list_of_dicts(\n",
    "                clean_transcript_str, episode_slug\n",
    "            )\n",
    "\n",
    "            processed_records.append(\n",
    "                {\n",
    "                    \"slug\": episode_slug,\n",
    "                    \"guest_name\": guest_name,\n",
    "                    \"restaurants_mentioned\": restaurants_mentioned,\n",
    "                    \"clean_transcript_text\": clean_transcript_str,\n",
    "                    \"periodic_timestamps\": timestamps,  # This will be a list of dictionaries\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"  Processed Episode {episode_slug} ({guest_name}): Extracted text and {len(timestamps)} timestamps.\"\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"  ERROR: Failed to process transcript for Episode {episode_slug} ({guest_name}) from {transcript_filepath}: {e}\"\n",
    "            )\n",
    "            continue  # For MVP, just skip and warn\n",
    "\n",
    "        if processed_records:\n",
    "            result_df = pd.DataFrame(processed_records)\n",
    "            result_df.to_parquet(output_filepath, index=False)\n",
    "            print(\n",
    "                f\"Successfully saved clean transcripts and timestamps for {len(result_df)} episodes to {output_filepath}\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                \"No transcripts were successfully processed. Output DataFrame will be empty.\"\n",
    "            )\n",
    "            pd.DataFrame().to_parquet(output_filepath, index=False)  # Save an empty DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e817fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function combining episode metadata df with transcripts and timestamps df---\n",
    "# Takes args transcripts/timestamps filepath, metadata filepath\n",
    "\n",
    "\n",
    "def combine_timestamps_and_metadata(\n",
    "    transcripts_timestamps_filepath: str, metadata_filepath: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads and combines the transcripts and timestamps dataframe with the metadata dataframe.\n",
    "\n",
    "    Args:\n",
    "        transcripts_timestamps_filepath(str)\n",
    "        metadata_filepath (str)\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe containing episode slug, restaurants mentioned, clean transcript,\n",
    "        and timestamps.\n",
    "    \"\"\"\n",
    "    metadata_df = try_read_parquet(metadata_filepath)\n",
    "    transcripts_timestamps_df = try_read_parquet(transcripts_timestamps_filepath)\n",
    "    combined_df = transcripts_timestamps_df.merge(\n",
    "        metadata_df[[\"slug\", \"restaurants_mentioned\"]],\n",
    "        on=\"slug\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1cd8936a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode slug is john-early\n",
      "  Processed Episode john-early (John Early): Extracted text and 324 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 1 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_test_timestamps.parquet\n",
      "Episode slug is nina-conti\n",
      "  Processed Episode nina-conti (Nina Conti): Extracted text and 272 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 2 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_test_timestamps.parquet\n",
      "Episode slug is ep-299-katherine-parkinson-live-in-london\n",
      "  Processed Episode ep-299-katherine-parkinson-live-in-london (Katherine Parkinson): Extracted text and 152 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 3 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_test_timestamps.parquet\n",
      "Episode slug is ep-288-bridget-christie-tasting-menu\n",
      "  Processed Episode ep-288-bridget-christie-tasting-menu (Bridget Christie): Extracted text and 168 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 4 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_test_timestamps.parquet\n",
      "Episode slug is ep-278-john-kearns-tasting-menu\n",
      "  Processed Episode ep-278-john-kearns-tasting-menu (John Kearns): Extracted text and 232 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 5 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_test_timestamps.parquet\n",
      "Episode slug is ep-268-jessica-hynes\n",
      "  Processed Episode ep-268-jessica-hynes (Jessica Hynes): Extracted text and 203 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 6 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_test_timestamps.parquet\n",
      "Episode slug is ep-258-phil-dunster\n",
      "  Processed Episode ep-258-phil-dunster (Phil Dunster): Extracted text and 182 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 7 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_test_timestamps.parquet\n",
      "Episode slug is ep-248-huge-davies\n",
      "  Processed Episode ep-248-huge-davies (Huge Davies): Extracted text and 206 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 8 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_test_timestamps.parquet\n",
      "Episode slug is ep-238-katy-wix\n",
      "  Processed Episode ep-238-katy-wix (Katy Wix): Extracted text and 161 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 9 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_test_timestamps.parquet\n",
      "Episode slug is ep-228-ray-winstone\n",
      "  Processed Episode ep-228-ray-winstone (Ray Winstone): Extracted text and 198 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 10 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_test_timestamps.parquet\n",
      "Episode slug is ep-218-jada-pinkett-smith\n",
      "  Processed Episode ep-218-jada-pinkett-smith (Jada Pinkett Smith): Extracted text and 198 timestamps.\n",
      "Successfully saved clean transcripts and timestamps for 11 episodes to c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\V2_tests\\first_ten_test_timestamps.parquet\n"
     ]
    }
   ],
   "source": [
    "# --- GENERATE TEST DATA: first ten eps and their timestamps and transcripts ---\n",
    "# --- Tests: extract and save clean transcripts and timestamps, using first 10 eps metadata ---\n",
    "\n",
    "# Timestamps/transcripts saved to first_ten_metadata_timestamps_out_path\n",
    "\n",
    "# Define path\n",
    "first_ten_metadata_timestamps_out_path = os.path.join(V2_tests_dir, \"first_ten_test_timestamps.parquet\")\n",
    "\n",
    "# Extract timestamps, save df inc. metadata and transcrtips/timestamps\n",
    "extract_save_clean_text_and_periodic_timestamps(ten_test_episodes_metadata_output_path, out_dir, first_ten_metadata_timestamps_out_path)\n",
    "\n",
    "# Read dataframe for use in fuzzymatching (requires df not path)\n",
    "first_ten_metadata_timestamps_df = try_read_parquet(first_ten_metadata_timestamps_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "15da589f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slug</th>\n",
       "      <th>guest_name</th>\n",
       "      <th>restaurants_mentioned</th>\n",
       "      <th>clean_transcript_text</th>\n",
       "      <th>periodic_timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>john-early</td>\n",
       "      <td>John Early</td>\n",
       "      <td>[Princes Hot Chicken, Hattie Bs]</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "      <td>[{'slug': 'john-early', 'start_index': 0, 'tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nina-conti</td>\n",
       "      <td>Nina Conti</td>\n",
       "      <td>[Di Palos]</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "      <td>[{'slug': 'nina-conti', 'start_index': 0, 'tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ep-299-katherine-parkinson-live-in-london</td>\n",
       "      <td>Katherine Parkinson</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 oh boy, oh boy, the...</td>\n",
       "      <td>[{'slug': 'ep-299-katherine-parkinson-live-in-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ep-288-bridget-christie-tasting-menu</td>\n",
       "      <td>Bridget Christie</td>\n",
       "      <td>[Soho Hotel Refuel Bar]</td>\n",
       "      <td>starting point is 00:00:00 huge news from off-...</td>\n",
       "      <td>[{'slug': 'ep-288-bridget-christie-tasting-men...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ep-278-john-kearns-tasting-menu</td>\n",
       "      <td>John Kearns</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 welcome to the off-...</td>\n",
       "      <td>[{'slug': 'ep-278-john-kearns-tasting-menu', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ep-268-jessica-hynes</td>\n",
       "      <td>Jessica Hynes</td>\n",
       "      <td>[Everest Cash and Carry]</td>\n",
       "      <td>starting point is 00:00:00 we get it. life get...</td>\n",
       "      <td>[{'slug': 'ep-268-jessica-hynes', 'start_index...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ep-258-phil-dunster</td>\n",
       "      <td>Phil Dunster</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 i am charlotte casa...</td>\n",
       "      <td>[{'slug': 'ep-258-phil-dunster', 'start_index'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ep-248-huge-davies</td>\n",
       "      <td>Huge Davies</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 welcome to the off ...</td>\n",
       "      <td>[{'slug': 'ep-248-huge-davies', 'start_index':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ep-238-katy-wix</td>\n",
       "      <td>Katy Wix</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "      <td>[{'slug': 'ep-238-katy-wix', 'start_index': 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ep-228-ray-winstone</td>\n",
       "      <td>Ray Winstone</td>\n",
       "      <td>[Clock Tower Cafe, Scotts, Smiths]</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "      <td>[{'slug': 'ep-228-ray-winstone', 'start_index'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ep-218-jada-pinkett-smith</td>\n",
       "      <td>Jada Pinkett Smith</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 ed. yeah, man. i'm ...</td>\n",
       "      <td>[{'slug': 'ep-218-jada-pinkett-smith', 'start_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         slug           guest_name  \\\n",
       "0                                  john-early           John Early   \n",
       "1                                  nina-conti           Nina Conti   \n",
       "2   ep-299-katherine-parkinson-live-in-london  Katherine Parkinson   \n",
       "3        ep-288-bridget-christie-tasting-menu     Bridget Christie   \n",
       "4             ep-278-john-kearns-tasting-menu          John Kearns   \n",
       "5                        ep-268-jessica-hynes        Jessica Hynes   \n",
       "6                         ep-258-phil-dunster         Phil Dunster   \n",
       "7                          ep-248-huge-davies          Huge Davies   \n",
       "8                             ep-238-katy-wix             Katy Wix   \n",
       "9                         ep-228-ray-winstone         Ray Winstone   \n",
       "10                  ep-218-jada-pinkett-smith   Jada Pinkett Smith   \n",
       "\n",
       "                 restaurants_mentioned  \\\n",
       "0     [Princes Hot Chicken, Hattie Bs]   \n",
       "1                           [Di Palos]   \n",
       "2                                   []   \n",
       "3              [Soho Hotel Refuel Bar]   \n",
       "4                                   []   \n",
       "5             [Everest Cash and Carry]   \n",
       "6                                   []   \n",
       "7                                   []   \n",
       "8                                   []   \n",
       "9   [Clock Tower Cafe, Scotts, Smiths]   \n",
       "10                                  []   \n",
       "\n",
       "                                clean_transcript_text  \\\n",
       "0   starting point is 00:00:00 oh no, it's james a...   \n",
       "1   starting point is 00:00:00 hello, it's ed gamb...   \n",
       "2   starting point is 00:00:00 oh boy, oh boy, the...   \n",
       "3   starting point is 00:00:00 huge news from off-...   \n",
       "4   starting point is 00:00:00 welcome to the off-...   \n",
       "5   starting point is 00:00:00 we get it. life get...   \n",
       "6   starting point is 00:00:00 i am charlotte casa...   \n",
       "7   starting point is 00:00:00 welcome to the off ...   \n",
       "8   starting point is 00:00:00 hello, it's ed gamb...   \n",
       "9   starting point is 00:00:00 hello, it's ed gamb...   \n",
       "10  starting point is 00:00:00 ed. yeah, man. i'm ...   \n",
       "\n",
       "                                  periodic_timestamps  \n",
       "0   [{'slug': 'john-early', 'start_index': 0, 'tim...  \n",
       "1   [{'slug': 'nina-conti', 'start_index': 0, 'tim...  \n",
       "2   [{'slug': 'ep-299-katherine-parkinson-live-in-...  \n",
       "3   [{'slug': 'ep-288-bridget-christie-tasting-men...  \n",
       "4   [{'slug': 'ep-278-john-kearns-tasting-menu', '...  \n",
       "5   [{'slug': 'ep-268-jessica-hynes', 'start_index...  \n",
       "6   [{'slug': 'ep-258-phil-dunster', 'start_index'...  \n",
       "7   [{'slug': 'ep-248-huge-davies', 'start_index':...  \n",
       "8   [{'slug': 'ep-238-katy-wix', 'start_index': 0,...  \n",
       "9   [{'slug': 'ep-228-ray-winstone', 'start_index'...  \n",
       "10  [{'slug': 'ep-218-jada-pinkett-smith', 'start_...  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Inspecting transcripts timestamps df for first 10 eps ---\n",
    "\n",
    "first_ten_metadata_timestamps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ce13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slug</th>\n",
       "      <th>guest_name</th>\n",
       "      <th>restaurants_mentioned_x</th>\n",
       "      <th>clean_transcript_text</th>\n",
       "      <th>periodic_timestamps</th>\n",
       "      <th>restaurants_mentioned_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>john-early</td>\n",
       "      <td>John Early</td>\n",
       "      <td>[Princes Hot Chicken, Hattie Bs]</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "      <td>[{'slug': 'john-early', 'start_index': 0, 'tim...</td>\n",
       "      <td>[Princes Hot Chicken, Hattie Bs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nina-conti</td>\n",
       "      <td>Nina Conti</td>\n",
       "      <td>[Di Palos]</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "      <td>[{'slug': 'nina-conti', 'start_index': 0, 'tim...</td>\n",
       "      <td>[Di Palos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ep-299-katherine-parkinson-live-in-london</td>\n",
       "      <td>Katherine Parkinson</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 oh boy, oh boy, the...</td>\n",
       "      <td>[{'slug': 'ep-299-katherine-parkinson-live-in-...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ep-288-bridget-christie-tasting-menu</td>\n",
       "      <td>Bridget Christie</td>\n",
       "      <td>[Soho Hotel Refuel Bar]</td>\n",
       "      <td>starting point is 00:00:00 huge news from off-...</td>\n",
       "      <td>[{'slug': 'ep-288-bridget-christie-tasting-men...</td>\n",
       "      <td>[Soho Hotel Refuel Bar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ep-278-john-kearns-tasting-menu</td>\n",
       "      <td>John Kearns</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 welcome to the off-...</td>\n",
       "      <td>[{'slug': 'ep-278-john-kearns-tasting-menu', '...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ep-268-jessica-hynes</td>\n",
       "      <td>Jessica Hynes</td>\n",
       "      <td>[Everest Cash and Carry]</td>\n",
       "      <td>starting point is 00:00:00 we get it. life get...</td>\n",
       "      <td>[{'slug': 'ep-268-jessica-hynes', 'start_index...</td>\n",
       "      <td>[Everest Cash and Carry]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ep-258-phil-dunster</td>\n",
       "      <td>Phil Dunster</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 i am charlotte casa...</td>\n",
       "      <td>[{'slug': 'ep-258-phil-dunster', 'start_index'...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ep-248-huge-davies</td>\n",
       "      <td>Huge Davies</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 welcome to the off ...</td>\n",
       "      <td>[{'slug': 'ep-248-huge-davies', 'start_index':...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ep-238-katy-wix</td>\n",
       "      <td>Katy Wix</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "      <td>[{'slug': 'ep-238-katy-wix', 'start_index': 0,...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ep-228-ray-winstone</td>\n",
       "      <td>Ray Winstone</td>\n",
       "      <td>[Clock Tower Cafe, Scotts, Smiths]</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "      <td>[{'slug': 'ep-228-ray-winstone', 'start_index'...</td>\n",
       "      <td>[Clock Tower Cafe, Scotts, Smiths]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ep-218-jada-pinkett-smith</td>\n",
       "      <td>Jada Pinkett Smith</td>\n",
       "      <td>[]</td>\n",
       "      <td>starting point is 00:00:00 ed. yeah, man. i'm ...</td>\n",
       "      <td>[{'slug': 'ep-218-jada-pinkett-smith', 'start_...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         slug           guest_name  \\\n",
       "0                                  john-early           John Early   \n",
       "1                                  nina-conti           Nina Conti   \n",
       "2   ep-299-katherine-parkinson-live-in-london  Katherine Parkinson   \n",
       "3        ep-288-bridget-christie-tasting-menu     Bridget Christie   \n",
       "4             ep-278-john-kearns-tasting-menu          John Kearns   \n",
       "5                        ep-268-jessica-hynes        Jessica Hynes   \n",
       "6                         ep-258-phil-dunster         Phil Dunster   \n",
       "7                          ep-248-huge-davies          Huge Davies   \n",
       "8                             ep-238-katy-wix             Katy Wix   \n",
       "9                         ep-228-ray-winstone         Ray Winstone   \n",
       "10                  ep-218-jada-pinkett-smith   Jada Pinkett Smith   \n",
       "\n",
       "               restaurants_mentioned_x  \\\n",
       "0     [Princes Hot Chicken, Hattie Bs]   \n",
       "1                           [Di Palos]   \n",
       "2                                   []   \n",
       "3              [Soho Hotel Refuel Bar]   \n",
       "4                                   []   \n",
       "5             [Everest Cash and Carry]   \n",
       "6                                   []   \n",
       "7                                   []   \n",
       "8                                   []   \n",
       "9   [Clock Tower Cafe, Scotts, Smiths]   \n",
       "10                                  []   \n",
       "\n",
       "                                clean_transcript_text  \\\n",
       "0   starting point is 00:00:00 oh no, it's james a...   \n",
       "1   starting point is 00:00:00 hello, it's ed gamb...   \n",
       "2   starting point is 00:00:00 oh boy, oh boy, the...   \n",
       "3   starting point is 00:00:00 huge news from off-...   \n",
       "4   starting point is 00:00:00 welcome to the off-...   \n",
       "5   starting point is 00:00:00 we get it. life get...   \n",
       "6   starting point is 00:00:00 i am charlotte casa...   \n",
       "7   starting point is 00:00:00 welcome to the off ...   \n",
       "8   starting point is 00:00:00 hello, it's ed gamb...   \n",
       "9   starting point is 00:00:00 hello, it's ed gamb...   \n",
       "10  starting point is 00:00:00 ed. yeah, man. i'm ...   \n",
       "\n",
       "                                  periodic_timestamps  \\\n",
       "0   [{'slug': 'john-early', 'start_index': 0, 'tim...   \n",
       "1   [{'slug': 'nina-conti', 'start_index': 0, 'tim...   \n",
       "2   [{'slug': 'ep-299-katherine-parkinson-live-in-...   \n",
       "3   [{'slug': 'ep-288-bridget-christie-tasting-men...   \n",
       "4   [{'slug': 'ep-278-john-kearns-tasting-menu', '...   \n",
       "5   [{'slug': 'ep-268-jessica-hynes', 'start_index...   \n",
       "6   [{'slug': 'ep-258-phil-dunster', 'start_index'...   \n",
       "7   [{'slug': 'ep-248-huge-davies', 'start_index':...   \n",
       "8   [{'slug': 'ep-238-katy-wix', 'start_index': 0,...   \n",
       "9   [{'slug': 'ep-228-ray-winstone', 'start_index'...   \n",
       "10  [{'slug': 'ep-218-jada-pinkett-smith', 'start_...   \n",
       "\n",
       "               restaurants_mentioned_y  \n",
       "0     [Princes Hot Chicken, Hattie Bs]  \n",
       "1                           [Di Palos]  \n",
       "2                                   []  \n",
       "3              [Soho Hotel Refuel Bar]  \n",
       "4                                   []  \n",
       "5             [Everest Cash and Carry]  \n",
       "6                                   []  \n",
       "7                                   []  \n",
       "8                                   []  \n",
       "9   [Clock Tower Cafe, Scotts, Smiths]  \n",
       "10                                  []  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- REDUNDANT: combining the test timestamps and transcripts with the metadata ---\n",
    "# They were already combined by extract and save transcripts timestamps\n",
    "\n",
    "\n",
    "combined_timestamps_metadata_df = combine_timestamps_and_metadata(test_timestamps_out_path, ten_test_episodes_metadata_output_path)\n",
    "combined_timestamps_metadata_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dbaddd",
   "metadata": {},
   "source": [
    "## Fuzzywuzzy testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00816b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Functions: fuzzymatching functions: Adapted to use slug IDs ---\n",
    "\n",
    "def _create_list_tuple_clean_sen_og_sen_og_index(\n",
    "    text: str,\n",
    ") -> List[Tuple[str, str, int]]:\n",
    "    \"\"\"\n",
    "    Takes in a clean transcript string, and creates a list of tuples containing cleaned sentences\n",
    "    for fuzzymatching, original sentences and starting index for locating quotes.\n",
    "\n",
    "    Splits text using delimiter \". \". Assumes no sentences start with puntuation (leading spaces are the only shift from the start of the original to the start\n",
    "    of the cleaned sentence).\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, str, int]]: a list containing a tuple, with cleaned sentence, original\n",
    "                                    stripped sentence, and true start index (the start index of the original sentence,\n",
    "                                    in the original text).\n",
    "\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    current_idx_in_original = 0  # This tracks our position in the original 'text'\n",
    "\n",
    "    # Split into 'segments' (what will become sentences) by full stop/space.\n",
    "    segments = text.split(\". \")\n",
    "\n",
    "    for i, segment in enumerate(\n",
    "        segments\n",
    "    ):  # Note enumerate is a way to loop and get index (rather than a manual counter)\n",
    "        original_full_sentence_segment = segment\n",
    "        # Calculate the actual start index of the content within the segment itself (after stripping leading/trailing spaces)\n",
    "        # It asssumes the start index (in processes sentence) will only move due to leading spaces\n",
    "        # So, it calculates the original (assuming none start with punctuation), and retains it\n",
    "        # Later, we will use this original index to compare against timestamps\n",
    "        leading_spaces_count = len(original_full_sentence_segment) - len(\n",
    "            original_full_sentence_segment.lstrip()\n",
    "        )\n",
    "        true_start_index = current_idx_in_original + leading_spaces_count\n",
    "\n",
    "        original_sentence_stripped = original_full_sentence_segment.strip()\n",
    "\n",
    "        # Only process if the sentence is not empty after stripping\n",
    "        if original_sentence_stripped:\n",
    "            # Apply original cleaning, explicitly converting to lowercase for fuzzy matching\n",
    "            cleaned_sentence = re.sub(\n",
    "                r\"[^\\w\\s]\", \"\", original_sentence_stripped\n",
    "            ).lower()\n",
    "\n",
    "            # Store cleaned, original, and start index\n",
    "            results.append(\n",
    "                (cleaned_sentence, original_sentence_stripped, true_start_index)\n",
    "            )\n",
    "\n",
    "        # Update current_idx_in_original for the next segment.\n",
    "        # Add the length of the current segment and the delimiter length (2 for \". \").\n",
    "        # This assumes all segments (except possibly the last) were followed by \". \".\n",
    "        current_idx_in_original += len(original_full_sentence_segment)\n",
    "        if (\n",
    "            i < len(segments) - 1\n",
    "        ):  # Only add delimiter length if it's not the last segment\n",
    "            current_idx_in_original += len(\". \")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def _find_timestamp(\n",
    "    original_sentence_start_index: int, transcript_timestamps: List[dict]\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds the nearest timestamp occurring before or at a given sentence index.\n",
    "\n",
    "    This function searches through a list of timestamp dictionaries (which should\n",
    "    be pre-sorted by `start_index`) to find the timestamp that immediately\n",
    "    precedes or is at the start of a matched sentence.\n",
    "\n",
    "    Args:\n",
    "        original_sentence_start_index (int): The starting index of the sentence\n",
    "            in the full transcript string.\n",
    "        transcript_timestamps (List[dict]): A list of dictionaries, where each dict\n",
    "            contains 'start_index' and 'timestamp' for a periodic timestamp.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: The timestamp string (e.g., '00:01:23') if a match is found,\n",
    "                       otherwise returns None.\n",
    "    \"\"\"\n",
    "    if original_sentence_start_index is None:\n",
    "        return None\n",
    "    # Could sort timestamps here for good practice, but should be sorted already\n",
    "    # Reverse-iterate over timestamps to find the \"nearest before or at\"\n",
    "    for timestamp_dict in reversed(transcript_timestamps):\n",
    "        if timestamp_dict[\"start_index\"] <= original_sentence_start_index:\n",
    "            return timestamp_dict[\"timestamp\"]\n",
    "\n",
    "    return None  # If no timestamp found before the quote's starting position (all eps start \"Starting point is 00:00:00\")\n",
    "\n",
    "\n",
    "def _matches_by_res_name_from_list_of_res_names(\n",
    "    restaurant_names: List[str], searchable_sentences: List[str], min_score: int\n",
    ") -> Dict[str, List[Tuple[str, int, int]]]:\n",
    "    \"\"\"\n",
    "    Finds fuzzy matches for a list of restaurant names within a list of cleaned sentences.\n",
    "\n",
    "    This function iterates through each restaurant name and uses fuzzy matching to find\n",
    "    sentences that are a close match. Matches are filtered based on a minimum score.\n",
    "\n",
    "    Args:\n",
    "        restaurant_names (List[str]): A list of restaurant names to search for.\n",
    "        searchable_sentences (List[str]): A list of pre-cleaned sentences to search within.\n",
    "        min_score (int): The minimum fuzzy match score (from 0-100) to consider\n",
    "                         a match valid.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[Tuple[str, int, int]]]: A dictionary where:\n",
    "            - Keys are the restaurant names from `restaurant_names`.\n",
    "            - Values are a list of filtered matches for that restaurant.\n",
    "            - Each match is a tuple containing:\n",
    "                - str: The matched sentence text.\n",
    "                - int: The fuzzy matching score.\n",
    "                - int: The index of the matched sentence in the `searchable_sentences` list.\n",
    "    \"\"\"\n",
    "    filtered_matches_by_string = {}\n",
    "    for res_name in restaurant_names:\n",
    "        matches = process.extract(\n",
    "            res_name, searchable_sentences, scorer=fuzz.partial_ratio, limit=20\n",
    "        )\n",
    "\n",
    "        filtered_matches = []\n",
    "        # --- FIX: Unpack the tuple of 2 items correctly ---\n",
    "        for match_text, score in matches:\n",
    "            if score >= min_score:\n",
    "                # Find the index of the matched sentence in the original list\n",
    "                # We use a try-except block for robustness in case of unexpected data.\n",
    "                try:\n",
    "                    original_sentence_index = searchable_sentences.index(match_text)\n",
    "                    # Append all three pieces of information\n",
    "                    filtered_matches.append(\n",
    "                        (match_text, score, original_sentence_index)\n",
    "                    )\n",
    "                except ValueError:\n",
    "                    # This will happen if the match text isn't found in the list,\n",
    "                    # e.g., due to slight string differences not captured by .index()\n",
    "                    continue\n",
    "\n",
    "        filtered_matches_by_string[res_name] = filtered_matches\n",
    "\n",
    "    return filtered_matches_by_string\n",
    "\n",
    "def find_top_match_and_timestamps(\n",
    "    combined_df: pd.DataFrame, min_match_score: int = 90\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Finds fuzzy matches for restaurant mentions in episode transcripts and associates them with timestamps.\n",
    "\n",
    "    This function iterates through each episode's metadata and transcript data. For each mentioned\n",
    "    restaurant, it performs a fuzzy search within the transcript. It then returns a DataFrame\n",
    "    of the top matches and their corresponding timestamps, or notes if no match was found.\n",
    "\n",
    "    Args:\n",
    "        combined_df (pd.DataFrame): A DataFrame containing episode metadata, cleaned transcripts,\n",
    "                                    and periodic timestamps.\n",
    "        min_match_score (int): The minimum fuzzy match score (0-100) required to consider\n",
    "                               a match valid.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame where each row represents a restaurant mention. It contains\n",
    "                      the following columns:\n",
    "                          - 'slug': The episode slug e.g. ep-217-ross-noble or elle-fanning\n",
    "                          - 'Restaurant': The name of the restaurant mentioned.\n",
    "                          - 'Mention text': The original sentence where the mention was found.\n",
    "                          - 'Match Score': The fuzzy match score.\n",
    "                          - 'Match Type': The type of match (e.g., 'full, over 90' or 'No match found').\n",
    "                          - 'Timestamp': The nearest preceding timestamp for the mention.\n",
    "                          - 'Transcript sample': A short sample of the transcript text.\n",
    "    \"\"\"\n",
    "    all_mentions_collected = []\n",
    "\n",
    "    for index, combined_row in combined_df.iterrows():\n",
    "        slug = combined_row.get(\"slug\")\n",
    "        guest_name = combined_row.get(\"guest_name\")\n",
    "        clean_transcript_text = combined_row.get(\"clean_transcript_text\")\n",
    "        periodic_timestamps = combined_row.get(\"periodic_timestamps\")\n",
    "\n",
    "        restaurants_data = combined_row.get(\"restaurants_mentioned\", [])\n",
    "        transcript_sample = (\n",
    "            clean_transcript_text[:200]\n",
    "            if isinstance(clean_transcript_text, str)\n",
    "            else \"No Transcript Found\"\n",
    "        )\n",
    "\n",
    "        # Unsure what data type the res mentions are, hence need for this\n",
    "        restaurants_list = []\n",
    "        print(f\"The data type of the restaurant matches is{type(restaurants_data)}\")\n",
    "        if isinstance(restaurants_data, list):\n",
    "            restaurants_list = restaurants_data\n",
    "        elif isinstance(restaurants_data, np.ndarray) and restaurants_data.size > 0:\n",
    "            # Flatten the array and convert it to a standard Python list of strings\n",
    "            restaurants_raw_list = restaurants_data.flatten().tolist()\n",
    "            restaurants_list = [\n",
    "                name.strip().lower() for name in restaurants_raw_list if name.strip()\n",
    "            ]\n",
    "        elif isinstance(restaurants_data, str):\n",
    "            restaurants_list = [\n",
    "                name.strip() for name in restaurants_data.split(\",\") if name.strip()\n",
    "            ]\n",
    "\n",
    "        if restaurants_list:\n",
    "            episode_sentences_data = _create_list_tuple_clean_sen_og_sen_og_index(\n",
    "                clean_transcript_text\n",
    "            )\n",
    "            searchable_sentences = [\n",
    "                item[0] for item in episode_sentences_data\n",
    "            ]  # This is to select the cleaned sentence from the list of tuple\n",
    "            # of cleaned sentence, original, and true start index that create_sentence_list creates\n",
    "\n",
    "            all_matches_for_episode = _matches_by_res_name_from_list_of_res_names(\n",
    "                restaurants_list, searchable_sentences, 90\n",
    "            )\n",
    "            # --- all_matches_for_episode is a dict with key res_name and value lists of matches (matches are tuples of quote, score)\n",
    "            for (\n",
    "                restaurant_name_query,\n",
    "                match_list_for_query,\n",
    "            ) in all_matches_for_episode.items():\n",
    "                if match_list_for_query:\n",
    "                    top_match = match_list_for_query[0]\n",
    "                    # Unpack the top match's data\n",
    "                    matched_cleaned_text, score, matched_sentence_index = top_match\n",
    "                    original_sentence_data = episode_sentences_data[\n",
    "                        matched_sentence_index\n",
    "                    ]  # This takes you back to episode sentences data for the sentence index\n",
    "                    # Which is a tuple of clean sentence, original, and index of sentence within sen list\n",
    "                    original_sentence_text = original_sentence_data[\n",
    "                        1\n",
    "                    ]  # The og sentence is at index 1 in this tuple\n",
    "                    original_start_index = original_sentence_data[\n",
    "                        2\n",
    "                    ]  # The og start index is at index 2 in this tuple\n",
    "\n",
    "                    timestamp = _find_timestamp(\n",
    "                        original_start_index, periodic_timestamps\n",
    "                    )\n",
    "\n",
    "                    mention = {\n",
    "                        \"Episode ID\": slug,\n",
    "                        \"Restaurant\": restaurant_name_query,\n",
    "                        \"Mention text\": original_sentence_text,\n",
    "                        \"Match Score\": score,\n",
    "                        \"Match Type\": f\"full, over {min_match_score}\",\n",
    "                        \"Timestamp\": timestamp,\n",
    "                        \"transcript_sample\": transcript_sample,\n",
    "                    }\n",
    "                    all_mentions_collected.append(mention)\n",
    "                else:\n",
    "                    null_mention = {\n",
    "                        \"Episode ID\": slug,\n",
    "                        \"Restaurant\": restaurant_name_query,\n",
    "                        \"Mention text\": None,\n",
    "                        \"Match Score\": 0,\n",
    "                        \"Match Type\": \"No match found\",\n",
    "                        \"Timestamp\": None,\n",
    "                        \"transcript_sample\": transcript_sample,\n",
    "                    }\n",
    "                    all_mentions_collected.append(null_mention)\n",
    "        else:\n",
    "            print(\n",
    "                f\"  No raw mentions found in 'restaurants_mentioned' list for Episode {slug}. Skipping\"\n",
    "            )\n",
    "    combined_df = pd.DataFrame(all_mentions_collected)\n",
    "    return combined_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34395ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_match_and_timestamps_v2(\n",
    "    combined_df: pd.DataFrame, min_match_score: int = 90\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Finds fuzzy matches for restaurant mentions in episode transcripts and associates them with timestamps.\n",
    "\n",
    "    This function iterates through each episode's metadata and transcript data. For each mentioned\n",
    "    restaurant, it performs a fuzzy search within the transcript. It then returns a DataFrame\n",
    "    of the top matches and their corresponding timestamps, or notes if no match was found.\n",
    "\n",
    "    Args:\n",
    "        combined_df (pd.DataFrame): A DataFrame containing episode metadata, cleaned transcripts,\n",
    "                                    and periodic timestamps.\n",
    "        min_match_score (int): The minimum fuzzy match score (0-100) required to consider\n",
    "                               a match valid.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame where each row represents a restaurant mention. It contains\n",
    "                      the following columns:\n",
    "                          - 'slug': The episode slug e.g. ep-217-ross-noble or elle-fanning\n",
    "                          - 'Restaurant': The name of the restaurant mentioned.\n",
    "                          - 'Mention text': The original sentence where the mention was found.\n",
    "                          - 'Match Score': The fuzzy match score.\n",
    "                          - 'Match Type': The type of match (e.g., 'full, over 90' or 'No match found').\n",
    "                          - 'Timestamp': The nearest preceding timestamp for the mention.\n",
    "                          - 'Transcript sample': A short sample of the transcript text.\n",
    "    \"\"\"\n",
    "    all_mentions_collected = []\n",
    "\n",
    "    for index, combined_row in combined_df.iterrows():\n",
    "        slug = combined_row.get(\"slug\")\n",
    "        guest_name = combined_row.get(\"guest_name\")\n",
    "        clean_transcript_text = combined_row.get(\"clean_transcript_text\")\n",
    "        periodic_timestamps = combined_row.get(\"periodic_timestamps\")\n",
    "\n",
    "        restaurants_data = combined_row.get(\"restaurants_mentioned\", [])\n",
    "        transcript_sample = (\n",
    "            clean_transcript_text[:200]\n",
    "            if isinstance(clean_transcript_text, str)\n",
    "            else \"No Transcript Found\"\n",
    "        )\n",
    "\n",
    "        # Unsure what data type the res mentions are, hence need for this\n",
    "        restaurants_list = []\n",
    "        # print(f\"The data type of the restaurant matches is{type(restaurants_data)}\")\n",
    "        # print(f\"restaurants data (raw) : {restaurants_data}\")\n",
    "        if isinstance(restaurants_data, list):\n",
    "            restaurants_list = restaurants_data\n",
    "        elif isinstance(restaurants_data, np.ndarray) and restaurants_data.size > 0:\n",
    "            # Flatten the array and convert it to a standard Python list of strings\n",
    "            restaurants_raw_list = restaurants_data.flatten().tolist()\n",
    "            restaurants_list = [\n",
    "                name.strip().lower() for name in restaurants_raw_list if name.strip()\n",
    "            ]\n",
    "            # print(f\"\\n restauratns list (processed): {restaurants_list}\")\n",
    "        elif isinstance(restaurants_data, str):\n",
    "            restaurants_list = [\n",
    "                name.strip() for name in restaurants_data.split(\",\") if name.strip()\n",
    "            ]\n",
    "\n",
    "        if restaurants_list:\n",
    "            episode_sentences_data = _create_list_tuple_clean_sen_og_sen_og_index(\n",
    "                clean_transcript_text\n",
    "            )\n",
    "            searchable_sentences = [\n",
    "                item[0] for item in episode_sentences_data\n",
    "            ]  # This is to select the cleaned sentence from the list of tuple\n",
    "            # of cleaned sentence, original, and true start index that create_sentence_list creates\n",
    "\n",
    "            all_matches_for_episode = _matches_by_res_name_from_list_of_res_names(\n",
    "                restaurants_list, searchable_sentences, 90\n",
    "            )\n",
    "            # --- all_matches_for_episode is a dict with key res_name and value lists of matches (matches r tuples of quote, score)\n",
    "            for (\n",
    "                restaurant_name_query,\n",
    "                match_list_for_query,\n",
    "            ) in all_matches_for_episode.items():\n",
    "                if match_list_for_query:\n",
    "                    top_match = match_list_for_query[0]\n",
    "                    # Unpack the top match's data\n",
    "                    matched_cleaned_text, score, matched_sentence_index = top_match\n",
    "                    original_sentence_data = episode_sentences_data[\n",
    "                        matched_sentence_index\n",
    "                    ]  # This takes you back to episode sentences data for the sentence index\n",
    "                    # Which is a tuple of clean sentence, original, and index of sentence within sen list\n",
    "                    original_sentence_text = original_sentence_data[\n",
    "                        1\n",
    "                    ]  # The og sentence is at index 1 in this tuple\n",
    "                    original_start_index = original_sentence_data[\n",
    "                        2\n",
    "                    ]  # The og start index is at index 2 in this tuple\n",
    "\n",
    "                    timestamp = _find_timestamp(\n",
    "                        original_start_index, periodic_timestamps\n",
    "                    )\n",
    "\n",
    "                    mention = {\n",
    "                        \"Episode ID\": slug,\n",
    "                        \"Restaurant\": restaurant_name_query,\n",
    "                        \"Mention text\": original_sentence_text,\n",
    "                        \"Match Score\": score,\n",
    "                        \"Match Type\": f\"full, over {min_match_score}\",\n",
    "                        \"Timestamp\": timestamp,\n",
    "                        \"transcript_sample\": transcript_sample,\n",
    "                    }\n",
    "                    all_mentions_collected.append(mention)\n",
    "                else:\n",
    "                    null_mention = {\n",
    "                        \"Episode ID\": slug,\n",
    "                        \"Restaurant\": restaurant_name_query,\n",
    "                        \"Mention text\": None,\n",
    "                        \"Match Score\": 0,\n",
    "                        \"Match Type\": \"No match found\",\n",
    "                        \"Timestamp\": None,\n",
    "                        \"transcript_sample\": transcript_sample,\n",
    "                    }\n",
    "                    all_mentions_collected.append(null_mention)\n",
    "        else:\n",
    "            print(\n",
    "                f\"  No raw mentions found in 'restaurants_mentioned' list for Episode {slug}. Skipping\"\n",
    "            )\n",
    "    combined_df = pd.DataFrame(all_mentions_collected)\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "288d840e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data type of the restaurant matches is<class 'numpy.ndarray'>\n",
      "restaurants data (raw) : ['Princes Hot Chicken' 'Hattie Bs']\n",
      "\n",
      " restaruatns list (processed): ['princes hot chicken', 'hattie bs']\n",
      "The data type of the restaurant matches is<class 'numpy.ndarray'>\n",
      "restaurants data (raw) : ['Di Palos']\n",
      "\n",
      " restaruatns list (processed): ['di palos']\n",
      "The data type of the restaurant matches is<class 'numpy.ndarray'>\n",
      "restaurants data (raw) : []\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-299-katherine-parkinson-live-in-london. Skipping\n",
      "The data type of the restaurant matches is<class 'numpy.ndarray'>\n",
      "restaurants data (raw) : ['Soho Hotel Refuel Bar']\n",
      "\n",
      " restaruatns list (processed): ['soho hotel refuel bar']\n",
      "The data type of the restaurant matches is<class 'numpy.ndarray'>\n",
      "restaurants data (raw) : []\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-278-john-kearns-tasting-menu. Skipping\n",
      "The data type of the restaurant matches is<class 'numpy.ndarray'>\n",
      "restaurants data (raw) : ['Everest Cash and Carry']\n",
      "\n",
      " restaruatns list (processed): ['everest cash and carry']\n",
      "The data type of the restaurant matches is<class 'numpy.ndarray'>\n",
      "restaurants data (raw) : []\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-258-phil-dunster. Skipping\n",
      "The data type of the restaurant matches is<class 'numpy.ndarray'>\n",
      "restaurants data (raw) : []\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-248-huge-davies. Skipping\n",
      "The data type of the restaurant matches is<class 'numpy.ndarray'>\n",
      "restaurants data (raw) : []\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-238-katy-wix. Skipping\n",
      "The data type of the restaurant matches is<class 'numpy.ndarray'>\n",
      "restaurants data (raw) : ['Clock Tower Cafe' 'Scotts' 'Smiths']\n",
      "\n",
      " restaruatns list (processed): ['clock tower cafe', 'scotts', 'smiths']\n",
      "The data type of the restaurant matches is<class 'numpy.ndarray'>\n",
      "restaurants data (raw) : []\n",
      "  No raw mentions found in 'restaurants_mentioned' list for Episode ep-218-jada-pinkett-smith. Skipping\n",
      "\n",
      "--- TOP COLLECTED ---\n",
      "Top Mentions DataFrame created with 8 rows.\n"
     ]
    }
   ],
   "source": [
    "# --- Test: fuzzymatching on first 10 eps metadata and mentions (first_ten_metadata_timestamps_df) ---\n",
    "# --- Metadata includes slug, guest_name, restaurants_mentioned\n",
    "\n",
    "# --- Run top matches on the test data ---\n",
    "top_mentions_df = find_top_match_and_timestamps_v2(first_ten_metadata_timestamps_df , 90)\n",
    "\n",
    "# --- Convert list into dataframe, print output ---\n",
    "\n",
    "print(f\"\\n--- TOP COLLECTED ---\")\n",
    "print(f\"Top Mentions DataFrame created with {len(top_mentions_df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4dbe56b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Episode ID</th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Mention text</th>\n",
       "      <th>Match Score</th>\n",
       "      <th>Match Type</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>transcript_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>john-early</td>\n",
       "      <td>princes hot chicken</td>\n",
       "      <td>you can go to prince's hot chicken? it's not n...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:58:14</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>john-early</td>\n",
       "      <td>hattie bs</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>No match found</td>\n",
       "      <td>None</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nina-conti</td>\n",
       "      <td>di palos</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>No match found</td>\n",
       "      <td>None</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ep-288-bridget-christie-tasting-menu</td>\n",
       "      <td>soho hotel refuel bar</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>No match found</td>\n",
       "      <td>None</td>\n",
       "      <td>starting point is 00:00:00 huge news from off-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ep-268-jessica-hynes</td>\n",
       "      <td>everest cash and carry</td>\n",
       "      <td>starting point is 00:19:42 there's a great, th...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:19:42</td>\n",
       "      <td>starting point is 00:00:00 we get it. life get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ep-228-ray-winstone</td>\n",
       "      <td>clock tower cafe</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>No match found</td>\n",
       "      <td>None</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ep-228-ray-winstone</td>\n",
       "      <td>scotts</td>\n",
       "      <td>but my favourite, favourite fish restaurant th...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:57:41</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ep-228-ray-winstone</td>\n",
       "      <td>smiths</td>\n",
       "      <td>in fact, once going back about ten years ago i...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:26:40</td>\n",
       "      <td>starting point is 00:00:00 hello, it's ed gamb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Episode ID              Restaurant  \\\n",
       "0                            john-early     princes hot chicken   \n",
       "1                            john-early               hattie bs   \n",
       "2                            nina-conti                di palos   \n",
       "3  ep-288-bridget-christie-tasting-menu   soho hotel refuel bar   \n",
       "4                  ep-268-jessica-hynes  everest cash and carry   \n",
       "5                   ep-228-ray-winstone        clock tower cafe   \n",
       "6                   ep-228-ray-winstone                  scotts   \n",
       "7                   ep-228-ray-winstone                  smiths   \n",
       "\n",
       "                                        Mention text  Match Score  \\\n",
       "0  you can go to prince's hot chicken? it's not n...          100   \n",
       "1                                               None            0   \n",
       "2                                               None            0   \n",
       "3                                               None            0   \n",
       "4  starting point is 00:19:42 there's a great, th...          100   \n",
       "5                                               None            0   \n",
       "6  but my favourite, favourite fish restaurant th...          100   \n",
       "7  in fact, once going back about ten years ago i...          100   \n",
       "\n",
       "       Match Type Timestamp                                  transcript_sample  \n",
       "0   full, over 90  00:58:14  starting point is 00:00:00 oh no, it's james a...  \n",
       "1  No match found      None  starting point is 00:00:00 oh no, it's james a...  \n",
       "2  No match found      None  starting point is 00:00:00 hello, it's ed gamb...  \n",
       "3  No match found      None  starting point is 00:00:00 huge news from off-...  \n",
       "4   full, over 90  00:19:42  starting point is 00:00:00 we get it. life get...  \n",
       "5  No match found      None  starting point is 00:00:00 hello, it's ed gamb...  \n",
       "6   full, over 90  00:57:41  starting point is 00:00:00 hello, it's ed gamb...  \n",
       "7   full, over 90  00:26:40  starting point is 00:00:00 hello, it's ed gamb...  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_mentions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0773fa",
   "metadata": {},
   "source": [
    "## Helpler to match old transcripts and avoid redownloading - tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ad34213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "\n",
    "\n",
    "def assimilate_existing_transcripts(\n",
    "    out_dir: Path,\n",
    "    url_map: Dict[str, str],\n",
    "    status: Dict[str, Dict],\n",
    "    legacy_dir: Optional[Path] = None,\n",
    "    rename_to_slug: bool = True,\n",
    "    overwrite: bool = False,\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Find legacy files named like 'ep_1.html' or 'ep-1.html' in out_dir (and optionally legacy_dir),\n",
    "    map them to url_map keys (slugs that contain 'ep-<num>' or '<num>'), update the status dict and\n",
    "    optionally rename/move them to the slug-based filename.\n",
    "\n",
    "    Args:\n",
    "        out_dir: Path where new slug files should live.\n",
    "        url_map: mapping slug -> url (used to find matching slug for an ep number).\n",
    "        status: status dict to update in-place (returned for convenience).\n",
    "        legacy_dir: optional extra directory to check for files (if your old files live elsewhere).\n",
    "        rename_to_slug: if True, move/rename legacy file to new slug filename (safe move).\n",
    "        overwrite: if True, allow overwriting existing slug files (be careful).\n",
    "\n",
    "    Returns:\n",
    "        Updated status dict (mutated in-place).\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    candidates = []\n",
    "\n",
    "    _LEGACY_EP_RE = re.compile(r\"ep[_\\-]?(\\d+)\\.html$\", flags=re.IGNORECASE)\n",
    "\n",
    "    # collect files to examine from out_dir\n",
    "    for p in out_dir.glob(\"*.html\"):\n",
    "        candidates.append(p)\n",
    "\n",
    "    # also check legacy_dir if provided\n",
    "    if legacy_dir:\n",
    "        legacy_dir = Path(legacy_dir)\n",
    "        if legacy_dir.exists():\n",
    "            for p in legacy_dir.glob(\"*.html\"):\n",
    "                # avoid double-adding files that are already in out_dir (same path)\n",
    "                if p.resolve() not in [c.resolve() for c in candidates]:\n",
    "                    candidates.append(p)\n",
    "\n",
    "    # build reverse map: number_str -> list of slugs that contain that number token\n",
    "    # e.g. '1' -> ['ep-1-john-doe', 'ep-1-other']\n",
    "    num_to_slugs = {}\n",
    "    for slug in url_map.keys():\n",
    "        # find first number token like ep-<num> or ep<num>\n",
    "        m = re.search(r\"ep[-_]?(?P<num>\\d+)\", slug, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            num = m.group(\"num\")\n",
    "            num_to_slugs.setdefault(num, []).append(slug)\n",
    "        else:\n",
    "            # also consider bare numbers anywhere (e.g. 'episode-23-guest')\n",
    "            m2 = re.search(r\"(?<!\\d)(\\d+)(?!\\d)\", slug)\n",
    "            if m2:\n",
    "                num = m2.group(1)\n",
    "                num_to_slugs.setdefault(num, []).append(slug)\n",
    "\n",
    "    summary = {\"found\": 0, \"mapped\": 0, \"renamed\": 0, \"skipped\": 0}\n",
    "\n",
    "    for p in candidates:\n",
    "        name = p.name\n",
    "        m = _LEGACY_EP_RE.match(name)\n",
    "        if not m:\n",
    "            # not a legacy ep_N file; ignore\n",
    "            continue\n",
    "        summary[\"found\"] += 1\n",
    "        epnum = m.group(1)\n",
    "\n",
    "        # find candidate slugs for this ep number\n",
    "        candidates_for_num = num_to_slugs.get(epnum, [])\n",
    "\n",
    "        if not candidates_for_num:\n",
    "            # no matching slug for the number — skip for now\n",
    "            logger.debug(\"Found legacy file %s but no slug contains ep-%s; skipping\", name, epnum)\n",
    "            summary[\"skipped\"] += 1\n",
    "            continue\n",
    "\n",
    "        # If multiple slugs match one number, prefer exact 'ep-<num>' prefix match\n",
    "        chosen_slug = None\n",
    "        for s in candidates_for_num:\n",
    "            if re.match(fr\"^ep[-_]?{epnum}(\\b|-|$)\", s, flags=re.IGNORECASE):\n",
    "                chosen_slug = s\n",
    "                break\n",
    "        if chosen_slug is None:\n",
    "            chosen_slug = candidates_for_num[0]\n",
    "\n",
    "        # Build destination path for slug file\n",
    "        safe_slug = _sanitize_key(chosen_slug)\n",
    "        dest = out_dir / f\"{safe_slug}.html\"\n",
    "\n",
    "        # If dest already exists and is same file, just update status\n",
    "        try:\n",
    "            if dest.exists() and dest.resolve() == p.resolve():\n",
    "                logger.debug(\"Legacy file %s already at desired location %s\", p, dest)\n",
    "            elif dest.exists() and not overwrite:\n",
    "                # dest already exists (someone downloaded or moved it earlier) -> skip rename but update status to point to dest\n",
    "                logger.info(\"Destination %s exists; skipping move of %s (overwrite=False)\", dest, p)\n",
    "                summary[\"skipped\"] += 1\n",
    "            else:\n",
    "                if rename_to_slug:\n",
    "                    # move (or copy+unlink) legacy file to dest in a safe manner\n",
    "                    logger.info(\"Renaming/moving legacy file %s -> %s\", p, dest)\n",
    "                    # ensure parent exists\n",
    "                    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    # use shutil.move to preserve contents; if same filesystem this is cheap\n",
    "                    shutil.move(str(p), str(dest))\n",
    "                    summary[\"renamed\"] += 1\n",
    "                else:\n",
    "                    # don't move but use p as saved_path\n",
    "                    dest = p\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Failed to move/inspect legacy file %s: %s\", p, e)\n",
    "            summary[\"skipped\"] += 1\n",
    "            continue\n",
    "\n",
    "        # Update status entry for chosen slug\n",
    "        meta = status.setdefault(chosen_slug, {\"url\": url_map.get(chosen_slug), \"attempts\": 0, \"status\": \"pending\", \"saved_path\": None, \"last_error\": None})\n",
    "        meta.update({\n",
    "            \"attempts\": max(meta.get(\"attempts\", 0), 1),\n",
    "            \"status\": \"success\",\n",
    "            \"saved_path\": str(dest),\n",
    "            \"last_error\": None,\n",
    "        })\n",
    "        logger.info(\"Associated legacy file %s -> slug=%s saved_path=%s\", name, chosen_slug, dest)\n",
    "        summary[\"mapped\"] += 1\n",
    "\n",
    "    logger.info(\"Assimilation summary: %s\", summary)\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "76ac5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_transcripts_legacy(\n",
    "    url_map: Dict[str, str],\n",
    "    out_dir: str,\n",
    "    status_path: str,\n",
    "    max_attempts_per_url: int = 5,\n",
    "    backoff_base: float = 1.0,\n",
    "    max_workers: int = 3,\n",
    "    session: Optional[requests.Session] = None,\n",
    "    timeout: float = 12.0,\n",
    "    legacy_dir = None\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Download URLs to out_dir using url_map (keys are slugs used as filenames).\n",
    "    Added logging provides visibility into what the function does on each run.\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    status_path = Path(status_path)\n",
    "\n",
    "    logger.info(\"Starting download_transcripts: %d urls, out_dir=%s, status_path=%s\",\n",
    "                len(url_map), out_dir, status_path)\n",
    "\n",
    "    # Load existing status if present (allows resume)\n",
    "    if status_path.exists():\n",
    "        try:\n",
    "            with open(status_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                status = json.load(f)\n",
    "            logger.debug(\"Loaded existing status.json with %d entries\", len(status))\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Failed to load status.json (%s). Starting with empty status.\", e)\n",
    "            status = {}\n",
    "    else:\n",
    "        logger.debug(\"No status.json file found at %s. Starting fresh.\", status_path)\n",
    "        status = {}\n",
    "\n",
    "    # Initialize status entries for any missing keys (log each new init)\n",
    "    for key, url in url_map.items():\n",
    "        if key not in status:\n",
    "            status[key] = {\n",
    "                \"url\": url,\n",
    "                \"attempts\": 0,\n",
    "                \"status\": \"pending\",  # pending | success | failed\n",
    "                \"saved_path\": None,\n",
    "                \"last_error\": None,\n",
    "            }\n",
    "            logger.debug(\"Initialized status for key='%s' -> %s\", key, url)\n",
    "\n",
    "    # try assimilating legacy files in out_dir and a legacy folder (if you have one)\n",
    "    if legacy_dir:\n",
    "        status = assimilate_existing_transcripts(out_dir=out_dir, url_map=url_map, status=status, legacy_dir=legacy_dir, rename_to_slug=True, overwrite=False)\n",
    "        # persist immediately so the status file reflects these existing files\n",
    "        with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(status, f, indent=2)\n",
    "        logger.debug(\"Persisted status.json after assimilating legacy transcripts.\")\n",
    "\n",
    "    # Use a single session for pooling\n",
    "    session = session or requests.Session()\n",
    "\n",
    "    def _attempt_download(key: str, meta: Dict) -> Dict:\n",
    "        url = meta[\"url\"]\n",
    "        attempts = meta[\"attempts\"]\n",
    "        result = dict(meta)\n",
    "\n",
    "        # If already succeeded, skip and log reason\n",
    "        if meta.get(\"status\") == \"success\":\n",
    "            logger.debug(\"Skipping key='%s' (already success, saved_path=%s)\", key, meta.get(\"saved_path\"))\n",
    "            return result\n",
    "\n",
    "        # If max attempts reached, log and skip\n",
    "        if attempts >= max_attempts_per_url:\n",
    "            result[\"status\"] = \"failed\"\n",
    "            result[\"last_error\"] = \"max_attempts_reached\"\n",
    "            logger.info(\"Key='%s' reached max attempts (%d). Marking failed.\", key, attempts)\n",
    "            return result\n",
    "\n",
    "        # Log the attempt about to be made\n",
    "        logger.debug(\"Attempting key='%s' (attempt %d) -> %s\", key, attempts + 1, url)\n",
    "        try:\n",
    "            headers = _choose_headers()\n",
    "            resp = session.get(url, headers=headers, timeout=timeout)\n",
    "\n",
    "            # If success (200)\n",
    "            if resp.status_code == 200:\n",
    "                safe_key = _sanitize_key(key)\n",
    "                filename = f\"{safe_key}.html\"\n",
    "                saved_path = str(out_dir / filename)\n",
    "\n",
    "                # If file already exists, log that we're overwriting (helps debug)\n",
    "                if Path(saved_path).exists():\n",
    "                    logger.debug(\"File %s already exists and will be overwritten by key='%s'\", saved_path, key)\n",
    "\n",
    "                with open(saved_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "                    fh.write(resp.text)\n",
    "\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"success\",\n",
    "                    \"saved_path\": saved_path,\n",
    "                    \"last_error\": None,\n",
    "                })\n",
    "                logger.info(\"Saved %s -> %s (key=%s)\", url, saved_path, key)\n",
    "                return result\n",
    "\n",
    "            # Retryable status codes\n",
    "            if resp.status_code in (429, 500, 502, 503, 504):\n",
    "                # Parse Retry-After header if present and include in result\n",
    "                retry_after_raw = resp.headers.get(\"Retry-After\")\n",
    "                retry_after_seconds = _parse_retry_after(retry_after_raw)\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"pending\",\n",
    "                    \"last_error\": f\"status_{resp.status_code}\",\n",
    "                    \"retry_after_seconds\": retry_after_seconds,\n",
    "                })\n",
    "                logger.warning(\"Retryable HTTP %s for key='%s' url=%s (attempt %s)\",\n",
    "                               resp.status_code, key, url, attempts + 1)\n",
    "                # Log headers optionally for 429 to see Retry-After\n",
    "                if resp.status_code == 429:\n",
    "                    logger.debug(\"429 response headers for key='%s': Retry-After=%s\", key, retry_after_raw)\n",
    "                    logger.debug(\"Parsed Retry-After seconds for key='%s': %s\", key, retry_after_seconds)\n",
    "                return result\n",
    "\n",
    "            # Non-retryable\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"failed\",\n",
    "                \"last_error\": f\"status_{resp.status_code}\"\n",
    "            })\n",
    "            logger.error(\"Non-retryable HTTP %s for key='%s' url=%s\", resp.status_code, key, url)\n",
    "            return result\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            # Network error: retryable\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"pending\",\n",
    "                \"last_error\": repr(e)\n",
    "            })\n",
    "            logger.warning(\"RequestException for key='%s' url=%s (attempt %s): %s\", key, url, attempts + 1, e)\n",
    "            return result\n",
    "\n",
    "    # Worker wrapper with backoff\n",
    "    def _worker_task(key):\n",
    "        meta = status[key]\n",
    "        if meta.get(\"status\") == \"success\" or meta.get(\"attempts\", 0) >= max_attempts_per_url:\n",
    "            return key, meta\n",
    "\n",
    "        new_meta = _attempt_download(key, meta)\n",
    "\n",
    "        if new_meta[\"status\"] == \"pending\":\n",
    "            # computed exponential backoff (what we would do)\n",
    "            comp_sleep = backoff_base * (2 ** (new_meta[\"attempts\"] - 1))\n",
    "            jitter = random.uniform(0, 1.0)\n",
    "            computed_sleep = comp_sleep + jitter\n",
    "\n",
    "            # server-provided advice (if any)\n",
    "            retry_after = new_meta.get(\"retry_after_seconds\")\n",
    "            if retry_after is not None:\n",
    "                # use the server's suggestion if it's longer than our computed wait\n",
    "                sleep_time = max(computed_sleep, float(retry_after))\n",
    "            else:\n",
    "                sleep_time = computed_sleep\n",
    "\n",
    "            # cap to avoid runaway sleeps (adjust cap as desired)\n",
    "            sleep_time = min(sleep_time, 600.0)\n",
    "\n",
    "            logger.info(\"Backing off %0.2fs for key='%s' (attempt %s) [computed=%0.2fs, server=%s]\",\n",
    "                        sleep_time, key, new_meta[\"attempts\"], computed_sleep, retry_after)\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        return key, new_meta\n",
    "\n",
    "    # Main loop\n",
    "    pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "    round_idx = 0\n",
    "    while pending_keys:\n",
    "        round_idx += 1\n",
    "        logger.info(\"Download round %d: %d pending\", round_idx, len(pending_keys))\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futures = {ex.submit(_worker_task, key): key for key in pending_keys}\n",
    "            for fut in as_completed(futures):\n",
    "                key = futures[fut]\n",
    "                try:\n",
    "                    k, new_meta = fut.result()\n",
    "                    status[k].update(new_meta)\n",
    "                except Exception as e:\n",
    "                    logger.exception(\"Unhandled exception for key %s: %s\", key, e)\n",
    "                    status[key][\"attempts\"] = status[key].get(\"attempts\", 0) + 1\n",
    "                    status[key][\"last_error\"] = repr(e)\n",
    "\n",
    "        # persist status to disk after every round\n",
    "        try:\n",
    "            with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(status, f, indent=2)\n",
    "            logger.debug(\"Persisted status.json (round %d).\", round_idx)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Failed to write status file: %s\", e)\n",
    "\n",
    "        # Prepare next round\n",
    "        pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "\n",
    "        if pending_keys:\n",
    "            logger.info(\"Sleeping 2s between rounds to be polite...\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    # final persist and summary\n",
    "    with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(status, f, indent=2)\n",
    "\n",
    "    # Final summary counts\n",
    "    succ = sum(1 for v in status.values() if v.get(\"status\") == \"success\")\n",
    "    failed = sum(1 for v in status.values() if v.get(\"status\") == \"failed\")\n",
    "    pending = sum(1 for v in status.values() if v.get(\"status\") == \"pending\")\n",
    "    logger.info(\"Download finished. success=%d failed=%d pending=%d\", succ, failed, pending)\n",
    "\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7972c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrate_scraper_legacy(\n",
    "    df,                     # DataFrame with 'slug' and optionally 'url'\n",
    "    base_url,               # base URL for constructing URLs if df has no 'url' column\n",
    "    out_dir,                # folder to save HTML transcripts\n",
    "    max_attempts_per_url=5,\n",
    "    backoff_base=1.0,\n",
    "    max_workers=3,\n",
    "    timeout=12.0,\n",
    "    legacy_dir = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Orchestrates the scraping process:\n",
    "      1. Prepares a slug → URL map\n",
    "      2. Ensures output folder exists\n",
    "      3. Calls download_transcripts() with sensible defaults\n",
    "      4. Returns the status dict for all downloads\n",
    "\n",
    "    Args:\n",
    "        df: dataframe (not filepath) with slugs and urls in (also raw titles, guest names)\n",
    "        base_url: The base url for the transcripts from podscripts.com\n",
    "        out_dir: The folder to save the transcripts to\n",
    "        max_attempts_per_url\n",
    "        backoff_base\n",
    "        max_workers\n",
    "        timeout\n",
    "    \"\"\"\n",
    "    # ---------------------\n",
    "    # Setup logger for this run\n",
    "    # ---------------------\n",
    "    logger = configure_logger()\n",
    "    logger.info(\"Starting scraper orchestration for %d episodes\", len(df))\n",
    "\n",
    "    # ---------------------\n",
    "    # Prepare URL map\n",
    "    # ---------------------\n",
    "    if \"url\" in df.columns:\n",
    "        url_map = {row[\"slug\"]: row[\"url\"] for _, row in df.iterrows()}\n",
    "        logger.info(\"Using existing URLs from DataFrame\")\n",
    "    else:\n",
    "        url_map = {row[\"slug\"]: base_url.rstrip(\"/\") + \"/\" + row[\"slug\"].lstrip(\"/\") for _, row in df.iterrows()}\n",
    "        logger.info(\"Constructed URLs from base_url and slugs\")\n",
    "\n",
    "    # ---------------------\n",
    "    # Ensure output folder exists\n",
    "    # ---------------------\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    status_path = out_dir / \"status.json\"\n",
    "\n",
    "    # ---------------------\n",
    "    # Call the scraper\n",
    "    # ---------------------\n",
    "    logger.info(\"Running download_transcripts with %d URLs\", len(url_map))\n",
    "    status = download_transcripts_legacy(\n",
    "        url_map=url_map,\n",
    "        out_dir=out_dir,\n",
    "        status_path=status_path,\n",
    "        max_attempts_per_url=max_attempts_per_url,\n",
    "        backoff_base=backoff_base,\n",
    "        max_workers=max_workers,\n",
    "        timeout=timeout,\n",
    "        legacy_dir = legacy_dir\n",
    "    )\n",
    "\n",
    "    logger.info(\"Scraper orchestration finished\")\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "18935a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-2\n",
      "-3\n",
      "-4\n",
      "-5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guest_name</th>\n",
       "      <th>url</th>\n",
       "      <th>slug</th>\n",
       "      <th>restaurants_mentioned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>Scroobius Pip</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-1-scroobius-pip</td>\n",
       "      <td>[Oli Babas Kerb Camden]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Grace Dent</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-2-grace-dent</td>\n",
       "      <td>[Little Owl, Trullo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>Richard Osman</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-3-richard-osman</td>\n",
       "      <td>[Five Guys, Cora Pearl, Berners Tavern]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Nish Kumar</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-4-nish-kumar</td>\n",
       "      <td>[Bademiya, The Owl and The Pussycat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Aisling Bea</td>\n",
       "      <td>https://podscripts.co/podcasts/off-menu-with-e...</td>\n",
       "      <td>ep-5-aisling-bea</td>\n",
       "      <td>[Cafe Gratitude, Burger and Lobster]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        guest_name                                                url  \\\n",
       "321  Scroobius Pip  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "320     Grace Dent  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "319  Richard Osman  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "318     Nish Kumar  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "317    Aisling Bea  https://podscripts.co/podcasts/off-menu-with-e...   \n",
       "\n",
       "                   slug                    restaurants_mentioned  \n",
       "321  ep-1-scroobius-pip                  [Oli Babas Kerb Camden]  \n",
       "320     ep-2-grace-dent                     [Little Owl, Trullo]  \n",
       "319  ep-3-richard-osman  [Five Guys, Cora Pearl, Berners Tavern]  \n",
       "318     ep-4-nish-kumar     [Bademiya, The Owl and The Pussycat]  \n",
       "317    ep-5-aisling-bea     [Cafe Gratitude, Burger and Lobster]  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the test data (episodes 1-5)\n",
    "\n",
    "indices_to_slice_3 = range(-1, -6, -1)\n",
    "\n",
    "for num in indices_to_slice_3:\n",
    "    print(num)\n",
    "\n",
    "first_five_eps_test_metadata_for_legacy_matcher = full_episodes_metadata_test_df.iloc[indices_to_slice_3]\n",
    "first_five_eps_test_metadata_for_legacy_matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "72d4ff28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 10:54:55,134 INFO: Starting scraper orchestration for 5 episodes\n",
      "2025-12-01 10:54:55,134 INFO: Using existing URLs from DataFrame\n",
      "2025-12-01 10:54:55,137 INFO: Running download_transcripts with 5 URLs\n",
      "2025-12-01 10:54:55,137 INFO: Starting download_transcripts: 5 urls, out_dir=c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp, status_path=c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\\data\\test_temp\\status.json\n",
      "2025-12-01 10:54:55,158 DEBUG: Loaded existing status.json with 16 entries\n",
      "2025-12-01 10:54:55,212 INFO: Assimilation summary: {'found': 0, 'mapped': 0, 'renamed': 0, 'skipped': 0}\n",
      "2025-12-01 10:54:55,212 DEBUG: Persisted status.json after assimilating legacy transcripts.\n",
      "2025-12-01 10:54:55,221 INFO: Download finished. success=16 failed=0 pending=0\n",
      "2025-12-01 10:54:55,221 INFO: Scraper orchestration finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ep-217-ross-noble-christmas-special': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-217-ross-noble-christmas-special',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'C:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-217-ross-noble-christmas-special.html',\n",
       "  'last_error': None},\n",
       " 'ep-207-nick-frost': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-207-nick-frost',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'C:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-207-nick-frost.html',\n",
       "  'last_error': None},\n",
       " 'ep-197-jenny-eclair': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-197-jenny-eclair',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'C:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-197-jenny-eclair.html',\n",
       "  'last_error': None},\n",
       " 'ep-187-lily-allen': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-187-lily-allen',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'C:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-187-lily-allen.html',\n",
       "  'last_error': None},\n",
       " 'ep-178-fern-brady': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-178-fern-brady',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'C:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-178-fern-brady.html',\n",
       "  'last_error': None},\n",
       " 'ep-169-ania-magliano': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-169-ania-magliano',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'C:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-169-ania-magliano.html',\n",
       "  'last_error': None},\n",
       " 'ep-159-felicity-ward': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-159-felicity-ward',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'C:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-159-felicity-ward.html',\n",
       "  'last_error': None},\n",
       " 'ep-149-adam-buxton': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-149-adam-buxton',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'C:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-149-adam-buxton.html',\n",
       "  'last_error': None},\n",
       " 'ep-139-nadiya-hussain': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-139-nadiya-hussain',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'C:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-139-nadiya-hussain.html',\n",
       "  'last_error': None},\n",
       " 'ep-129-jason-reitman': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-129-jason-reitman',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'C:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-129-jason-reitman.html',\n",
       "  'last_error': None},\n",
       " 'ep-119-jamie-oliver': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-119-jamie-oliver',\n",
       "  'attempts': 2,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'C:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-119-jamie-oliver.html',\n",
       "  'last_error': None,\n",
       "  'retry_after_seconds': 56.0},\n",
       " 'ep-1-scroobius-pip': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-1-scroobius-pip',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'c:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-1-scroobius-pip.html',\n",
       "  'last_error': None},\n",
       " 'ep-2-grace-dent': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-2-grace-dent',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'c:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-2-grace-dent.html',\n",
       "  'last_error': None},\n",
       " 'ep-3-richard-osman': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-3-richard-osman',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'c:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-3-richard-osman.html',\n",
       "  'last_error': None},\n",
       " 'ep-4-nish-kumar': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-4-nish-kumar',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'c:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-4-nish-kumar.html',\n",
       "  'last_error': None},\n",
       " 'ep-5-aisling-bea': {'url': 'https://podscripts.co/podcasts/off-menu-with-ed-gamble-and-james-acaster/ep-5-aisling-bea',\n",
       "  'attempts': 1,\n",
       "  'status': 'success',\n",
       "  'saved_path': 'c:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project\\\\data\\\\test_temp\\\\ep-5-aisling-bea.html',\n",
       "  'last_error': None}}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test legacy matching scraper\n",
    "\n",
    "\n",
    "orchestrate_scraper_legacy(\n",
    "    first_five_eps_test_metadata_for_legacy_matcher,\n",
    "    transcript_base_url,\n",
    "    Test_data_dir,\n",
    "    5,\n",
    "    1,\n",
    "    2,\n",
    "    12,\n",
    "    Test_data_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df962720",
   "metadata": {},
   "source": [
    "## Pre legacy integration scraper version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "85dfeb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to set up logger for new web scraper\n",
    "def configure_logger(log_file: Optional[str] = None, level: int = logging.DEBUG):\n",
    "    \"\"\"\n",
    "    Configure a compact logger for the scraper.\n",
    "    - Console handler always enabled.\n",
    "    - Optional file handler if log_file provided.\n",
    "    - Default level: DEBUG for maximum visibility while testing.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"scraper\")\n",
    "    logger.setLevel(level)\n",
    "\n",
    "    # Avoid adding handlers multiple times when running multiple times in a notebook\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    # Console handler (clear, one-line format)\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(level)\n",
    "    ch.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s: %(message)s\"))\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    # Optional file handler (rotating not necessary here — keep simple)\n",
    "    if log_file:\n",
    "        fh = logging.FileHandler(log_file, encoding=\"utf-8\")\n",
    "        fh.setLevel(level)\n",
    "        fh.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s: %(message)s\"))\n",
    "        logger.addHandler(fh)\n",
    "\n",
    "    return logger\n",
    "\n",
    "# small sanitize helper (same as before)\n",
    "def _sanitize_key(key: str) -> str:\n",
    "    if not isinstance(key, str):\n",
    "        key = str(key)\n",
    "    s = key.strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\-_]+\", \"-\", s)\n",
    "    s = re.sub(r\"-{2,}\", \"-\", s)\n",
    "    return s.strip(\"-_\")\n",
    "\n",
    "# ---- Helper: random-ish UA list (small) ----\n",
    "_SIMPLE_USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "]\n",
    "\n",
    "def _choose_headers():\n",
    "    return {\"User-Agent\": random.choice(_SIMPLE_USER_AGENTS)}\n",
    "\n",
    "# ----- Helper to access retry limits from the server (for use in scraper)\n",
    "def _parse_retry_after(header_value: Optional[str]) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Parse Retry-After header. It can be:\n",
    "      - an integer number of seconds, e.g. \"120\"\n",
    "      - a HTTP-date string, e.g. \"Wed, 21 Oct 2015 07:28:00 GMT\"\n",
    "    Return number of seconds to wait (float), or None if not parseable.\n",
    "    \"\"\"\n",
    "    if not header_value:\n",
    "        return None\n",
    "    header_value = header_value.strip()\n",
    "    # try integer seconds\n",
    "    if header_value.isdigit():\n",
    "        try:\n",
    "            return float(header_value)\n",
    "        except Exception:\n",
    "            return None\n",
    "    # try HTTP-date\n",
    "    try:\n",
    "        dt = parsedate_to_datetime(header_value)\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        now = datetime.now(timezone.utc)\n",
    "        delta = (dt - now).total_seconds()\n",
    "        return max(0.0, float(delta))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "315dd2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_transcripts(\n",
    "    url_map: Dict[str, str],\n",
    "    out_dir: str,\n",
    "    status_path: str,\n",
    "    max_attempts_per_url: int = 5,\n",
    "    backoff_base: float = 1.0,\n",
    "    max_workers: int = 3,\n",
    "    session: Optional[requests.Session] = None,\n",
    "    timeout: float = 12.0,\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Download URLs to out_dir using url_map (keys are slugs used as filenames).\n",
    "    Added logging provides visibility into what the function does on each run.\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    status_path = Path(status_path)\n",
    "\n",
    "    logger.info(\"Starting download_transcripts: %d urls, out_dir=%s, status_path=%s\",\n",
    "                len(url_map), out_dir, status_path)\n",
    "\n",
    "    # Load existing status if present (allows resume)\n",
    "    if status_path.exists():\n",
    "        try:\n",
    "            with open(status_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                status = json.load(f)\n",
    "            logger.debug(\"Loaded existing status.json with %d entries\", len(status))\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Failed to load status.json (%s). Starting with empty status.\", e)\n",
    "            status = {}\n",
    "    else:\n",
    "        logger.debug(\"No status.json file found at %s. Starting fresh.\", status_path)\n",
    "        status = {}\n",
    "\n",
    "    # Initialize status entries for any missing keys (log each new init)\n",
    "    for key, url in url_map.items():\n",
    "        if key not in status:\n",
    "            status[key] = {\n",
    "                \"url\": url,\n",
    "                \"attempts\": 0,\n",
    "                \"status\": \"pending\",  # pending | success | failed\n",
    "                \"saved_path\": None,\n",
    "                \"last_error\": None,\n",
    "            }\n",
    "            logger.debug(\"Initialized status for key='%s' -> %s\", key, url)\n",
    "\n",
    "    # Use a single session for pooling\n",
    "    session = session or requests.Session()\n",
    "\n",
    "    def _attempt_download(key: str, meta: Dict) -> Dict:\n",
    "        url = meta[\"url\"]\n",
    "        attempts = meta[\"attempts\"]\n",
    "        result = dict(meta)\n",
    "\n",
    "        # If already succeeded, skip and log reason\n",
    "        if meta.get(\"status\") == \"success\":\n",
    "            logger.debug(\"Skipping key='%s' (already success, saved_path=%s)\", key, meta.get(\"saved_path\"))\n",
    "            return result\n",
    "\n",
    "        # If max attempts reached, log and skip\n",
    "        if attempts >= max_attempts_per_url:\n",
    "            result[\"status\"] = \"failed\"\n",
    "            result[\"last_error\"] = \"max_attempts_reached\"\n",
    "            logger.info(\"Key='%s' reached max attempts (%d). Marking failed.\", key, attempts)\n",
    "            return result\n",
    "\n",
    "        # Log the attempt about to be made\n",
    "        logger.debug(\"Attempting key='%s' (attempt %d) -> %s\", key, attempts + 1, url)\n",
    "        try:\n",
    "            headers = _choose_headers()\n",
    "            resp = session.get(url, headers=headers, timeout=timeout)\n",
    "\n",
    "            # If success (200)\n",
    "            if resp.status_code == 200:\n",
    "                safe_key = _sanitize_key(key)\n",
    "                filename = f\"{safe_key}.html\"\n",
    "                saved_path = str(out_dir / filename)\n",
    "\n",
    "                # If file already exists, log that we're overwriting (helps debug)\n",
    "                if Path(saved_path).exists():\n",
    "                    logger.debug(\"File %s already exists and will be overwritten by key='%s'\", saved_path, key)\n",
    "\n",
    "                with open(saved_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "                    fh.write(resp.text)\n",
    "\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"success\",\n",
    "                    \"saved_path\": saved_path,\n",
    "                    \"last_error\": None,\n",
    "                })\n",
    "                logger.info(\"Saved %s -> %s (key=%s)\", url, saved_path, key)\n",
    "                return result\n",
    "\n",
    "            # Retryable status codes\n",
    "            if resp.status_code in (429, 500, 502, 503, 504):\n",
    "                # Parse Retry-After header if present and include in result\n",
    "                retry_after_raw = resp.headers.get(\"Retry-After\")\n",
    "                retry_after_seconds = _parse_retry_after(retry_after_raw)\n",
    "                result.update({\n",
    "                    \"attempts\": attempts + 1,\n",
    "                    \"status\": \"pending\",\n",
    "                    \"last_error\": f\"status_{resp.status_code}\",\n",
    "                    \"retry_after_seconds\": retry_after_seconds,\n",
    "                })\n",
    "                logger.warning(\"Retryable HTTP %s for key='%s' url=%s (attempt %s)\",\n",
    "                               resp.status_code, key, url, attempts + 1)\n",
    "                # Log headers optionally for 429 to see Retry-After\n",
    "                if resp.status_code == 429:\n",
    "                    logger.debug(\"429 response headers for key='%s': Retry-After=%s\", key, retry_after_raw)\n",
    "                    logger.debug(\"Parsed Retry-After seconds for key='%s': %s\", key, retry_after_seconds)\n",
    "                return result\n",
    "\n",
    "            # Non-retryable\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"failed\",\n",
    "                \"last_error\": f\"status_{resp.status_code}\"\n",
    "            })\n",
    "            logger.error(\"Non-retryable HTTP %s for key='%s' url=%s\", resp.status_code, key, url)\n",
    "            return result\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            # Network error: retryable\n",
    "            result.update({\n",
    "                \"attempts\": attempts + 1,\n",
    "                \"status\": \"pending\",\n",
    "                \"last_error\": repr(e)\n",
    "            })\n",
    "            logger.warning(\"RequestException for key='%s' url=%s (attempt %s): %s\", key, url, attempts + 1, e)\n",
    "            return result\n",
    "\n",
    "    # Worker wrapper with backoff\n",
    "    def _worker_task(key):\n",
    "        meta = status[key]\n",
    "        if meta.get(\"status\") == \"success\" or meta.get(\"attempts\", 0) >= max_attempts_per_url:\n",
    "            return key, meta\n",
    "\n",
    "        new_meta = _attempt_download(key, meta)\n",
    "\n",
    "        if new_meta[\"status\"] == \"pending\":\n",
    "            # computed exponential backoff (what we would do)\n",
    "            comp_sleep = backoff_base * (2 ** (new_meta[\"attempts\"] - 1))\n",
    "            jitter = random.uniform(0, 1.0)\n",
    "            computed_sleep = comp_sleep + jitter\n",
    "\n",
    "            # server-provided advice (if any)\n",
    "            retry_after = new_meta.get(\"retry_after_seconds\")\n",
    "            if retry_after is not None:\n",
    "                # use the server's suggestion if it's longer than our computed wait\n",
    "                sleep_time = max(computed_sleep, float(retry_after))\n",
    "            else:\n",
    "                sleep_time = computed_sleep\n",
    "\n",
    "            # cap to avoid runaway sleeps (adjust cap as desired)\n",
    "            sleep_time = min(sleep_time, 600.0)\n",
    "\n",
    "            logger.info(\"Backing off %0.2fs for key='%s' (attempt %s) [computed=%0.2fs, server=%s]\",\n",
    "                        sleep_time, key, new_meta[\"attempts\"], computed_sleep, retry_after)\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        return key, new_meta\n",
    "\n",
    "    # Main loop\n",
    "    pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "    round_idx = 0\n",
    "    while pending_keys:\n",
    "        round_idx += 1\n",
    "        logger.info(\"Download round %d: %d pending\", round_idx, len(pending_keys))\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futures = {ex.submit(_worker_task, key): key for key in pending_keys}\n",
    "            for fut in as_completed(futures):\n",
    "                key = futures[fut]\n",
    "                try:\n",
    "                    k, new_meta = fut.result()\n",
    "                    status[k].update(new_meta)\n",
    "                except Exception as e:\n",
    "                    logger.exception(\"Unhandled exception for key %s: %s\", key, e)\n",
    "                    status[key][\"attempts\"] = status[key].get(\"attempts\", 0) + 1\n",
    "                    status[key][\"last_error\"] = repr(e)\n",
    "\n",
    "        # persist status to disk after every round\n",
    "        try:\n",
    "            with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(status, f, indent=2)\n",
    "            logger.debug(\"Persisted status.json (round %d).\", round_idx)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Failed to write status file: %s\", e)\n",
    "\n",
    "        # Prepare next round\n",
    "        pending_keys = [k for k, v in status.items() if v[\"status\"] != \"success\" and v[\"attempts\"] < max_attempts_per_url]\n",
    "\n",
    "        if pending_keys:\n",
    "            logger.info(\"Sleeping 2s between rounds to be polite...\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    # final persist and summary\n",
    "    with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(status, f, indent=2)\n",
    "\n",
    "    # Final summary counts\n",
    "    succ = sum(1 for v in status.values() if v.get(\"status\") == \"success\")\n",
    "    failed = sum(1 for v in status.values() if v.get(\"status\") == \"failed\")\n",
    "    pending = sum(1 for v in status.values() if v.get(\"status\") == \"pending\")\n",
    "    logger.info(\"Download finished. success=%d failed=%d pending=%d\", succ, failed, pending)\n",
    "\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "82b0f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orchestration function for the scraper, to scrape transcripts\n",
    "# Replaces extract_and_save_transcripts_html\n",
    "def orchestrate_scraper(\n",
    "    df,                     # DataFrame with 'slug' and optionally 'url'\n",
    "    base_url,               # base URL for constructing URLs if df has no 'url' column\n",
    "    out_dir,                # folder to save HTML transcripts\n",
    "    max_attempts_per_url=5,\n",
    "    backoff_base=1.0,\n",
    "    max_workers=3,\n",
    "    timeout=12.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Orchestrates the scraping process:\n",
    "      1. Prepares a slug → URL map\n",
    "      2. Ensures output folder exists\n",
    "      3. Calls download_transcripts() with sensible defaults\n",
    "      4. Returns the status dict for all downloads\n",
    "\n",
    "    Args:\n",
    "        df: dataframe (not filepath) with slugs and urls in (also raw titles, guest names)\n",
    "        base_url: The base url for the transcripts from podscripts.com\n",
    "        out_dir: The folder to save the transcripts to\n",
    "        max_attempts_per_url\n",
    "        backoff_base\n",
    "        max_workers\n",
    "        timeout\n",
    "    \"\"\"\n",
    "    # ---------------------\n",
    "    # Setup logger for this run\n",
    "    # ---------------------\n",
    "    logger = configure_logger()\n",
    "    logger.info(\"Starting scraper orchestration for %d episodes\", len(df))\n",
    "\n",
    "    # ---------------------\n",
    "    # Prepare URL map\n",
    "    # ---------------------\n",
    "    if \"url\" in df.columns:\n",
    "        url_map = {row[\"slug\"]: row[\"url\"] for _, row in df.iterrows()}\n",
    "        logger.info(\"Using existing URLs from DataFrame\")\n",
    "    else:\n",
    "        url_map = {row[\"slug\"]: base_url.rstrip(\"/\") + \"/\" + row[\"slug\"].lstrip(\"/\") for _, row in df.iterrows()}\n",
    "        logger.info(\"Constructed URLs from base_url and slugs\")\n",
    "\n",
    "    # ---------------------\n",
    "    # Ensure output folder exists\n",
    "    # ---------------------\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    status_path = out_dir / \"status.json\"\n",
    "\n",
    "    # ---------------------\n",
    "    # Call the scraper\n",
    "    # ---------------------\n",
    "    logger.info(\"Running download_transcripts with %d URLs\", len(url_map))\n",
    "    status = download_transcripts(\n",
    "        url_map=url_map,\n",
    "        out_dir=out_dir,\n",
    "        status_path=status_path,\n",
    "        max_attempts_per_url=max_attempts_per_url,\n",
    "        backoff_base=backoff_base,\n",
    "        max_workers=max_workers,\n",
    "        timeout=timeout\n",
    "    )\n",
    "\n",
    "    logger.info(\"Scraper orchestration finished\")\n",
    "    return status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "eab33a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Episode ID</th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Mention text</th>\n",
       "      <th>Match Score</th>\n",
       "      <th>Match Type</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>transcript_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>john-early</td>\n",
       "      <td>princes hot chicken</td>\n",
       "      <td>you can go to prince's hot chicken? it's not n...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:58:14</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>john-early</td>\n",
       "      <td>hattie bs</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>No match found</td>\n",
       "      <td>None</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kunal-nayyar</td>\n",
       "      <td>moti mahal</td>\n",
       "      <td>ah</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>01:00:27</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kunal-nayyar</td>\n",
       "      <td>the tamil prince</td>\n",
       "      <td>there's a pub, an indian pub called the tamil ...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:32:33</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kunal-nayyar</td>\n",
       "      <td>the dover</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>No match found</td>\n",
       "      <td>None</td>\n",
       "      <td>starting point is 00:00:00 oh no, it's james a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>ep-3-richard-osman</td>\n",
       "      <td>cora pearl</td>\n",
       "      <td>this was a difficult question for me until lit...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:13:00</td>\n",
       "      <td>starting point is 00:00:00 hello, listeners of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>ep-3-richard-osman</td>\n",
       "      <td>berners tavern</td>\n",
       "      <td>there's a restaurant near here called the burn...</td>\n",
       "      <td>93</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:38:58</td>\n",
       "      <td>starting point is 00:00:00 hello, listeners of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>ep-2-grace-dent</td>\n",
       "      <td>little owl</td>\n",
       "      <td>it would be, the side dish would be from littl...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:34:52</td>\n",
       "      <td>starting point is 00:00:00 hello, listeners of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>ep-2-grace-dent</td>\n",
       "      <td>trullo</td>\n",
       "      <td>and it's the beef shin ragu with probably it's...</td>\n",
       "      <td>100</td>\n",
       "      <td>full, over 90</td>\n",
       "      <td>00:20:26</td>\n",
       "      <td>starting point is 00:00:00 hello, listeners of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>ep-1-scroobius-pip</td>\n",
       "      <td>oli babas kerb camden</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>No match found</td>\n",
       "      <td>None</td>\n",
       "      <td>starting point is 00:00:00 hello, listeners of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>519 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Episode ID             Restaurant  \\\n",
       "0            john-early    princes hot chicken   \n",
       "1            john-early              hattie bs   \n",
       "2          kunal-nayyar             moti mahal   \n",
       "3          kunal-nayyar       the tamil prince   \n",
       "4          kunal-nayyar              the dover   \n",
       "..                  ...                    ...   \n",
       "514  ep-3-richard-osman             cora pearl   \n",
       "515  ep-3-richard-osman         berners tavern   \n",
       "516     ep-2-grace-dent             little owl   \n",
       "517     ep-2-grace-dent                 trullo   \n",
       "518  ep-1-scroobius-pip  oli babas kerb camden   \n",
       "\n",
       "                                          Mention text  Match Score  \\\n",
       "0    you can go to prince's hot chicken? it's not n...          100   \n",
       "1                                                 None            0   \n",
       "2                                                   ah          100   \n",
       "3    there's a pub, an indian pub called the tamil ...          100   \n",
       "4                                                 None            0   \n",
       "..                                                 ...          ...   \n",
       "514  this was a difficult question for me until lit...          100   \n",
       "515  there's a restaurant near here called the burn...           93   \n",
       "516  it would be, the side dish would be from littl...          100   \n",
       "517  and it's the beef shin ragu with probably it's...          100   \n",
       "518                                               None            0   \n",
       "\n",
       "         Match Type Timestamp  \\\n",
       "0     full, over 90  00:58:14   \n",
       "1    No match found      None   \n",
       "2     full, over 90  01:00:27   \n",
       "3     full, over 90  00:32:33   \n",
       "4    No match found      None   \n",
       "..              ...       ...   \n",
       "514   full, over 90  00:13:00   \n",
       "515   full, over 90  00:38:58   \n",
       "516   full, over 90  00:34:52   \n",
       "517   full, over 90  00:20:26   \n",
       "518  No match found      None   \n",
       "\n",
       "                                     transcript_sample  \n",
       "0    starting point is 00:00:00 oh no, it's james a...  \n",
       "1    starting point is 00:00:00 oh no, it's james a...  \n",
       "2    starting point is 00:00:00 oh no, it's james a...  \n",
       "3    starting point is 00:00:00 oh no, it's james a...  \n",
       "4    starting point is 00:00:00 oh no, it's james a...  \n",
       "..                                                 ...  \n",
       "514  starting point is 00:00:00 hello, listeners of...  \n",
       "515  starting point is 00:00:00 hello, listeners of...  \n",
       "516  starting point is 00:00:00 hello, listeners of...  \n",
       "517  starting point is 00:00:00 hello, listeners of...  \n",
       "518  starting point is 00:00:00 hello, listeners of...  \n",
       "\n",
       "[519 rows x 7 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easy_wins_inspection_path = os.path.join(PROCESSED_DATA_DIR, \"easy_win_mention_search_df.parquet\")\n",
    "\n",
    "easy_wins_df = try_read_parquet(easy_wins_inspection_path)\n",
    "\n",
    "easy_wins_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Off Menu Project Venv",
   "language": "python",
   "name": "off_menu_project_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
