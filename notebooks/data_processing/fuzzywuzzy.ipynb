{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c306d348",
   "metadata": {},
   "source": [
    "# Off menu fuzzy matching\n",
    "\n",
    "This notebook serves as a development environment for the logic to identify mentions of the restaurants in the transcript, using fuzzywuzzy library. The final production code is located in off_menu/data_processing.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc2a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to sys.path: c:\\Users\\jbara\\Data science projects (store here not desktop on onedrive)\\Off Menu project\n",
      "Current sys.path: ['c:\\\\Users\\\\jbara\\\\Data science projects (store here not desktop on onedrive)\\\\Off Menu project', 'C:\\\\Users\\\\jbara\\\\miniconda3\\\\python312.zip', 'C:\\\\Users\\\\jbara\\\\miniconda3\\\\DLLs', 'C:\\\\Users\\\\jbara\\\\miniconda3\\\\Lib', 'C:\\\\Users\\\\jbara\\\\miniconda3', 'c:\\\\Users\\\\jbara\\\\OneDrive\\\\Desktop\\\\Data_science\\\\Python projects\\\\Off Menu project\\\\.venv', '', 'c:\\\\Users\\\\jbara\\\\OneDrive\\\\Desktop\\\\Data_science\\\\Python projects\\\\Off Menu project\\\\.venv\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\jbara\\\\OneDrive\\\\Desktop\\\\Data_science\\\\Python projects\\\\Off Menu project\\\\.venv\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\jbara\\\\OneDrive\\\\Desktop\\\\Data_science\\\\Python projects\\\\Off Menu project\\\\.venv\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\jbara\\\\OneDrive\\\\Desktop\\\\Data_science\\\\Python projects\\\\Off Menu project\\\\.venv\\\\Lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "# Ensure imports can find my utils:\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..', '..'))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"Project root added to sys.path: {project_root}\")\n",
    "print(f\"Current sys.path: {sys.path}\")\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "import re \n",
    "from typing import List, Tuple, Dict # For type hinting the new return type#\n",
    "import numpy as np\n",
    "from fuzzywuzzy import process\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Project specific imports\n",
    "from off_menu.utils import clean_text\n",
    "from off_menu.utils import num_check\n",
    "from off_menu.utils import find_num_end\n",
    "from off_menu.utils import name_num_split\n",
    "from off_menu.utils import clean_res\n",
    "from off_menu.utils import get_episode_sentences\n",
    "from off_menu.utils import create_sentence_list\n",
    "from off_menu.utils import try_read_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f461a4a",
   "metadata": {},
   "source": [
    "## Configuration and filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f6dadbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_temp_dir = os.path.join(project_root, 'data', 'test_temp')\n",
    "first_five_path = os.path.join(test_temp_dir, 'test_clean_text_and__timestamps_df.parquet') # First five transcripts and timestamps\n",
    "ep_metadata_head_filepath = os.path.join(test_temp_dir, 'ep_meta_and_mentions_head.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3099c490",
   "metadata": {},
   "source": [
    "## Read test data (transcripts/timestamps, epsiode metadat), confirm structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2dcba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------First five df:------\n",
      "   episode_number     guest_name  \\\n",
      "0               1  Scroobius Pip   \n",
      "1               2     Grace Dent   \n",
      "2               3  Richard Osman   \n",
      "3               4     Nish Kumar   \n",
      "4               5    Aisling Bea   \n",
      "\n",
      "                               clean_transcript_text  \\\n",
      "0  starting point is 00:00:00 hello, listeners of...   \n",
      "1  starting point is 00:00:00 hello, listeners of...   \n",
      "2  starting point is 00:00:00 hello, listeners of...   \n",
      "3                                                      \n",
      "4  starting point is 00:00:00 hello, listeners of...   \n",
      "\n",
      "                                 periodic_timestamps  \n",
      "0  [{'episode_number': 1, 'start_index': 0, 'time...  \n",
      "1  [{'episode_number': 2, 'start_index': 0, 'time...  \n",
      "2  [{'episode_number': 3, 'start_index': 0, 'time...  \n",
      "3                                                 []  \n",
      "4  [{'episode_number': 5, 'start_index': 0, 'time...  \n",
      "\n",
      "------ep and mentions df head------\n",
      "   episode_number     guest_name  \\\n",
      "0               1  Scroobius Pip   \n",
      "1               2     Grace Dent   \n",
      "2               3  Richard Osman   \n",
      "3               4     Nish Kumar   \n",
      "4               5    Aisling Bea   \n",
      "\n",
      "                                                 url  \\\n",
      "0  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "1  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "2  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "3  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "4  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "\n",
      "                     restaurants_mentioned  \n",
      "0                  [Oli Babas Kerb Camden]  \n",
      "1                     [Little Owl, Trullo]  \n",
      "2  [Five Guys, Cora Pearl, Berners Tavern]  \n",
      "3     [Bademiya, The Owl and The Pussycat]  \n",
      "4     [Cafe Gratitude, Burger and Lobster]  \n"
     ]
    }
   ],
   "source": [
    "# Reading first five transcripts and timestamps\n",
    "\n",
    "first_five_clean_transcript_timestamps_df = try_read_parquet(first_five_path)\n",
    "\n",
    "# Reading full episode metadata\n",
    "ep_meta_and_mentions_head = try_read_parquet(ep_metadata_head_filepath)\n",
    "\n",
    "# Confirm structure of test data\n",
    "print(\"------First five df:------\")\n",
    "print(first_five_clean_transcript_timestamps_df.head())\n",
    "print(\"\\n------ep and mentions df head------\")\n",
    "print(ep_meta_and_mentions_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314fb68f",
   "metadata": {},
   "source": [
    "## Refactoring fuzzymatching and timestamp algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c91dabd",
   "metadata": {},
   "source": [
    "### Merge dataframes, develop function to merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c98ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata_df:    episode_number     guest_name  \\\n",
      "0               1  Scroobius Pip   \n",
      "1               2     Grace Dent   \n",
      "2               3  Richard Osman   \n",
      "3               4     Nish Kumar   \n",
      "4               5    Aisling Bea   \n",
      "\n",
      "                                                 url  \\\n",
      "0  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "1  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "2  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "3  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "4  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "\n",
      "                     restaurants_mentioned  \n",
      "0                  [Oli Babas Kerb Camden]  \n",
      "1                     [Little Owl, Trullo]  \n",
      "2  [Five Guys, Cora Pearl, Berners Tavern]  \n",
      "3     [Bademiya, The Owl and The Pussycat]  \n",
      "4     [Cafe Gratitude, Burger and Lobster]  \n",
      "transcripts timestamps df:    episode_number     guest_name  \\\n",
      "0               1  Scroobius Pip   \n",
      "1               2     Grace Dent   \n",
      "2               3  Richard Osman   \n",
      "3               4     Nish Kumar   \n",
      "4               5    Aisling Bea   \n",
      "\n",
      "                               clean_transcript_text  \\\n",
      "0  starting point is 00:00:00 hello, listeners of...   \n",
      "1  starting point is 00:00:00 hello, listeners of...   \n",
      "2  starting point is 00:00:00 hello, listeners of...   \n",
      "3                                                      \n",
      "4  starting point is 00:00:00 hello, listeners of...   \n",
      "\n",
      "                                 periodic_timestamps  \n",
      "0  [{'episode_number': 1, 'start_index': 0, 'time...  \n",
      "1  [{'episode_number': 2, 'start_index': 0, 'time...  \n",
      "2  [{'episode_number': 3, 'start_index': 0, 'time...  \n",
      "3                                                 []  \n",
      "4  [{'episode_number': 5, 'start_index': 0, 'time...  \n",
      "   episode_number     guest_name  \\\n",
      "0               1  Scroobius Pip   \n",
      "1               2     Grace Dent   \n",
      "2               3  Richard Osman   \n",
      "3               4     Nish Kumar   \n",
      "4               5    Aisling Bea   \n",
      "\n",
      "                               clean_transcript_text  \\\n",
      "0  starting point is 00:00:00 hello, listeners of...   \n",
      "1  starting point is 00:00:00 hello, listeners of...   \n",
      "2  starting point is 00:00:00 hello, listeners of...   \n",
      "3                                                      \n",
      "4  starting point is 00:00:00 hello, listeners of...   \n",
      "\n",
      "                                 periodic_timestamps  \\\n",
      "0  [{'episode_number': 1, 'start_index': 0, 'time...   \n",
      "1  [{'episode_number': 2, 'start_index': 0, 'time...   \n",
      "2  [{'episode_number': 3, 'start_index': 0, 'time...   \n",
      "3                                                 []   \n",
      "4  [{'episode_number': 5, 'start_index': 0, 'time...   \n",
      "\n",
      "                     restaurants_mentioned  \n",
      "0                  [Oli Babas Kerb Camden]  \n",
      "1                     [Little Owl, Trullo]  \n",
      "2  [Five Guys, Cora Pearl, Berners Tavern]  \n",
      "3     [Bademiya, The Owl and The Pussycat]  \n",
      "4     [Cafe Gratitude, Burger and Lobster]  \n"
     ]
    }
   ],
   "source": [
    "# Note selecting only episode number and res mentions from ep_and_mentions to be merged\n",
    "combined_test_df = first_five_clean_transcript_timestamps_df.merge(\n",
    "        ep_meta_and_mentions_head[['episode_number', 'restaurants_mentioned']],\n",
    "        on='episode_number',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "def combine_metadata_timestamps(transcripts_timestamps_filepath, metadata_filepath):\n",
    "    metadata_df = try_read_parquet(metadata_filepath)\n",
    "    transcripts_timestamps_df = try_read_parquet(transcripts_timestamps_filepath)\n",
    "    print(f\"metadata_df: {metadata_df}\")\n",
    "    print(f\"transcripts timestamps df: {transcripts_timestamps_df}\")\n",
    "    combined_df = transcripts_timestamps_df.merge(\n",
    "            metadata_df[['episode_number', 'restaurants_mentioned']],\n",
    "            on='episode_number',\n",
    "            how='left'\n",
    "        )\n",
    "    return combined_df\n",
    "\n",
    "combined_test_df = combine_metadata_timestamps(first_five_path, ep_metadata_head_filepath)\n",
    "print(combined_test_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b0f465",
   "metadata": {},
   "source": [
    "## Function to turn transcript string into list of clean sentences for search using RegEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70661b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Episode 1 (Scroobius Pip) ---\n",
      "  Transcript length: 73155 characters\n",
      "  Number of periodic timestamps: 170\n",
      "  Found 1 raw mentions for this episode:\n",
      "Here they are: ['oli babas kerb camden']\n",
      "DEBUG: Query 'oli babas kerb camden'\n",
      "No specific mentions processed for Episode 1.\n",
      "\n",
      "--- Processing Episode 2 (Grace Dent) ---\n",
      "  Transcript length: 57238 characters\n",
      "  Number of periodic timestamps: 156\n",
      "  Found 2 raw mentions for this episode:\n",
      "Here they are: ['little owl', 'trullo']\n",
      "DEBUG: Query 'little owl'\n",
      "DEBUG: Query 'trullo'\n",
      "Mentions collected for Episode 2: 2 rows.\n",
      "\n",
      "--- Processing Episode 3 (Richard Osman) ---\n",
      "  Transcript length: 87390 characters\n",
      "  Number of periodic timestamps: 280\n",
      "  Found 3 raw mentions for this episode:\n",
      "Here they are: ['five guys', 'cora pearl', 'berners tavern']\n",
      "DEBUG: Query 'five guys'\n",
      "DEBUG: Query 'cora pearl'\n",
      "DEBUG: Query 'berners tavern'\n",
      "Mentions collected for Episode 3: 9 rows.\n",
      "\n",
      "--- Processing Episode 4 (Nish Kumar) ---\n",
      "  Transcript length: 0 characters\n",
      "  Number of periodic timestamps: 0\n",
      "  Found 2 raw mentions for this episode:\n",
      "Here they are: ['bademiya', 'the owl and the pussycat']\n",
      "DEBUG: Query 'bademiya'\n",
      "DEBUG: Query 'the owl and the pussycat'\n",
      "No specific mentions processed for Episode 4.\n",
      "\n",
      "--- Processing Episode 5 (Aisling Bea) ---\n",
      "  Transcript length: 77801 characters\n",
      "  Number of periodic timestamps: 211\n",
      "  Found 2 raw mentions for this episode:\n",
      "Here they are: ['cafe gratitude', 'burger and lobster']\n",
      "DEBUG: Query 'cafe gratitude'\n",
      "DEBUG: Query 'burger and lobster'\n",
      "Mentions collected for Episode 5: 11 rows.\n",
      "\n",
      "--- ALL MENTIONS COLLECTED ---\n",
      "Total Mentions DataFrame created with 22 rows.\n",
      "\n",
      " --- Here is the full dataframe ---\n",
      "    Episode ID          Restaurant  \\\n",
      "0            2          little owl   \n",
      "1            2              trullo   \n",
      "2            3           five guys   \n",
      "3            3           five guys   \n",
      "4            3           five guys   \n",
      "5            3           five guys   \n",
      "6            3          cora pearl   \n",
      "7            3          cora pearl   \n",
      "8            3          cora pearl   \n",
      "9            3      berners tavern   \n",
      "10           3      berners tavern   \n",
      "11           5      cafe gratitude   \n",
      "12           5      cafe gratitude   \n",
      "13           5  burger and lobster   \n",
      "14           5  burger and lobster   \n",
      "15           5  burger and lobster   \n",
      "16           5  burger and lobster   \n",
      "17           5  burger and lobster   \n",
      "18           5  burger and lobster   \n",
      "19           5  burger and lobster   \n",
      "20           5  burger and lobster   \n",
      "21           5  burger and lobster   \n",
      "\n",
      "                                         Mention text  Match Score  \\\n",
      "0   it would be, the side dish would be from littl...          100   \n",
      "1   and it's the beef shin ragu with probably it's...          100   \n",
      "2   what if every single thing that i'm going to m...          100   \n",
      "3   they don't really do find the starters at five...          100   \n",
      "4   have you ever eaten the peanuts in five guys? ...          100   \n",
      "5   starting point is 00:06:00 and if you do say a...          100   \n",
      "6   this was a difficult question for me until lit...          100   \n",
      "7   because at cora pearl, they bring you out this...          100   \n",
      "8   however lovely this cora pearl was and it was,...          100   \n",
      "9   there's a restaurant near here called the burn...           93   \n",
      "10         so you can afford to go to burner's tavern           93   \n",
      "11  do you know actually, if there's any vegans ou...          100   \n",
      "12  so cafe gratitude is this notorious, not notor...          100   \n",
      "13  and they were like, have you ever been to burg...          100   \n",
      "14           and so we all went to burger and lobster          100   \n",
      "15           and i tell them about burger and lobster          100   \n",
      "16  so i would have the thai calamari from the sab...          100   \n",
      "17  your main from burger and lobster comes with f...          100   \n",
      "18                                              and..          100   \n",
      "19  you would like lobster from burger and lobster...          100   \n",
      "20  also lobster from burger and lobster if it cou...          100   \n",
      "21  yeah, well, it was close to the thai calamari ...          100   \n",
      "\n",
      "       Match Type Timestamp  \n",
      "0   full, over 90  00:34:52  \n",
      "1   full, over 90  00:20:26  \n",
      "2   full, over 90  00:03:42  \n",
      "3   full, over 90  00:03:42  \n",
      "4   full, over 90  00:04:06  \n",
      "5   full, over 90  00:06:00  \n",
      "6   full, over 90  00:13:00  \n",
      "7   full, over 90  00:13:33  \n",
      "8   full, over 90  00:14:39  \n",
      "9   full, over 90  00:38:58  \n",
      "10  full, over 90  00:39:27  \n",
      "11  full, over 90  00:22:02  \n",
      "12  full, over 90  00:22:02  \n",
      "13  full, over 90  00:35:52  \n",
      "14  full, over 90  00:36:24  \n",
      "15  full, over 90  00:37:46  \n",
      "16  full, over 90  00:44:51  \n",
      "17  full, over 90  00:45:09  \n",
      "18  full, over 90  00:49:13  \n",
      "19  full, over 90  01:03:57  \n",
      "20  full, over 90  01:04:13  \n",
      "21  full, over 90  01:05:37  \n"
     ]
    }
   ],
   "source": [
    "# We must clean the sentence list using regex to improve matching, so we have to also store the 'true' (original) start index of each sentence.\n",
    "# So we can use it to locate nearest prior timestamp later.\n",
    "\n",
    "def _create_list_tuple_clean_sen_og_sen_og_index(text: str) -> List[Tuple[str, str, int]]:\n",
    "    \"\"\"\n",
    "    Takes in a string (designed for clean transcript). Outputs a list containing a tuple, with cleaned sentence, original \n",
    "    stripped sentence, and true start index (the start index of the original sentence, in the original text). \n",
    "\n",
    "    If a fw match is found within the sentence, we can then use this \"true\" index to find nearest prior timestamp from the\n",
    "    cleaned transcript text.\n",
    "    \n",
    "    Splits text using delimiter \". \". \n",
    "\n",
    "    Assumes no sentences start with puntuation (leading spaces are the only shift from the start of the original to the start\n",
    "    of the cleaned sentence). \n",
    "    \"\"\"\n",
    "    results = []\n",
    "    current_idx_in_original = 0 # This tracks our position in the original 'text'\n",
    "\n",
    "    # Split into 'segments' (what will become sentences) by full stop/space. \n",
    "    segments = text.split(\". \")\n",
    "\n",
    "    for i, segment in enumerate(segments): # Note enumerate is a way to loop and get index (rather than a manual counter)\n",
    "        original_full_sentence_segment = segment \n",
    "        # Calculate the actual start index of the content within the segment itself (after stripping leading/trailing spaces)\n",
    "        # It asssumes the start index (in processes sentence) will only move due to leading spaces\n",
    "        # So, it calculates the original (assuming none start with punctuation), and retains it\n",
    "        # Later, we will use this original index to compare against timestamps\n",
    "        leading_spaces_count = len(original_full_sentence_segment) - len(original_full_sentence_segment.lstrip())\n",
    "        true_start_index = current_idx_in_original + leading_spaces_count\n",
    "\n",
    "        original_sentence_stripped = original_full_sentence_segment.strip() \n",
    "\n",
    "        # Only process if the sentence is not empty after stripping\n",
    "        if original_sentence_stripped: \n",
    "            # Apply original cleaning, explicitly converting to lowercase for fuzzy matching\n",
    "            cleaned_sentence = re.sub(r\"[^\\w\\s]\", \"\", original_sentence_stripped).lower()\n",
    "            \n",
    "            # Store cleaned, original, and start index\n",
    "            results.append((cleaned_sentence, original_sentence_stripped, true_start_index))\n",
    "\n",
    "        # Update current_idx_in_original for the next segment.\n",
    "        # Add the length of the current segment and the delimiter length (2 for \". \").\n",
    "        # This assumes all segments (except possibly the last) were followed by \". \".\n",
    "        current_idx_in_original += len(original_full_sentence_segment)\n",
    "        if i < len(segments) - 1: # Only add delimiter length if it's not the last segment\n",
    "            current_idx_in_original += len(\". \") \n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16439e72",
   "metadata": {},
   "source": [
    "### Fuzzy matching function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4bea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _matches_by_res_name_from_list_of_res_names(restaurant_names: List[str], searchable_sentences: List[str], min_score: int) -> Dict[str, List[Tuple[str, int, int]]]:\n",
    "    \"\"\"\n",
    "    Finds fuzzy matches for a list of restaurant names, in a list of cleaned sentences.\n",
    "    Returns a dict, where each key is a res name, and values are a list of filtered matches.\n",
    "    Each match is a tuple containing match text, score, and the original index from the searchable_sentences list.\n",
    "    \"\"\"\n",
    "    filtered_matches_by_string = {}\n",
    "    for res_name in restaurant_names:\n",
    "        matches = process.extract(\n",
    "            res_name,\n",
    "            searchable_sentences, \n",
    "            scorer=fuzz.partial_ratio,\n",
    "            limit=20\n",
    "        )\n",
    "        \n",
    "        print(f\"DEBUG: Query '{res_name}'\")\n",
    "        \n",
    "        filtered_matches = []\n",
    "        # --- FIX: Unpack the tuple of 2 items correctly ---\n",
    "        for match_text, score in matches:\n",
    "            if score >= min_score:\n",
    "                # Find the index of the matched sentence in the original list\n",
    "                # We use a try-except block for robustness in case of unexpected data.\n",
    "                try:\n",
    "                    original_sentence_index = searchable_sentences.index(match_text)\n",
    "                    # Append all three pieces of information\n",
    "                    filtered_matches.append((match_text, score, original_sentence_index)) \n",
    "                except ValueError:\n",
    "                    # This will happen if the match text isn't found in the list,\n",
    "                    # e.g., due to slight string differences not captured by .index()\n",
    "                    continue\n",
    "        \n",
    "        filtered_matches_by_string[res_name] = filtered_matches\n",
    "\n",
    "    return filtered_matches_by_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9802c351",
   "metadata": {},
   "source": [
    "### Find timestamp function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4937bbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcript_timestamps in the first_five_clean_transcript_timestamps_df are a list of dicts\n",
    "# Each dict (stamp) looks like this: 'episode_number': ep_num, 'timestamp': actual_time_string, 'start_index': start_position_in_text\n",
    "\n",
    "def _find_timestamp(original_sentence_start_index: int, transcript_timestamps: List[dict]):\n",
    "    \"\"\"\n",
    "    Takes an original start index for a sentence where a match was found, and a list of timestamp dictionaries,\n",
    "    and returns the nearest timestamp occurring before or at that index.\n",
    "    \"\"\"\n",
    "    if original_sentence_start_index is None:\n",
    "        return None \n",
    "    # Could sort timestamps here for good practice, but should be sorted already\n",
    "    # Reverse-iterate over timestamps to find the \"nearest before or at\"\n",
    "    for timestamp_dict in reversed(transcript_timestamps):\n",
    "        if timestamp_dict[\"start_index\"] <= original_sentence_start_index:\n",
    "            return timestamp_dict[\"timestamp\"]\n",
    "            \n",
    "    return None # If no timestamp found before the quote's starting position (all eps start \"Starting point is 00:00:00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45e0890",
   "metadata": {},
   "source": [
    "### Find matches and timestamps function\n",
    "\n",
    "Iterates through episodes, where there are mentions, creates a dict for each mention, and returns a list of dicts (all mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aa6243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can store in a seperate df for now, and merge if we want to (good for MVP testing, good for flexibility)\n",
    "\n",
    "def find_matches_and_timestamps(combined_df: pd.DataFrame, min_match_score: int = 90):\n",
    "    \"\"\"\n",
    "    Takes in a combined_df (combines ep_meta_and_mentions with clean_transcripts_timestamps), iterates through episodes\n",
    "    and applies matches_by_res_name_from_list_of_res_names (which iterates through each restaurant, and searches for matches using the\n",
    "    full mention as a query). Returns a list of all mentions.\n",
    "\n",
    "    Mentions are structured:\n",
    "    mention = {\n",
    "                        \"Episode ID\": episode_number, \n",
    "                        \"Restaurant\": restaurant_name_query,\n",
    "                        \"Mention text\": original_sentence_text, \n",
    "                        \"Match Score\": score,\n",
    "                        \"Match Type\": \"full, over 90\",\n",
    "                        \"Timestamp\": timestamp, \n",
    "                    }\n",
    "\n",
    "    Note mentions have been cleaned using same logic applied to sentences (free of puntuation), and replaces e with an accent.\n",
    "    \"\"\"\n",
    "    all_mentions_collected = []\n",
    "\n",
    "    for index, combined_row in combined_test_df.iterrows():\n",
    "        mentions_for_current_episode = [] # This list now holds mentions for the CURRENT episode only\n",
    "        episode_number = combined_row.get('episode_number')\n",
    "        guest_name = combined_row.get('guest_name')\n",
    "        clean_transcript_text = combined_row.get('clean_transcript_text')\n",
    "        periodic_timestamps = combined_row.get('periodic_timestamps')\n",
    "        \n",
    "        restaurants_data = combined_row.get('restaurants_mentioned', [])\n",
    "\n",
    "        # Unsure what data type the res mentions are, hence need for this\n",
    "        restaurants_list = []\n",
    "        if isinstance(restaurants_data, list):\n",
    "            restaurants_list = restaurants_data\n",
    "            print(f\"Res mentions are in list form\")\n",
    "        elif isinstance(restaurants_data, np.ndarray) and restaurants_data.size > 0:\n",
    "            # Flatten the array and convert it to a standard Python list of strings\n",
    "            restaurants_raw_list = restaurants_data.flatten().tolist()\n",
    "            restaurants_list = [name.strip().lower() for name in restaurants_raw_list if name.strip()]\n",
    "        elif isinstance(restaurants_data, str):\n",
    "            restaurants_list = [name.strip() for name in restaurants_data.split(',') if name.strip()]\n",
    "            print(f\"Res mentions are in string form\")\n",
    "\n",
    "        print(f\"\\n--- Processing Episode {episode_number} ({guest_name}) ---\")\n",
    "        print(f\"  Transcript length: {len(clean_transcript_text)} characters\")\n",
    "        print(f\"  Number of periodic timestamps: {len(periodic_timestamps)}\")\n",
    "\n",
    "        if restaurants_list:\n",
    "            print(f\"  Found {len(restaurants_list)} raw mentions for this episode:\")\n",
    "            print(f\"Here they are: {restaurants_list}\")\n",
    "            \n",
    "            episode_sentences_data = _create_list_tuple_clean_sen_og_sen_og_index(clean_transcript_text)\n",
    "            searchable_sentences = [item[0] for item in episode_sentences_data] # This is to select the cleaned sentence from the list of tuple\n",
    "            # of cleaned sentence, original, and true start index that create_sentence_list creates\n",
    "\n",
    "            all_matches_for_episode = _matches_by_res_name_from_list_of_res_names(restaurants_list, searchable_sentences, 90)\n",
    "            # === all_matches_for_episode is a dict with key res_name and value lists of matches (matches r tuples of quote, score)\n",
    "            for restaurant_name_query, match_list_for_query in all_matches_for_episode.items():\n",
    "                if match_list_for_query: \n",
    "                    for matched_cleaned_text, score, matched_sentence_index in match_list_for_query:\n",
    "                        original_sentence_data = episode_sentences_data[matched_sentence_index] # This takes you back to episode sentences data for the sentence index \n",
    "                        # Which is a tuple of clean sentence, original, and index of sentence within sen list\n",
    "                        original_sentence_text = original_sentence_data[1] # The og sentence is at index 1 in this tuple\n",
    "                        original_start_index = original_sentence_data[2]   # The og start index is at index 2 in this tuple\n",
    "                        \n",
    "                        timestamp = _find_timestamp(original_start_index, periodic_timestamps)\n",
    "                        \n",
    "                        mention = {\n",
    "                            \"Episode ID\": episode_number, \n",
    "                            \"Restaurant\": restaurant_name_query,\n",
    "                            \"Mention text\": original_sentence_text, \n",
    "                            \"Match Score\": score,\n",
    "                            \"Match Type\": \"full, over 90\",\n",
    "                            \"Timestamp\": timestamp, \n",
    "                        }\n",
    "                        # 2. Append each individual mention to the GLOBAL list\n",
    "                        all_mentions_collected.append(mention) \n",
    "                        mentions_for_current_episode.append(mention) # Keep this to count for the print statement below\n",
    "\n",
    "            if mentions_for_current_episode: # Check the current episode's list for printing\n",
    "                print(f\"Mentions collected for Episode {episode_number}: {len(mentions_for_current_episode)} rows.\")\n",
    "            else:\n",
    "                print(f\"No specific mentions processed for Episode {episode_number}.\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"  No raw mentions found in 'restaurants_mentioned' list for Episode {episode_number}.\")\n",
    "    return all_mentions_collected\n",
    "\n",
    "\n",
    "all_mentions_collected = find_matches_and_timestamps(combined_test_df, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064e1ede",
   "metadata": {},
   "source": [
    "### Convert list of dicts into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc36ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_mentions_collected:\n",
    "    final_mentions_df = pd.DataFrame(all_mentions_collected)\n",
    "    print(f\"\\n--- ALL MENTIONS COLLECTED ---\")\n",
    "    print(f\"Total Mentions DataFrame created with {len(final_mentions_df)} rows.\")\n",
    "    print(f\"\\n --- Here is the full dataframe ---\")\n",
    "    print(final_mentions_df) \n",
    "else:\n",
    "    print(\"\\nNo mentions were found across all episodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf1b884",
   "metadata": {},
   "source": [
    "## Function to list top matche for each query only\n",
    "\n",
    "The MVP will first identify easy wins (top match from using the full restaurant name as a query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbd3534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Episode 1 (Scroobius Pip) ---\n",
      "  Transcript length: 73155 characters\n",
      "  Number of periodic timestamps: 170\n",
      "  Found 1 raw mentions for this episode:\n",
      "Here they are: ['oli babas kerb camden']\n",
      "DEBUG: Query 'oli babas kerb camden'\n",
      "No specific mentions processed for Episode 1.\n",
      "\n",
      "--- Processing Episode 2 (Grace Dent) ---\n",
      "  Transcript length: 57238 characters\n",
      "  Number of periodic timestamps: 156\n",
      "  Found 2 raw mentions for this episode:\n",
      "Here they are: ['little owl', 'trullo']\n",
      "DEBUG: Query 'little owl'\n",
      "DEBUG: Query 'trullo'\n",
      "Mentions collected for Episode 2: 2 rows.\n",
      "\n",
      "--- Processing Episode 3 (Richard Osman) ---\n",
      "  Transcript length: 87390 characters\n",
      "  Number of periodic timestamps: 280\n",
      "  Found 3 raw mentions for this episode:\n",
      "Here they are: ['five guys', 'cora pearl', 'berners tavern']\n",
      "DEBUG: Query 'five guys'\n",
      "DEBUG: Query 'cora pearl'\n",
      "DEBUG: Query 'berners tavern'\n",
      "Mentions collected for Episode 3: 3 rows.\n",
      "\n",
      "--- Processing Episode 4 (Nish Kumar) ---\n",
      "  Transcript length: 0 characters\n",
      "  Number of periodic timestamps: 0\n",
      "  Found 2 raw mentions for this episode:\n",
      "Here they are: ['bademiya', 'the owl and the pussycat']\n",
      "DEBUG: Query 'bademiya'\n",
      "DEBUG: Query 'the owl and the pussycat'\n",
      "No specific mentions processed for Episode 4.\n",
      "\n",
      "--- Processing Episode 5 (Aisling Bea) ---\n",
      "  Transcript length: 77801 characters\n",
      "  Number of periodic timestamps: 211\n",
      "  Found 2 raw mentions for this episode:\n",
      "Here they are: ['cafe gratitude', 'burger and lobster']\n",
      "DEBUG: Query 'cafe gratitude'\n",
      "DEBUG: Query 'burger and lobster'\n",
      "Mentions collected for Episode 5: 2 rows.\n",
      "\n",
      "--- TOP COLLECTED ---\n",
      "Top Mentions DataFrame created with 22 rows.\n",
      "   Episode ID          Restaurant  \\\n",
      "0           2          little owl   \n",
      "1           2              trullo   \n",
      "2           3           five guys   \n",
      "3           3          cora pearl   \n",
      "4           3      berners tavern   \n",
      "5           5      cafe gratitude   \n",
      "6           5  burger and lobster   \n",
      "\n",
      "                                        Mention text  Match Score  \\\n",
      "0  it would be, the side dish would be from littl...          100   \n",
      "1  and it's the beef shin ragu with probably it's...          100   \n",
      "2  what if every single thing that i'm going to m...          100   \n",
      "3  this was a difficult question for me until lit...          100   \n",
      "4  there's a restaurant near here called the burn...           93   \n",
      "5  do you know actually, if there's any vegans ou...          100   \n",
      "6  and they were like, have you ever been to burg...          100   \n",
      "\n",
      "      Match Type Timestamp  \n",
      "0  full, over 90  00:34:52  \n",
      "1  full, over 90  00:20:26  \n",
      "2  full, over 90  00:03:42  \n",
      "3  full, over 90  00:13:00  \n",
      "4  full, over 90  00:38:58  \n",
      "5  full, over 90  00:22:02  \n",
      "6  full, over 90  00:35:52  \n",
      "\n",
      " --- Here is the full top mentions dataframe ---\n"
     ]
    }
   ],
   "source": [
    "def find_top_match_and_timestamps(combined_df: pd.DataFrame, min_match_score: int = 90):\n",
    "    \"\"\"\n",
    "    Takes in a combined_df (combines ep_meta_and_mentions with clean_transcripts_timestamps), iterates through episodes\n",
    "    and applies matches_by_res_name_from_list_of_res_names (which iterates through each restaurant, and searches for matches using the\n",
    "    full mention as a query). Returns a list of all mentions.\n",
    "\n",
    "    Mentions are structured:\n",
    "    mention = {\n",
    "                        \"Episode ID\": episode_number, \n",
    "                        \"Restaurant\": restaurant_name_query,\n",
    "                        \"Mention text\": original_sentence_text, \n",
    "                        \"Match Score\": score,\n",
    "                        \"Match Type\": \"full, over 90\",\n",
    "                        \"Timestamp\": timestamp, \n",
    "                    }\n",
    "\n",
    "    Note mentions have been cleaned using same logic applied to sentences (free of puntuation), and replaces e with an accent.\n",
    "    \"\"\"\n",
    "    all_mentions_collected = []\n",
    "\n",
    "    for index, combined_row in combined_df.iterrows():\n",
    "        mentions_for_current_episode = [] # This list now holds mentions for the CURRENT episode only\n",
    "        episode_number = combined_row.get('episode_number')\n",
    "        guest_name = combined_row.get('guest_name')\n",
    "        clean_transcript_text = combined_row.get('clean_transcript_text')\n",
    "        periodic_timestamps = combined_row.get('periodic_timestamps')\n",
    "        \n",
    "        restaurants_data = combined_row.get('restaurants_mentioned', [])\n",
    "\n",
    "        # Unsure what data type the res mentions are, hence need for this\n",
    "        restaurants_list = []\n",
    "        if isinstance(restaurants_data, list):\n",
    "            restaurants_list = restaurants_data\n",
    "            print(f\"Res mentions are in list form\")\n",
    "        elif isinstance(restaurants_data, np.ndarray) and restaurants_data.size > 0:\n",
    "            # Flatten the array and convert it to a standard Python list of strings\n",
    "            restaurants_raw_list = restaurants_data.flatten().tolist()\n",
    "            restaurants_list = [name.strip().lower() for name in restaurants_raw_list if name.strip()]\n",
    "        elif isinstance(restaurants_data, str):\n",
    "            restaurants_list = [name.strip() for name in restaurants_data.split(',') if name.strip()]\n",
    "            print(f\"Res mentions are in string form\")\n",
    "\n",
    "        print(f\"\\n--- Processing Episode {episode_number} ({guest_name}) ---\")\n",
    "        print(f\"  Transcript length: {len(clean_transcript_text)} characters\")\n",
    "        print(f\"  Number of periodic timestamps: {len(periodic_timestamps)}\")\n",
    "\n",
    "        if restaurants_list:\n",
    "            print(f\"  Found {len(restaurants_list)} raw mentions for this episode:\")\n",
    "            print(f\"Here they are: {restaurants_list}\")\n",
    "            \n",
    "            episode_sentences_data = _create_list_tuple_clean_sen_og_sen_og_index(clean_transcript_text)\n",
    "            searchable_sentences = [item[0] for item in episode_sentences_data] # This is to select the cleaned sentence from the list of tuple\n",
    "            # of cleaned sentence, original, and true start index that create_sentence_list creates\n",
    "\n",
    "            all_matches_for_episode = _matches_by_res_name_from_list_of_res_names(restaurants_list, searchable_sentences, 90)\n",
    "            # === all_matches_for_episode is a dict with key res_name and value lists of matches (matches r tuples of quote, score)\n",
    "            for restaurant_name_query, match_list_for_query in all_matches_for_episode.items():\n",
    "                if match_list_for_query: \n",
    "                    top_match = match_list_for_query[0]\n",
    "                    # Unpack the top match's data\n",
    "                    matched_cleaned_text, score, matched_sentence_index = top_match\n",
    "                    original_sentence_data = episode_sentences_data[matched_sentence_index] # This takes you back to episode sentences data for the sentence index \n",
    "                    # Which is a tuple of clean sentence, original, and index of sentence within sen list\n",
    "                    original_sentence_text = original_sentence_data[1] # The og sentence is at index 1 in this tuple\n",
    "                    original_start_index = original_sentence_data[2]   # The og start index is at index 2 in this tuple\n",
    "                    \n",
    "                    timestamp = _find_timestamp(original_start_index, periodic_timestamps)\n",
    "                    \n",
    "                    mention = {\n",
    "                        \"Episode ID\": episode_number, \n",
    "                        \"Restaurant\": restaurant_name_query,\n",
    "                        \"Mention text\": original_sentence_text, \n",
    "                        \"Match Score\": score,\n",
    "                        \"Match Type\": \"full, over 90\",\n",
    "                        \"Timestamp\": timestamp, \n",
    "                    }\n",
    "                    # 2. Append each individual mention to the GLOBAL list\n",
    "                    all_mentions_collected.append(mention) \n",
    "                    mentions_for_current_episode.append(mention) # Keep this to count for the print statement below\n",
    "\n",
    "            if mentions_for_current_episode: # Check the current episode's list for printing\n",
    "                print(f\"Mentions collected for Episode {episode_number}: {len(mentions_for_current_episode)} rows.\")\n",
    "            else:\n",
    "                print(f\"No specific mentions processed for Episode {episode_number}.\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"  No raw mentions found in 'restaurants_mentioned' list for Episode {episode_number}.\")\n",
    "    return all_mentions_collected\n",
    "    \n",
    "\n",
    "top_matches = find_top_match_and_timestamps(combined_test_df, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b261b6",
   "metadata": {},
   "source": [
    "### Convert list of top mentions into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db65b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if top_matches:\n",
    "    top_mentions_df = pd.DataFrame(top_matches)\n",
    "    print(f\"\\n--- TOP COLLECTED ---\")\n",
    "    print(f\"Top Mentions DataFrame created with {len(top_mentions_df)} rows.\")\n",
    "    print(top_mentions_df) \n",
    "    print(f\"\\n --- Here is the full top mentions dataframe ---\")\n",
    "else:\n",
    "    print(\"\\nNo mentions were found across all episodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36eaaf6",
   "metadata": {},
   "source": [
    "## Debugging matches functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208177a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res mens: ['little owl', 'trullo']\n",
      "\n",
      " transcript sample: starting point is 00:00:00 hello, listeners of the off menu podcast. it is ed gamble here from the off menu podcast. i have a very exciting announcement. i have written my first ever book. i am absolutely over the moon to announce this. i'm very, very proud of it. of course, what else could i write a book about? but food. my book is all about food. my life in food. how greedy i am. what a greedy little boy i was. what a greedy adult i am. i think it's very funny. i'm very proud of it. the book is called glutton, the multi-course life of a very greedy boy. and it's coming out this october, but it is available to pre-order now, wherever you pre-order books from. and if you like my signature, i've done some signed copies, starting point is 00:00:43 which are exclusively available from waterstones. but go and pre-order your copy of glutton, the multi-course life of a very greedy boy now. please? bon appetito and welcome to the off menu podcast with ed gamble and james a. caster. hello. tha\n",
      "DEBUG: Query 'little owl'\n",
      "DEBUG: Query 'trullo'\n",
      "\n",
      "---Results for query: little owl\n",
      "\n",
      "Match: ('it would be the side dish would be from little owl in manhattan', 100, 0)\n",
      "\n",
      "Match: ('what a greedy little boy i was', 80, 1)\n",
      "\n",
      "Match: ('delicious raw salmon cut up with these crispy little oniony bits in there and some fried and some rice and guacamole loads of guacamole', 80, 2)\n",
      "\n",
      "Match: ('i think i like to i bring sort of good chat fun conversation topics a little bit of fun food knowledge', 70, 3)\n",
      "\n",
      "Match: ('well i mean its a little bit of a flipup from laughing cow triangles isnt it ive got james', 70, 4)\n",
      "\n",
      "Match: ('you saw the little bits off', 70, 5)\n",
      "\n",
      "Match: ('starting point is 002834 but they send a sommelier over and i thought right im just going to im going to say to him not youre not fooling me into spending 200 pounds on a bottle of wine because i just so i just kind of went look i just want to spend like choose me one but 60 quid', 70, 6)\n",
      "\n",
      "Match: ('starting point is 003003 its got a little bit of heat', 70, 7)\n",
      "\n",
      "Match: ('starting point is 003032 well go out for the night and were going to go and watch this festival this like little local festival', 70, 8)\n",
      "\n",
      "Match: ('youre having such small little small portions and you just end up absolutely wasted at the starting point is 004636 end of the night', 70, 9)\n",
      "\n",
      "Match: ('so you get the taste of the banoffee pie right its incredible that its you know putting everything in tiny little its a kickback starting point is 004734 from probably the late nineties and experimental', 70, 10)\n",
      "\n",
      "Match: ('and he so i went for dinner there and that was one of those where every so often theyd bring like one tiny little dim sum in like with some kind of incredibly expensive caviar on it', 70, 11)\n",
      "\n",
      "Match: ('they said its a little bit of risk i hope youre not going to be offended', 70, 12)\n",
      "\n",
      "Match: ('its like a little extra thing to this podcast', 70, 13)\n",
      "\n",
      "Match: ('wow', 67, 14)\n",
      "\n",
      "Match: ('hes written off three cars in his life', 60, 15)\n",
      "\n",
      "Match: ('starting point is 000821 what flat smells of cheese and chive pringles well probably that', 60, 16)\n",
      "\n",
      "Match: ('but you could literally have been up until four in the morning going crazy but you still feel like youre nailing life down', 60, 17)\n",
      "\n",
      "Match: ('yeah but when they or they yeah they put down a basket and yeah and its and its hard and you think well how long has it been sitting there so yes', 60, 18)\n",
      "\n",
      "Match: ('and it has to be in kind of silver terrenes that are on a kind of stand where the four the four of them are like in a little tree', 60, 19)\n",
      "\n",
      "---Results for query: trullo\n",
      "\n",
      "Match: ('and its the beef shin ragu with probably its like freshly made tagliatelle and its a restaurant called trullo in islington', 100, 0)\n",
      "\n",
      "Match: ('and you know he was scrupulous pip', 67, 1)\n",
      "\n",
      "Match: ('its a very light aniseed flavour and it can actually bring a lot to a dish and raw', 67, 2)\n",
      "\n",
      "Match: ('this all just feels really sinister actually', 67, 3)\n",
      "\n",
      "Match: ('is that how it works fail the driving test marry the instructor', 67, 4)\n",
      "\n",
      "Match: ('i havent actually got high function', 67, 5)\n",
      "\n",
      "Match: ('but yeah theres something i think that theres something about that whole ritual of papa doms coming and everybody getting one and crunching them', 67, 6)\n",
      "\n",
      "Match: ('yeah thats thats that feels like a truly british thing', 67, 7)\n",
      "\n",
      "Match: ('and also like why why why do you think you have the right to do that but i would never go and get a full one', 67, 8)\n",
      "\n",
      "Match: ('im saying like almost vegan about three or four months actually', 67, 9)\n",
      "\n",
      "Match: ('now and again it wasnt the hiding contempt that i had trouble for there', 67, 10)\n",
      "\n",
      "Match: ('yes ive actually just checked with the kitchen if we had a beef for goo', 67, 11)\n",
      "\n",
      "Match: ('i actually think thats thats a myth', 67, 12)\n",
      "\n",
      "Match: ('starting point is 003032 well go out for the night and were going to go and watch this festival this like little local festival', 67, 13)\n",
      "\n",
      "Match: ('you are the first critic weve had in the restaurant actually', 67, 14)\n",
      "\n",
      "Match: ('i should probably start getting like a ready wrecking her out and working out whether my mother could actually have sex with him or not', 67, 15)\n",
      "\n",
      "Match: ('so then youre picking it up and youre literally like oh so you eat the whole thing', 67, 16)\n",
      "\n",
      "Match: ('theres actually no other staff in this restaurant', 67, 17)\n",
      "\n",
      "Match: ('no actually goodbye', 67, 18)\n",
      "\n",
      "Match: ('hello', 60, 19)\n"
     ]
    }
   ],
   "source": [
    "# Gather restaurant mentions from df\n",
    "\n",
    "def res_mens(ep_num, dataframe=combined_test_df):\n",
    "    combined_df = dataframe\n",
    "    ep_row = combined_df[combined_df['episode_number'] == ep_num]\n",
    "    res_data = ep_row.get('restaurants_mentioned').item()\n",
    "    restaurants_raw_list = res_data.flatten().tolist()\n",
    "    res_mentions = [name.strip().lower() for name in restaurants_raw_list if name.strip()]\n",
    "    return res_mentions\n",
    "\n",
    "print(f\"res mens: {res_mens(2)}\")\n",
    "ep2mentions = res_mens(2)\n",
    "\n",
    "# Examine transcript\n",
    "\n",
    "def transcript(ep_num, dataframe=combined_test_df):\n",
    "    combined_df = dataframe\n",
    "    ep_row = combined_df[combined_df['episode_number'] == ep_num]\n",
    "    transcript_series = ep_row['clean_transcript_text']\n",
    "    transcript_string = transcript_series.iloc[0]\n",
    "    return transcript_string\n",
    "print(f\"\\n transcript sample: {transcript(2)[:1000]}\")\n",
    "\n",
    "ep2transcript = transcript(2)\n",
    "\n",
    "# Create sentences list(s)\n",
    "\n",
    "episode_sentences_data = _create_list_tuple_clean_sen_og_sen_og_index(ep2transcript)\n",
    "ep2_searchable_sentences = [item[0] for item in episode_sentences_data]\n",
    "\n",
    "# Collate top 20 matches for restautant mention(s)\n",
    "\n",
    "def check_top_twenty(matches_list, searchable_sentences, score=0):\n",
    "    matches = _matches_by_res_name_from_list_of_res_names(matches_list, searchable_sentences, score)\n",
    "    for res_name in matches.keys():\n",
    "        print(f\"\\n---Results for query: {res_name}\")\n",
    "        for match in matches[res_name]:\n",
    "            print(f\"\\nMatch: {match}\")\n",
    "\n",
    "check_top_twenty(ep2mentions, ep2_searchable_sentences )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Off Menu Project Venv",
   "language": "python",
   "name": "off_menu_project_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
