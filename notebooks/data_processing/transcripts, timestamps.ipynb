{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dfa27ed",
   "metadata": {},
   "source": [
    "# Off menu transcripts and timestamps processing\n",
    "\n",
    "This notebook serves as a development environment for the logic to process the transcripts and generate timestamps from them. The final production code is located in off_menu/data_processing.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8102bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to sys.path: c:\\Users\\jbara\\OneDrive\\Desktop\\Data_science\\Python projects\\Off Menu project\n",
      "Current sys.path: ['c:\\\\Users\\\\jbara\\\\OneDrive\\\\Desktop\\\\Data_science\\\\Python projects\\\\Off Menu project', 'C:\\\\Users\\\\jbara\\\\miniconda3\\\\python312.zip', 'C:\\\\Users\\\\jbara\\\\miniconda3\\\\DLLs', 'C:\\\\Users\\\\jbara\\\\miniconda3\\\\Lib', 'C:\\\\Users\\\\jbara\\\\miniconda3', 'c:\\\\Users\\\\jbara\\\\OneDrive\\\\Desktop\\\\Data_science\\\\Python projects\\\\Off Menu project\\\\.venv', '', 'c:\\\\Users\\\\jbara\\\\OneDrive\\\\Desktop\\\\Data_science\\\\Python projects\\\\Off Menu project\\\\.venv\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\jbara\\\\OneDrive\\\\Desktop\\\\Data_science\\\\Python projects\\\\Off Menu project\\\\.venv\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\jbara\\\\OneDrive\\\\Desktop\\\\Data_science\\\\Python projects\\\\Off Menu project\\\\.venv\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\jbara\\\\OneDrive\\\\Desktop\\\\Data_science\\\\Python projects\\\\Off Menu project\\\\.venv\\\\Lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "# Ensure imports can find my utils:\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..', '..'))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"Project root added to sys.path: {project_root}\")\n",
    "print(f\"Current sys.path: {sys.path}\")\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "import re \n",
    "from typing import List\n",
    "from fuzzywuzzy import process\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Project specific imports\n",
    "from off_menu.utils import Episode\n",
    "from off_menu.utils import clean_text\n",
    "from off_menu.utils import num_check\n",
    "from off_menu.utils import find_num_end\n",
    "from off_menu.utils import name_num_split\n",
    "from off_menu.utils import clean_res\n",
    "from off_menu.utils import get_episode_sentences\n",
    "from off_menu.utils import create_sentence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99ecafa",
   "metadata": {},
   "source": [
    "## Functions to get transcript string from html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c82942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting point is 00:00:00 hello, listeners of the off menu podcast. it is ed gamble here from the o\n"
     ]
    }
   ],
   "source": [
    "#  Function to access sentences bs4 elements, which will be converted into cleaned text later\n",
    "def get_episode_sentences(html):\n",
    "    \"\"\"\n",
    "    Given the html of an Off Menu episode transcript site, returns the soup object with div elements with class 'single sentence'\n",
    "\n",
    "    Parameters:\n",
    "        episode_HTML (html): HTML of the podcast episode from podscripts.po\n",
    "\n",
    "    Returns:\n",
    "        List[str]: list of div elements with class 'single sentence'\"\"\"\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    all_sentences = soup.find_all(\"div\", class_=\"single-sentence\")\n",
    "    return all_sentences\n",
    "\n",
    "\n",
    "#  Function to take in sentces bs4 elements and return a cleaned string transcript\n",
    "def clean_text(sentences):\n",
    "    \"\"\"\n",
    "    Given \"all sentences\" (a list of bs4.Tag elements) this function returns cleaned text (str).\n",
    "\n",
    "    \"all sentences\" is a list of bs4.Tag element representing all divs with class single sentence in this case. The\n",
    "    function also lowercases all text, removes extra whitespace and lines, and all puntuation besides full stops.\n",
    "\n",
    "    Args:\n",
    "        sentences (bs4.Tag): all sentences using the find_all bs4 function.\n",
    "\n",
    "    Returns:\n",
    "        cleaned text\n",
    "    \"\"\"\n",
    "    cleaned_line_list = []\n",
    "    for section in sentences:\n",
    "        # make lowercase\n",
    "        text_lower = section.text.lower()\n",
    "        # Split into lines\n",
    "        lines = text_lower.splitlines()\n",
    "        # Clean whitespace from end/start of lines, and don't include empty lines (recall list comp conditional at the end, and empty string is falsy)\n",
    "        cleaned_lines = [line.strip() for line in lines if line.strip()]\n",
    "        # Join lines using .join, with a space between then\n",
    "        single_line_text = \" \".join(cleaned_lines)\n",
    "        if single_line_text:\n",
    "            cleaned_line_list.append(single_line_text)\n",
    "    all_text_single_line = \" \".join(cleaned_line_list)\n",
    "    return all_text_single_line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab3e723",
   "metadata": {},
   "source": [
    "## Testing transcript production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ccaa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_filepath = os.path.join(project_root, 'data/test_temp/ep_1.html')\n",
    "try:\n",
    "    with open(test_filepath, 'r', encoding='utf-8') as html:\n",
    "        html_text = html.read()\n",
    "    transcript_str = clean_text(get_episode_sentences(html_text))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {test_filepath}. Did it save correctly?\")\n",
    "\n",
    "print(transcript_str[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e463b1",
   "metadata": {},
   "source": [
    "## Collating timestamps from transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6161d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'episode_number': 1, 'timestamp': '00:00:00', 'start_index': 0}, {'episode_number': 1, 'timestamp': '00:00:43', 'start_index': 729}]\n"
     ]
    }
   ],
   "source": [
    "# Helper function which contains the actual collation logic\n",
    "\n",
    "def _extract_timestamps_as_list_of_dicts(transcript, ep_num):\n",
    "    \"\"\"\n",
    "    Given transcript (str), and ep_num, returns a list of dicts containing all timestamps in the transcript. \n",
    "    Each dict contains episode number, timestamp, and start_index.\n",
    "    \"\"\"\n",
    "    timestamp_pattern = re.compile(r\"starting point is (\\d{2}:\\d{2}:\\d{2})\")\n",
    "    all_timestamps_in_transcript = []\n",
    "    for match in timestamp_pattern.finditer(transcript):\n",
    "        # Get the captured timestamp string (e.g., \"00:00:05\")\n",
    "        actual_time_string = match.group(\n",
    "            1\n",
    "        )  \n",
    "        # We use group(1) because that's our (HH:MM:SS) part, group(0) refers to the whole string by default\n",
    "\n",
    "        # Get the starting index of the entire match\n",
    "        start_position_in_text = match.start()\n",
    "        # Store this as a dict with episode_number as key\n",
    "        stamp_dict = {\n",
    "        'episode_number': ep_num,\n",
    "        'timestamp': actual_time_string,\n",
    "        'start_index': start_position_in_text\n",
    "        }\n",
    "        # Store this extracted data (the timestamp string and its position)\n",
    "        all_timestamps_in_transcript.append(stamp_dict)\n",
    "    return all_timestamps_in_transcript\n",
    "\n",
    "# Testing function using transcript_str from cell above\n",
    "\n",
    "timestamps_dict_list = _extract_timestamps_as_list_of_dicts(transcript_str, 1)\n",
    "print(timestamps_dict_list[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c0face",
   "metadata": {},
   "source": [
    "## Creating a dataframe from the timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb892536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_number</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>start_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>00:00:43</td>\n",
       "      <td>729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>00:01:45</td>\n",
       "      <td>1473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>00:02:36</td>\n",
       "      <td>2284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>00:03:13</td>\n",
       "      <td>3064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "      <td>01:06:08</td>\n",
       "      <td>71654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1</td>\n",
       "      <td>01:06:24</td>\n",
       "      <td>71973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1</td>\n",
       "      <td>01:07:02</td>\n",
       "      <td>72331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>1</td>\n",
       "      <td>01:07:21</td>\n",
       "      <td>72689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>1</td>\n",
       "      <td>01:07:36</td>\n",
       "      <td>72980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     episode_number timestamp  start_index\n",
       "0                 1  00:00:00            0\n",
       "1                 1  00:00:43          729\n",
       "2                 1  00:01:45         1473\n",
       "3                 1  00:02:36         2284\n",
       "4                 1  00:03:13         3064\n",
       "..              ...       ...          ...\n",
       "165               1  01:06:08        71654\n",
       "166               1  01:06:24        71973\n",
       "167               1  01:07:02        72331\n",
       "168               1  01:07:21        72689\n",
       "169               1  01:07:36        72980\n",
       "\n",
       "[170 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "timestamps_test_df = pd.DataFrame(timestamps_dict_list)\n",
    "timestamps_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978f5884",
   "metadata": {},
   "source": [
    "## Script to apply timestamp collation to each episode and store in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19702a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Ep and mentions dataframe head---\n",
      "   episode_number     guest_name  \\\n",
      "0               1  Scroobius Pip   \n",
      "1               2     Grace Dent   \n",
      "2               3  Richard Osman   \n",
      "3               4     Nish Kumar   \n",
      "4               5    Aisling Bea   \n",
      "\n",
      "                                                 url  \\\n",
      "0  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "1  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "2  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "3  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "4  https://podscripts.co/podcasts/off-menu-with-e...   \n",
      "\n",
      "                     restaurants_mentioned  \n",
      "0                [Oli Baba’s, Kerb Camden]  \n",
      "1                     [Little Owl, Trullo]  \n",
      "2  [Five Guys, Cora Pearl, Berners Tavern]  \n",
      "3       [Bademiya, The Owl & The Pussycat]  \n",
      "4       [Café Gratitude, Burger & Lobster]  \n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 5] Failed to open local file 'c:/Users/jbara/OneDrive/Desktop/Data_science/Python projects/Off Menu project/data/test_temp'. Detail: [Windows error 5] Access is denied.\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Save a small version for testing (required for testing transcript extraction logic)\u001b[39;00m\n\u001b[32m     12\u001b[39m five_row_test_ep_meta_and_mentions = ep_and_mentions.head()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mfive_row_test_ep_meta_and_mentions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_temp_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Create list to store lists of dict entries with individual timestamps\u001b[39;00m\n\u001b[32m     16\u001b[39m test_timestamp_list = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jbara\\OneDrive\\Desktop\\Data_science\\Python projects\\Off Menu project\\.venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jbara\\OneDrive\\Desktop\\Data_science\\Python projects\\Off Menu project\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:3118\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[39m\n\u001b[32m   3037\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3038\u001b[39m \u001b[33;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[32m   3039\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3114\u001b[39m \u001b[33;03m>>> content = f.read()\u001b[39;00m\n\u001b[32m   3115\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3116\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3118\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3119\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jbara\\OneDrive\\Desktop\\Data_science\\Python projects\\Off Menu project\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:482\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m impl = get_engine(engine)\n\u001b[32m    480\u001b[39m path_or_buf: FilePath | WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] = io.BytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    494\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io.BytesIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jbara\\OneDrive\\Desktop\\Data_science\\Python projects\\Off Menu project\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:229\u001b[39m, in \u001b[36mPyArrowImpl.write\u001b[39m\u001b[34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    219\u001b[39m         \u001b[38;5;28mself\u001b[39m.api.parquet.write_to_dataset(\n\u001b[32m    220\u001b[39m             table,\n\u001b[32m    221\u001b[39m             path_or_handle,\n\u001b[32m   (...)\u001b[39m\u001b[32m    225\u001b[39m             **kwargs,\n\u001b[32m    226\u001b[39m         )\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    228\u001b[39m         \u001b[38;5;66;03m# write to single output file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    237\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jbara\\OneDrive\\Desktop\\Data_science\\Python projects\\Off Menu project\\.venv\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1869\u001b[39m, in \u001b[36mwrite_table\u001b[39m\u001b[34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, use_byte_stream_split, column_encoding, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, write_page_index, write_page_checksum, sorting_columns, **kwargs)\u001b[39m\n\u001b[32m   1867\u001b[39m use_int96 = use_deprecated_int96_timestamps\n\u001b[32m   1868\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1869\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mParquetWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1870\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1871\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1872\u001b[39m \u001b[43m            \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1873\u001b[39m \u001b[43m            \u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1874\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_dictionary\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwrite_statistics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcoerce_timestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1877\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata_page_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_page_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mallow_truncated_timestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_truncated_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1879\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1880\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_deprecated_int96_timestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_int96\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompression_level\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_byte_stream_split\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_byte_stream_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcolumn_encoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumn_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata_page_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_page_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_compliant_nested_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_compliant_nested_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencryption_properties\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwrite_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdictionary_pagesize_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdictionary_pagesize_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstore_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstore_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwrite_page_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_page_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1891\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwrite_page_checksum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_page_checksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1892\u001b[39m \u001b[43m            \u001b[49m\u001b[43msorting_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43msorting_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1893\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[32m   1894\u001b[39m         writer.write_table(table, row_group_size=row_group_size)\n\u001b[32m   1895\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jbara\\OneDrive\\Desktop\\Data_science\\Python projects\\Off Menu project\\.venv\\Lib\\site-packages\\pyarrow\\parquet\\core.py:996\u001b[39m, in \u001b[36mParquetWriter.__init__\u001b[39m\u001b[34m(self, where, schema, filesystem, flavor, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, compression_level, use_byte_stream_split, column_encoding, writer_engine_version, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, write_page_index, write_page_checksum, sorting_columns, **options)\u001b[39m\n\u001b[32m    991\u001b[39m filesystem, path = _resolve_filesystem_and_path(where, filesystem)\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filesystem \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    993\u001b[39m     \u001b[38;5;66;03m# ARROW-10480: do not auto-detect compression.  While\u001b[39;00m\n\u001b[32m    994\u001b[39m     \u001b[38;5;66;03m# a filename like foo.parquet.gz is nonconforming, it\u001b[39;00m\n\u001b[32m    995\u001b[39m     \u001b[38;5;66;03m# shouldn't implicitly apply compression.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m     sink = \u001b[38;5;28mself\u001b[39m.file_handle = \u001b[43mfilesystem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_output_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    999\u001b[39m     sink = where\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jbara\\OneDrive\\Desktop\\Data_science\\Python projects\\Off Menu project\\.venv\\Lib\\site-packages\\pyarrow\\_fs.pyx:881\u001b[39m, in \u001b[36mpyarrow._fs.FileSystem.open_output_stream\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jbara\\OneDrive\\Desktop\\Data_science\\Python projects\\Off Menu project\\.venv\\Lib\\site-packages\\pyarrow\\error.pxi:154\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jbara\\OneDrive\\Desktop\\Data_science\\Python projects\\Off Menu project\\.venv\\Lib\\site-packages\\pyarrow\\error.pxi:91\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mPermissionError\u001b[39m: [WinError 5] Failed to open local file 'c:/Users/jbara/OneDrive/Desktop/Data_science/Python projects/Off Menu project/data/test_temp'. Detail: [Windows error 5] Access is denied.\r\n"
     ]
    }
   ],
   "source": [
    "# Load ep_and_mentions data (ep num, name, url etc.)\n",
    "test_temp_dir = os.path.join(project_root, 'data/test_temp')\n",
    "ep_and_mentions_filepath = os.path.join(test_temp_dir, 'ep_and_mentions.parquet')\n",
    "ep_and_mentions = pd.read_parquet(ep_and_mentions_filepath)\n",
    "\n",
    "print(\"---Ep and mentions dataframe head---\")\n",
    "print(ep_and_mentions.head())\n",
    "\n",
    "# Save a small version for testing (required for testing transcript extraction logic)\n",
    "five_row_test_ep_meta_and_mentions = ep_and_mentions.head()\n",
    "five_row_test_ep_meta_and_mentions.to_parquet(test_temp_dir, index=False)\n",
    "\n",
    "# Create list to store lists of dict entries with individual timestamps\n",
    "test_timestamp_list = []\n",
    "\n",
    "# iterate through test episodes (html already stored via earlier tests)\n",
    "for index, row in ep_and_mentions.head(2).iterrows(): \n",
    "    episode_num = row['episode_number']\n",
    "    episode_url = row['url']\n",
    "    filepath = os.path.join(project_root, f'data/test_temp/ep_{episode_num}.html')\n",
    "    try:\n",
    "        with open(test_filepath, 'r', encoding='utf-8') as html:\n",
    "            html_text_ = html.read()\n",
    "            transcript_str = clean_text(get_episode_sentences(html_text))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file was not found at {test_filepath}. Did it save correctly?\")\n",
    "    timestamps_dict = _extract_timestamps_as_list_of_dicts(transcript_str, episode_num)\n",
    "    test_timestamp_list.extend(timestamps_dict) # Note using extend so we will have a \"flat\" list of all entries rather than list of lists, better for df\n",
    "\n",
    "# Print first 2 dict entries from the single list containing all entries\n",
    "print(test_timestamp_list[:2])\n",
    "\n",
    "# Create the dataframe\n",
    "\n",
    "test_timestamp_df = pd.DataFrame(test_timestamp_list)\n",
    "test_timestamp_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0bd377",
   "metadata": {},
   "source": [
    "## Function to locate specific quote location using RegEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b3a085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_timestamp(quote, transcript, transcript_timestamps):\n",
    "    quote_loc = transcript.find(quote)\n",
    "    for timestamp in reversed(transcript_timestamps):\n",
    "        for key in timestamp.keys():\n",
    "            if timestamp[\"start_index\"] <= quote_loc:\n",
    "                return timestamp[\"timestamp_str\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Off Menu Project Venv",
   "language": "python",
   "name": "off_menu_project_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
